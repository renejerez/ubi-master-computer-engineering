{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from matplotlib.ticker import (MultipleLocator, FormatStrFormatter)\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libs to resize and read files\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "# List all files in the folder\n",
    "#######################################\n",
    "files = list(map(str, filter(lambda file: file.is_file(), Path(\"C:/temp/AR_out\").rglob('*'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "#declare arrays\n",
    "#######################################\n",
    "image_resized_list = []\n",
    "file_names_list = []\n",
    "\n",
    "\n",
    "#######################################\n",
    "# resize all files and save in arrays\n",
    "#######################################\n",
    "for file in files:\n",
    "    # read image\n",
    "    img = cv2.imread(file, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "    # Set Scale\n",
    "    scale_percent = 10 \n",
    "    width = int(img.shape[1] * scale_percent / 100)\n",
    "    height = int(img.shape[0] * scale_percent / 100)\n",
    "    dimension = (width, height)\n",
    "\n",
    "    # resize image\n",
    "    resized = cv2.resize(img, dimension, interpolation = cv2.INTER_AREA)\n",
    "\n",
    "    #save image resized and file name in arrays\n",
    "    image_resized_list.append(resized)\n",
    "    file_names_list.append(file.split('\\\\')[-1].split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##clear memory\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3315, 57, 76, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the list to a numpy array\n",
    "image_resized_array = np.array(image_resized_list)\n",
    "image_resized_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Get the number of images, dimensions and channels of each image\n",
    "num_images, height, width, num_channels = image_resized_array.shape\n",
    "\n",
    "# Reshape the array into a 2D array where each row is a vectorized image\n",
    "data_2d = image_resized_array.reshape(num_images, -1)  # -1 is a placeholder that tells numpy to compute the size of this dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vector = np.mean(data_2d, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center the data by subtracting the mean\n",
    "centered_data = data_2d - mean_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance_matrix = np.dot(centered_data.T, centered_data) / (num_images - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance_matrix = np.cov(data_2d, rowvar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  # Optional, for better visualization\n",
    "\n",
    "# Display a heatmap of the covariance matrix\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(covariance_matrix, cmap='coolwarm', fmt='.2f')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##clear memory\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices that would sort the eigenvalues in descending order\n",
    "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "\n",
    "# Sort the eigenvalues and eigenvectors\n",
    "sorted_eigenvalues = eigenvalues[sorted_indices]\n",
    "sorted_eigenvectors = eigenvectors[:, sorted_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_variance = np.sum(sorted_eigenvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_variance = np.cumsum(sorted_eigenvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_eigenvectors_to_retain = np.argmax(cumulative_variance / total_variance >= 0.99) + 1\n",
    "print (num_eigenvectors_to_retain)\n",
    "#num_eigenvectors_to_retain = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the top eigenvectors to retain 99% of the variance\n",
    "top_eigenvectors = sorted_eigenvectors[:, :num_eigenvectors_to_retain]\n",
    "\n",
    "\n",
    "# Project the centered data onto the selected eigenvectors\n",
    "projected_data = np.dot(centered_data, top_eigenvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project the centered data onto the selected eigenvectors\n",
    "projected_data = np.dot(centered_data, top_eigenvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#Making a test with a orignal, resized and projected image\n",
    "##############################################################################\n",
    "index = 2034  # Index of the image you want to select\n",
    "source_image = cv2.imread(files[index], cv2.IMREAD_UNCHANGED) \n",
    "resized_image = image_resized_array[index]\n",
    "original_image = image_resized_array[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_representation = projected_data[index]\n",
    "reconstructed_image_data = np.dot(projected_representation, top_eigenvectors.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_image = reconstructed_image_data.reshape(height, width, num_channels)\n",
    "reconstructed_image = np.clip(reconstructed_image, 0, 255).astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))  # Adjust the figure size to accommodate three images\n",
    "\n",
    "# Display source image\n",
    "plt.subplot(1, 3, 1)  # 1 row, 3 columns, 1st subplot\n",
    "plt.imshow(cv2.cvtColor(source_image, cv2.COLOR_BGR2RGB))  # Convert BGR to RGB if necessary\n",
    "plt.title('Source Image')\n",
    "\n",
    "# Display original image\n",
    "plt.subplot(1, 3, 2)  # 1 row, 3 columns, 2nd subplot\n",
    "plt.imshow(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB))  # Convert BGR to RGB if necessary\n",
    "#axs[1].imshow(resized_image)\n",
    "plt.title('resized Image')\n",
    "\n",
    "# Display reconstructed image\n",
    "plt.subplot(1, 3, 3)  # 1 row, 3 columns, 3rd subplot\n",
    "plt.imshow(cv2.cvtColor(reconstructed_image, cv2.COLOR_BGR2RGB))  # Convert BGR to RGB if necessary\n",
    "plt.title('Vector Image remapped')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merge projected_data with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scr_deep_learning_cnn_keras_py as keras_ml\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting each string by hyphen and storing the results in a new list\n",
    "file_names_list = [name.split('-') for name in file_names_list]\n",
    "\n",
    "# Creating a DataFrame from the list of split parts\n",
    "file_names_list = pd.DataFrame(file_names_list, columns=['gender', 'seq', 'type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>seq</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>m</td>\n",
       "      <td>001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>m</td>\n",
       "      <td>001</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>m</td>\n",
       "      <td>001</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>m</td>\n",
       "      <td>001</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>m</td>\n",
       "      <td>001</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3310</th>\n",
       "      <td>w</td>\n",
       "      <td>060</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3311</th>\n",
       "      <td>w</td>\n",
       "      <td>060</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3312</th>\n",
       "      <td>w</td>\n",
       "      <td>060</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3313</th>\n",
       "      <td>w</td>\n",
       "      <td>060</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3314</th>\n",
       "      <td>w</td>\n",
       "      <td>060</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3315 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     gender  seq type\n",
       "0         m  001    1\n",
       "1         m  001   10\n",
       "2         m  001   11\n",
       "3         m  001   12\n",
       "4         m  001   13\n",
       "...     ...  ...  ...\n",
       "3310      w  060    5\n",
       "3311      w  060    6\n",
       "3312      w  060    7\n",
       "3313      w  060    8\n",
       "3314      w  060    9\n",
       "\n",
       "[3315 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_names_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace genre man and woman to 0 and 1\n",
    "file_names_list['gender'] = file_names_list['gender'].replace('m', 0)\n",
    "file_names_list['gender'] = file_names_list['gender'].replace('w', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace from string to int\n",
    "file_names_list['gender'] = file_names_list['gender'].astype(int)\n",
    "file_names_list['seq'] = file_names_list['seq'].astype(int)\n",
    "file_names_list['type'] = file_names_list['type'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Must pass 2-d input. shape=(3315, 57, 76, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-7ce7d814fb11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimage_resized_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_resized_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\rene_\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    495\u001b[0m                 \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 497\u001b[1;33m                 \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_ndarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m         \u001b[1;31m# For data is list-like, or Iterable (will consume into list)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\rene_\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36minit_ndarray\u001b[1;34m(values, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;31m# by definition an array here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[1;31m# the dtypes will be coerced to a single dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m     \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_prep_ndarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\rene_\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36m_prep_ndarray\u001b[1;34m(values, copy)\u001b[0m\n\u001b[0;32m    322\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Must pass 2-d input. shape={values.shape}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Must pass 2-d input. shape=(3315, 57, 76, 3)"
     ]
    }
   ],
   "source": [
    "image_resized_array = pd.DataFrame(image_resized_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##clear memory\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(projected_data.shape)\n",
    "print(file_names_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_data = pd.concat([projected_data, file_names_list], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot concatenate object of type '<class 'numpy.ndarray'>'; only Series and DataFrame objs are valid",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-10deade8806c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimage_resized_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mimage_resized_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_names_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\rene_\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    272\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIndexes\u001b[0m \u001b[0mhave\u001b[0m \u001b[0moverlapping\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m     \"\"\"\n\u001b[1;32m--> 274\u001b[1;33m     op = _Concatenator(\n\u001b[0m\u001b[0;32m    275\u001b[0m         \u001b[0mobjs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\rene_\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    357\u001b[0m                     \u001b[1;34m\"only Series and DataFrame objs are valid\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m                 )\n\u001b[1;32m--> 359\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m             \u001b[1;31m# consolidate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot concatenate object of type '<class 'numpy.ndarray'>'; only Series and DataFrame objs are valid"
     ]
    }
   ],
   "source": [
    "image_resized_array = pd.concat([image_resized_array, file_names_list], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(projected_data.iloc[:, :748].values, projected_data.iloc[:, 749].values, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG. Plot some of the images\n",
    "plt.figure(figsize=(18, 9))\n",
    "\n",
    "num_rows = 4\n",
    "num_cols = 748\n",
    "\n",
    "# plot each of the images in the batch and the associated ground truth labels.\n",
    "for i in range(num_rows*num_cols):\n",
    "    ax = plt.subplot(num_rows, num_cols, i + 1)\n",
    "    plt.imshow(X_train[i,:,:])\n",
    "    ax.title.set_text(y_train[i,0])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Normalize images to the range [0, 1].\n",
    "X_train = X_train.astype(\"float32\") / 255\n",
    "X_test  = X_test.astype(\"float32\") / 255\n",
    "\n",
    "# Change the labels from integer to categorical data.\n",
    "print('Original (integer) label for the first training sample: ', y_train[0])\n",
    "\n",
    "# Convert labels to one-hot encoding.\n",
    "y_train = keras_ml.to_categorical(y_train)\n",
    "y_test  = keras_ml.to_categorical(y_test)\n",
    "\n",
    "print('After conversion to categorical one-hot encoded labels: ', y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(748,)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Single neuron for binary classification\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',  # Using binary_crossentropy for binary classification\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    batch_size=32,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve training results.\n",
    "train_loss = history.history[\"loss\"]\n",
    "train_acc  = history.history[\"accuracy\"]\n",
    "valid_loss = history.history[\"val_loss\"]\n",
    "valid_acc  = history.history[\"val_accuracy\"]\n",
    "\n",
    "keras_ml.plot_results([ train_loss, valid_loss ],\n",
    "            ylabel=\"Loss\",\n",
    "            ylim = [0.0, 5.0],\n",
    "            metric_name=[\"Training Loss\", \"Validation Loss\"],\n",
    "            color=[\"g\", \"b\"]);\n",
    "\n",
    "keras_ml.plot_results([ train_acc, valid_acc ],\n",
    "            ylabel=\"Accuracy\",\n",
    "            ylim = [0.0, 1.0],\n",
    "            metric_name=[\"Training Accuracy\", \"Validation Accuracy\"],\n",
    "            color=[\"g\", \"b\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "learning_rate = 0.0000000001  # This is a commonly used starting point, but you may need to adjust it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(748,)),\n",
    "    Dropout(0.5),  # Adding dropout\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),  # Adding dropout\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Adam optimizer with a custom learning rate\n",
    "optimizer = Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)  # Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=32, epochs=100,  # Increased epochs\n",
    "                    verbose=1, validation_split=0.3, shuffle=True,  # Added shuffle\n",
    "                    callbacks=[early_stopping])  # Added early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve training results.\n",
    "train_loss = history.history[\"loss\"]\n",
    "train_acc  = history.history[\"accuracy\"]\n",
    "valid_loss = history.history[\"val_loss\"]\n",
    "valid_acc  = history.history[\"val_accuracy\"]\n",
    "\n",
    "keras_ml.plot_results([ train_loss, valid_loss ],\n",
    "            ylabel=\"Loss\",\n",
    "            metric_name=[\"Training Loss\", \"Validation Loss\"],\n",
    "            color=[\"g\", \"b\"]);\n",
    "\n",
    "keras_ml.plot_results([ train_acc, valid_acc ],\n",
    "            ylabel=\"Accuracy\",\n",
    "            metric_name=[\"Training Accuracy\", \"Validation Accuracy\"],\n",
    "            color=[\"g\", \"b\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from matplotlib.ticker import (MultipleLocator, FormatStrFormatter)\n",
    "from dataclasses import dataclass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset (CIFAR-10)\n",
    "(X_train, y_train), (X_test, y_test) = keras_ml.cifar10.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Learning set:')\n",
    "print('X: ', end='')\n",
    "print(X_train.shape)\n",
    "print('y: ', end='')\n",
    "print(y_train.shape)\n",
    "\n",
    "print('Test set:')\n",
    "print('X: ', end='')\n",
    "print(X_test.shape)\n",
    "print('y: ', end='')\n",
    "print(y_test.shape)\n",
    "\n",
    "# DEBUG. Plot some of the images\n",
    "plt.figure(figsize=(18, 9))\n",
    "\n",
    "num_rows = 4\n",
    "num_cols = 5\n",
    "\n",
    "# plot each of the images in the batch and the associated ground truth labels.\n",
    "for i in range(num_rows*num_cols):\n",
    "    ax = plt.subplot(num_rows, num_cols, i + 1)\n",
    "    plt.imshow(X_train[i,:,:])\n",
    "    ax.title.set_text(y_train[i,0])\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "# Normalize images to the range [0, 1].\n",
    "X_train = X_train.astype(\"float32\") / 255\n",
    "X_test  = X_test.astype(\"float32\") / 255\n",
    "\n",
    "# Change the labels from integer to categorical data.\n",
    "print('Original (integer) label for the first training sample: ', y_train[0])\n",
    "\n",
    "# Convert labels to one-hot encoding.\n",
    "y_train = keras_ml.to_categorical(y_train)\n",
    "y_test  = keras_ml.to_categorical(y_test)\n",
    "\n",
    "print('After conversion to categorical one-hot encoded labels: ', y_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the model.\n",
    "model = keras_ml.cnn_model()\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'],\n",
    "             )\n",
    "\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    batch_size=32,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_split=.3,\n",
    "                   )\n",
    "\n",
    "\n",
    "# Retrieve training results.\n",
    "train_loss = history.history[\"loss\"]\n",
    "train_acc  = history.history[\"accuracy\"]\n",
    "valid_loss = history.history[\"val_loss\"]\n",
    "valid_acc  = history.history[\"val_accuracy\"]\n",
    "\n",
    "keras_ml.plot_results([ train_loss, valid_loss ],\n",
    "            ylabel=\"Loss\",\n",
    "            ylim = [0.0, 5.0],\n",
    "            metric_name=[\"Training Loss\", \"Validation Loss\"],\n",
    "            color=[\"g\", \"b\"]);\n",
    "\n",
    "keras_ml.plot_results([ train_acc, valid_acc ],\n",
    "            ylabel=\"Accuracy\",\n",
    "            ylim = [0.0, 1.0],\n",
    "            metric_name=[\"Training Accuracy\", \"Validation Accuracy\"],\n",
    "            color=[\"g\", \"b\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "#3th attempt   #############################\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names_list = file_names_list.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  1],\n",
       "       [ 0,  1, 10],\n",
       "       [ 0,  1, 11],\n",
       "       ...,\n",
       "       [ 1, 60,  7],\n",
       "       [ 1, 60,  8],\n",
       "       [ 1, 60,  9]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_names_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_resized_array = np.concatenate((image_resized_array, file_names_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3315, 57, 76, 3)\n",
      "(3315, 3)\n"
     ]
    }
   ],
   "source": [
    "print(image_resized_array.shape)\n",
    "print(file_names_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_names_list[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(image_resized_array, file_names_list[:, 0], test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning set:\n",
      "X: (2320, 57, 76, 3)\n",
      "y: (2320,)\n",
      "Test set:\n",
      "X: (995, 57, 76, 3)\n",
      "y: (995,)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-f28a6f235ba9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_cols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"off\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1296x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def plot_results(metrics, title=None, ylabel=None, ylim=None, metric_name=None, color=None):\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 4))\n",
    "\n",
    "    if not (isinstance(metric_name, list) or isinstance(metric_name, tuple)):\n",
    "        metrics = [metrics,]\n",
    "        metric_name = [metric_name,]\n",
    "\n",
    "    for idx, metric in enumerate(metrics):\n",
    "        ax.plot(metric, color=color[idx])\n",
    "\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.xlim([0, 9])\n",
    "    plt.ylim(ylim)\n",
    "    # Tailor x-axis tick marks\n",
    "    ax.xaxis.set_major_locator(MultipleLocator(5))\n",
    "    ax.xaxis.set_major_formatter(FormatStrFormatter('%d'))\n",
    "    ax.xaxis.set_minor_locator(MultipleLocator(1))\n",
    "    plt.grid(True)\n",
    "    plt.legend(metric_name)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def cnn_model(input_shape=(57, 76, 3)):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    #------------------------------------\n",
    "    # Conv Block 1: 32 Filters, MaxPool.\n",
    "    #------------------------------------\n",
    "    model.add(Conv2D(filters=57, kernel_size=3, padding='same', activation='relu', input_shape=input_shape))\n",
    "    model.add(Conv2D(filters=57, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    #------------------------------------\n",
    "    # Conv Block 2: 64 Filters, MaxPool.\n",
    "    #------------------------------------\n",
    "    model.add(Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    #------------------------------------\n",
    "    # Conv Block 3: 64 Filters, MaxPool.\n",
    "    #------------------------------------\n",
    "    model.add(Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    #------------------------------------\n",
    "    # Flatten the convolutional features.\n",
    "    #------------------------------------\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# ##########################################################################\n",
    "\n",
    "# Load the dataset (CIFAR-10)\n",
    "#(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print('Learning set:')\n",
    "print('X: ', end='')\n",
    "print(X_train.shape)\n",
    "print('y: ', end='')\n",
    "print(y_train.shape)\n",
    "\n",
    "print('Test set:')\n",
    "print('X: ', end='')\n",
    "print(X_test.shape)\n",
    "print('y: ', end='')\n",
    "print(y_test.shape)\n",
    "\n",
    "# DEBUG. Plot some of the images\n",
    "plt.figure(figsize=(18, 9))\n",
    "\n",
    "num_rows = 4\n",
    "num_cols = 5\n",
    "\n",
    "# plot each of the images in the batch and the associated ground truth labels.\n",
    "for i in range(num_rows*num_cols):\n",
    "    ax = plt.subplot(num_rows, num_cols, i + 1)\n",
    "    plt.imshow(X_train[i,:,:])\n",
    "    ax.title.set_text(y_train[i,0])\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "# Normalize images to the range [0, 1].\n",
    "X_train = X_train.astype(\"float32\") / 255\n",
    "X_test  = X_test.astype(\"float32\") / 255\n",
    "\n",
    "# Change the labels from integer to categorical data.\n",
    "print('Original (integer) label for the first training sample: ', y_train[0])\n",
    "\n",
    "# Convert labels to one-hot encoding.\n",
    "y_train = to_categorical(y_train)\n",
    "y_test  = to_categorical(y_test)\n",
    "\n",
    "print('After conversion to categorical one-hot encoded labels: ', y_train[0])\n",
    "\n",
    "# Create the model.\n",
    "model = cnn_model()\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'],\n",
    "             )\n",
    "\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    batch_size=32,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_split=.3,\n",
    "                   )\n",
    "\n",
    "\n",
    "# Retrieve training results.\n",
    "train_loss = history.history[\"loss\"]\n",
    "train_acc  = history.history[\"accuracy\"]\n",
    "valid_loss = history.history[\"val_loss\"]\n",
    "valid_acc  = history.history[\"val_accuracy\"]\n",
    "\n",
    "plot_results([ train_loss, valid_loss ],\n",
    "            ylabel=\"Loss\",\n",
    "            ylim = [0.0, 5.0],\n",
    "            metric_name=[\"Training Loss\", \"Validation Loss\"],\n",
    "            color=[\"g\", \"b\"]);\n",
    "\n",
    "plot_results([ train_acc, valid_acc ],\n",
    "            ylabel=\"Accuracy\",\n",
    "            ylim = [0.0, 1.0],\n",
    "            metric_name=[\"Training Accuracy\", \"Validation Accuracy\"],\n",
    "            color=[\"g\", \"b\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a Gender model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a Glasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a ID model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a Facial expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
