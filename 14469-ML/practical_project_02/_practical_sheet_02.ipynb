{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import uniform\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 4s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rene_\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.871\n",
      "Confusion Matrix: [[ 946    0    3    3    0    4   15    1    8    0]\n",
      " [   0 1089    5    3    1    4    4    0   29    0]\n",
      " [  15   19  846   26   20    0   28   22   49    7]\n",
      " [   5    2   22  880    1   32    8   20   26   14]\n",
      " [   3    8    5    0  865    1   17    2   10   71]\n",
      " [  24   15    7   74   25  650   27   11   42   17]\n",
      " [  20    5   13    2   13   20  879    0    6    0]\n",
      " [   4   38   25    1   13    0    4  889   10   44]\n",
      " [  10   16   13   40   12   22   18   14  809   20]\n",
      " [  14   13   11   12   53   11    1   28   11  855]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Fetch and preprocess the MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Flatten and normalize\n",
    "X_train = X_train.reshape(X_train.shape[0], 28*28) / 255.0\n",
    "X_test = X_test.reshape(X_test.shape[0], 28*28) / 255.0\n",
    "\n",
    "# One-hot encode the labels for softmax regression\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_train_encoded = encoder.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test_encoded = encoder.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Softmax function\n",
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n",
    "\n",
    "# Model training\n",
    "def fit(X, y, n_iters, lr):\n",
    "    n_samples, n_features = X.shape\n",
    "    n_classes = y.shape[1]\n",
    "    weights = np.zeros((n_features, n_classes))\n",
    "    biases = np.zeros((1, n_classes))\n",
    "    losses = []\n",
    "\n",
    "    for _ in range(n_iters):\n",
    "        # Compute model predictions\n",
    "        logits = np.dot(X, weights) + biases\n",
    "        probs = softmax(logits)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = - np.mean(np.sum(y * np.log(probs + 1e-9), axis=1))\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # Gradient computation\n",
    "        dz = probs - y\n",
    "        dw = (1 / n_samples) * np.dot(X.T, dz)\n",
    "        db = (1 / n_samples) * np.sum(dz, axis=0)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        weights -= lr * dw\n",
    "        biases -= lr * db\n",
    "\n",
    "    return weights, biases, losses\n",
    "\n",
    "# Model prediction\n",
    "def predict(X, weights, biases):\n",
    "    logits = np.dot(X, weights) + biases\n",
    "    probs = softmax(logits)\n",
    "    return np.argmax(probs, axis=1)\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_iters = 1000\n",
    "\n",
    "weights, biases, losses = fit(X_train, y_train_encoded, n_iters, learning_rate)\n",
    "\n",
    "# Plotting loss\n",
    "plt.figure()\n",
    "plt.plot(range(n_iters), losses, '-g')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = predict(X_test, weights, biases)\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "print(\"Test accuracy: {0:.3f}\".format(np.sum(np.diag(cm))/np.sum(cm)))\n",
    "print(\"Confusion Matrix:\", cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.141\n",
      "Confusion Matrix: [[243  40  18 143  32  49 292  43  63  57]\n",
      " [181 162 423   3  27   2 174  37  70  56]\n",
      " [ 92  35  76 156  58  13 276  97  40 189]\n",
      " [143  44  26 226  42  10 380  66  35  38]\n",
      " [ 57 189  28 174  42  44 148 140  36 124]\n",
      " [ 43  65  68 132  26  17 412  38  41  50]\n",
      " [123  58  36 244  10   8 270  72  20 117]\n",
      " [ 48 127  99  54  56  91 156 275  75  47]\n",
      " [ 40  79  57  98  16  23 471 102  24  64]\n",
      " [ 42 148  29 183  24  46 296 142  28  71]]\n"
     ]
    }
   ],
   "source": [
    "# Activation functions and their derivatives\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "# Model training with 1 hidden layer\n",
    "def fit(X, y, n_iters, lr):\n",
    "    n_samples, n_features = X.shape\n",
    "    n_hidden_neurons = 10\n",
    "    n_classes = y.shape[1]\n",
    "\n",
    "    # Initialize weights and biases\n",
    "    W1 = np.random.randn(n_features, n_hidden_neurons)\n",
    "    b1 = np.zeros((1, n_hidden_neurons))\n",
    "    W2 = np.random.randn(n_hidden_neurons, n_classes)\n",
    "    b2 = np.zeros((1, n_classes))\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for _ in range(n_iters):\n",
    "        # Feedforward\n",
    "        Z1 = np.dot(X, W1) + b1\n",
    "        A1 = sigmoid(Z1)\n",
    "        Z2 = np.dot(A1, W2) + b2\n",
    "        A2 = softmax(Z2)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = - np.mean(np.sum(y * np.log(A2 + 1e-9), axis=1))\n",
    "        losses.append(loss)\n",
    "\n",
    "        # Backpropagation\n",
    "        dZ2 = A2 - y\n",
    "        dW2 = (1 / n_samples) * np.dot(A1.T, dZ2)\n",
    "        db2 = (1 / n_samples) * np.sum(dZ2, axis=0)\n",
    "        dZ1 = np.dot(dZ2, W2.T) * sigmoid_derivative(Z1)\n",
    "        dW1 = (1 / n_samples) * np.dot(X.T, dZ1)\n",
    "        db1 = (1 / n_samples) * np.sum(dZ1, axis=0)\n",
    "\n",
    "        # Update weights and biases\n",
    "        W2 -= lr * dW2\n",
    "        b2 -= lr * db2\n",
    "        W1 -= lr * dW1\n",
    "        b1 -= lr * db1\n",
    "\n",
    "    return W1, b1, W2, b2, losses\n",
    "\n",
    "# Model prediction with 1 hidden layer\n",
    "def predict(X, W1, b1, W2, b2):\n",
    "    Z1 = np.dot(X, W1) + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    A2 = softmax(Z2)\n",
    "    return np.argmax(A2, axis=1)\n",
    "\n",
    "# ... [Your previous code for data preprocessing]\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_iters = 1000\n",
    "\n",
    "W1, b1, W2, b2, losses = fit(X_train, y_train_encoded, n_iters, learning_rate)\n",
    "\n",
    "# Plotting loss\n",
    "plt.figure()\n",
    "plt.plot(range(n_iters), losses, '-g')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = predict(X_test, W1, b1, W2, b2)\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "print(\"Test accuracy: {0:.3f}\".format(np.sum(np.diag(cm))/np.sum(cm)))\n",
    "print(\"Confusion Matrix:\", cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==========================================#\n",
    "#Start of Practical Sheet 2\n",
    "#==========================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = pd.read_csv(\"C:/temp/mnist/mnist_train.csv\")\n",
    "mnist_test = pd.read_csv(\"C:/temp/mnist/mnist_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns with zeros: 67\n",
      "columns with values: 718\n"
     ]
    }
   ],
   "source": [
    "# columns that contain only zeros\n",
    "empty_columns = mnist_train.columns[(mnist_train == 0).all()]\n",
    "\n",
    "# Counting columns\n",
    "num_empty_columns = len(empty_columns)\n",
    "num_non_empty_columns = mnist_train.shape[1] - num_empty_columns\n",
    "\n",
    "print(f\"columns with zeros: {num_empty_columns}\")\n",
    "print(f\"columns with values: {num_non_empty_columns}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train) = mnist_train.iloc[:,1:].values, mnist_train[\"label\"].values\n",
    "(X_test, y_test) = mnist_test.iloc[:,1:].values, mnist_test[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale X_train\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "\n",
    "# Use the same scaler for X_test\n",
    "X_test_scaled = scaler_X.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rene_\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# encoder instance\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# y_train to one-hot encoded format\n",
    "y_train_encoded = encoder.fit_transform(y_train.reshape(-1, 1))\n",
    "\n",
    "# y_test using the same encoder\n",
    "y_test_encoded = encoder.transform(y_test.reshape(-1, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions and their derivatives\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit with n_iters = 10000 and lr = 0.15\n",
    "def fit(X, y, n_hidden=10, n_iters=1000, lr=0.15):\n",
    "    n_samples, n_features = X.shape\n",
    "    n_classes = y.shape[1]\n",
    "\n",
    "    # Initialize weights and biases\n",
    "    W1 = np.random.randn(n_features, n_hidden)\n",
    "    b1 = np.zeros((1, n_hidden))\n",
    "    W2 = np.random.randn(n_hidden, n_classes)\n",
    "    b2 = np.zeros((1, n_classes))\n",
    "\n",
    "    losses = []\n",
    "\n",
    "# mudar o for  in por while loss nao trocar enquando for 10% alteraçao\n",
    "    for _ in range(n_iters):\n",
    "        # Feedforward\n",
    "        Z1 = np.dot(X, W1) + b1\n",
    "        A1 = sigmoid(Z1)\n",
    "        Z2 = np.dot(A1, W2) + b2\n",
    "        A2 = softmax(Z2)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = -np.mean(np.sum(y * np.log(A2 + 1e-9), axis=1))\n",
    "        losses.append(loss)\n",
    "\n",
    "        # Backpropagation\n",
    "        dZ2 = A2 - y\n",
    "        dW2 = (1 / n_samples) * np.dot(A1.T, dZ2)\n",
    "        db2 = (1 / n_samples) * np.sum(dZ2, axis=0)\n",
    "        dZ1 = np.dot(dZ2, W2.T) * sigmoid_derivative(Z1)\n",
    "        dW1 = (1 / n_samples) * np.dot(X.T, dZ1)\n",
    "        db1 = (1 / n_samples) * np.sum(dZ1, axis=0)\n",
    "\n",
    "        # Update weights and biases\n",
    "        W2 -= lr * dW2\n",
    "        b2 -= lr * db2\n",
    "        W1 -= lr * dW1\n",
    "        b1 -= lr * db1\n",
    "\n",
    "        print(f\"Iteration: {_}\")\n",
    "        print(f\"Loss: {loss}\")\n",
    "        \n",
    "    return W1, b1, W2, b2, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, W1, b1, W2, b2):\n",
    "    Z1 = np.dot(X, W1) + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    A2 = softmax(Z2)\n",
    "    return np.argmax(A2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Loss: 3.101505263005804\n",
      "Iteration: 1\n",
      "Loss: 3.027566119013125\n",
      "Iteration: 2\n",
      "Loss: 2.966044241697464\n",
      "Iteration: 3\n",
      "Loss: 2.9138194269347846\n",
      "Iteration: 4\n",
      "Loss: 2.8687208804796898\n",
      "Iteration: 5\n",
      "Loss: 2.8292099287163324\n",
      "Iteration: 6\n",
      "Loss: 2.794173057437691\n",
      "Iteration: 7\n",
      "Loss: 2.762787119103166\n",
      "Iteration: 8\n",
      "Loss: 2.7344308342582617\n",
      "Iteration: 9\n",
      "Loss: 2.7086259264594905\n",
      "Iteration: 10\n",
      "Loss: 2.6849973556976994\n",
      "Iteration: 11\n",
      "Loss: 2.663246002941844\n",
      "Iteration: 12\n",
      "Loss: 2.643129585619741\n",
      "Iteration: 13\n",
      "Loss: 2.624449096381779\n",
      "Iteration: 14\n",
      "Loss: 2.607039005501641\n",
      "Iteration: 15\n",
      "Loss: 2.5907600674187266\n",
      "Iteration: 16\n",
      "Loss: 2.5754939565384696\n",
      "Iteration: 17\n",
      "Loss: 2.5611392071733183\n",
      "Iteration: 18\n",
      "Loss: 2.547608096898592\n",
      "Iteration: 19\n",
      "Loss: 2.5348242222238593\n",
      "Iteration: 20\n",
      "Loss: 2.522720589506314\n",
      "Iteration: 21\n",
      "Loss: 2.5112380946154604\n",
      "Iteration: 22\n",
      "Loss: 2.5003242998169024\n",
      "Iteration: 23\n",
      "Loss: 2.4899324407718444\n",
      "Iteration: 24\n",
      "Loss: 2.480020613808398\n",
      "Iteration: 25\n",
      "Loss: 2.470551105951382\n",
      "Iteration: 26\n",
      "Loss: 2.4614898391081224\n",
      "Iteration: 27\n",
      "Loss: 2.4528059063240653\n",
      "Iteration: 28\n",
      "Loss: 2.4444711828470567\n",
      "Iteration: 29\n",
      "Loss: 2.4364599983581834\n",
      "Iteration: 30\n",
      "Loss: 2.4287488594777944\n",
      "Iteration: 31\n",
      "Loss: 2.421316213773624\n",
      "Iteration: 32\n",
      "Loss: 2.414142248150059\n",
      "Iteration: 33\n",
      "Loss: 2.4072087158011857\n",
      "Iteration: 34\n",
      "Loss: 2.4004987869499406\n",
      "Iteration: 35\n",
      "Loss: 2.3939969194327038\n",
      "Iteration: 36\n",
      "Loss: 2.3876887458682816\n",
      "Iteration: 37\n",
      "Loss: 2.381560974706323\n",
      "Iteration: 38\n",
      "Loss: 2.375601302908378\n",
      "Iteration: 39\n",
      "Loss: 2.3697983383947236\n",
      "Iteration: 40\n",
      "Loss: 2.3641415307067564\n",
      "Iteration: 41\n",
      "Loss: 2.3586211085999067\n",
      "Iteration: 42\n",
      "Loss: 2.3532280235046033\n",
      "Iteration: 43\n",
      "Loss: 2.3479538979798895\n",
      "Iteration: 44\n",
      "Loss: 2.342790978441394\n",
      "Iteration: 45\n",
      "Loss: 2.337732091576898\n",
      "Iteration: 46\n",
      "Loss: 2.3327706039723783\n",
      "Iteration: 47\n",
      "Loss: 2.327900384562184\n",
      "Iteration: 48\n",
      "Loss: 2.323115769591472\n",
      "Iteration: 49\n",
      "Loss: 2.318411529839572\n",
      "Iteration: 50\n",
      "Loss: 2.313782839901501\n",
      "Iteration: 51\n",
      "Loss: 2.3092252493633167\n",
      "Iteration: 52\n",
      "Loss: 2.3047346557369304\n",
      "Iteration: 53\n",
      "Loss: 2.3003072790429786\n",
      "Iteration: 54\n",
      "Loss: 2.2959396379475385\n",
      "Iteration: 55\n",
      "Loss: 2.2916285273711643\n",
      "Iteration: 56\n",
      "Loss: 2.287370997497757\n",
      "Iteration: 57\n",
      "Loss: 2.283164334117143\n",
      "Iteration: 58\n",
      "Loss: 2.279006040239535\n",
      "Iteration: 59\n",
      "Loss: 2.274893818922911\n",
      "Iteration: 60\n",
      "Loss: 2.2708255572562406\n",
      "Iteration: 61\n",
      "Loss: 2.266799311442762\n",
      "Iteration: 62\n",
      "Loss: 2.262813292928438\n",
      "Iteration: 63\n",
      "Loss: 2.2588658555215537\n",
      "Iteration: 64\n",
      "Loss: 2.2549554834502143\n",
      "Iteration: 65\n",
      "Loss: 2.2510807803054456\n",
      "Iteration: 66\n",
      "Loss: 2.247240458818664\n",
      "Iteration: 67\n",
      "Loss: 2.243433331423561\n",
      "Iteration: 68\n",
      "Loss: 2.2396583015538707\n",
      "Iteration: 69\n",
      "Loss: 2.235914355630121\n",
      "Iteration: 70\n",
      "Loss: 2.2322005556902083\n",
      "Iteration: 71\n",
      "Loss: 2.2285160326205067\n",
      "Iteration: 72\n",
      "Loss: 2.224859979946192\n",
      "Iteration: 73\n",
      "Loss: 2.22123164814145\n",
      "Iteration: 74\n",
      "Loss: 2.2176303394222776\n",
      "Iteration: 75\n",
      "Loss: 2.2140554029866224\n",
      "Iteration: 76\n",
      "Loss: 2.2105062306685883\n",
      "Iteration: 77\n",
      "Loss: 2.206982252975417\n",
      "Iteration: 78\n",
      "Loss: 2.2034829354778314\n",
      "Iteration: 79\n",
      "Loss: 2.2000077755261813\n",
      "Iteration: 80\n",
      "Loss: 2.1965562992665557\n",
      "Iteration: 81\n",
      "Loss: 2.193128058932733\n",
      "Iteration: 82\n",
      "Loss: 2.1897226303913886\n",
      "Iteration: 83\n",
      "Loss: 2.186339610919533\n",
      "Iteration: 84\n",
      "Loss: 2.1829786171945518\n",
      "Iteration: 85\n",
      "Loss: 2.17963928347859\n",
      "Iteration: 86\n",
      "Loss: 2.1763212599803134\n",
      "Iteration: 87\n",
      "Loss: 2.1730242113783045\n",
      "Iteration: 88\n",
      "Loss: 2.169747815491497\n",
      "Iteration: 89\n",
      "Loss: 2.1664917620831865\n",
      "Iteration: 90\n",
      "Loss: 2.163255751786193\n",
      "Iteration: 91\n",
      "Loss: 2.160039495137751\n",
      "Iteration: 92\n",
      "Loss: 2.1568427117136637\n",
      "Iteration: 93\n",
      "Loss: 2.153665129352168\n",
      "Iteration: 94\n",
      "Loss: 2.150506483458787\n",
      "Iteration: 95\n",
      "Loss: 2.1473665163842823\n",
      "Iteration: 96\n",
      "Loss: 2.144244976868555\n",
      "Iteration: 97\n",
      "Loss: 2.1411416195440345\n",
      "Iteration: 98\n",
      "Loss: 2.1380562044927816\n",
      "Iteration: 99\n",
      "Loss: 2.13498849685208\n",
      "Iteration: 100\n",
      "Loss: 2.131938266463873\n",
      "Iteration: 101\n",
      "Loss: 2.1289052875638657\n",
      "Iteration: 102\n",
      "Loss: 2.125889338506567\n",
      "Iteration: 103\n",
      "Loss: 2.122890201522917\n",
      "Iteration: 104\n",
      "Loss: 2.119907662507505\n",
      "Iteration: 105\n",
      "Loss: 2.1169415108326706\n",
      "Iteration: 106\n",
      "Loss: 2.1139915391870234\n",
      "Iteration: 107\n",
      "Loss: 2.1110575434361603\n",
      "Iteration: 108\n",
      "Loss: 2.1081393225035256\n",
      "Iteration: 109\n",
      "Loss: 2.105236678269526\n",
      "Iteration: 110\n",
      "Loss: 2.1023494154871387\n",
      "Iteration: 111\n",
      "Loss: 2.0994773417123804\n",
      "Iteration: 112\n",
      "Loss: 2.0966202672480767\n",
      "Iteration: 113\n",
      "Loss: 2.0937780050994856\n",
      "Iteration: 114\n",
      "Loss: 2.0909503709403885\n",
      "Iteration: 115\n",
      "Loss: 2.0881371830883317\n",
      "Iteration: 116\n",
      "Loss: 2.0853382624877845\n",
      "Iteration: 117\n",
      "Loss: 2.0825534327000255\n",
      "Iteration: 118\n",
      "Loss: 2.0797825198986484\n",
      "Iteration: 119\n",
      "Loss: 2.077025352869637\n",
      "Iteration: 120\n",
      "Loss: 2.0742817630150303\n",
      "Iteration: 121\n",
      "Loss: 2.0715515843592627\n",
      "Iteration: 122\n",
      "Loss: 2.0688346535573383\n",
      "Iteration: 123\n",
      "Loss: 2.066130809904069\n",
      "Iteration: 124\n",
      "Loss: 2.063439895343674\n",
      "Iteration: 125\n",
      "Loss: 2.0607617544791235\n",
      "Iteration: 126\n",
      "Loss: 2.0580962345806575\n",
      "Iteration: 127\n",
      "Loss: 2.0554431855930133\n",
      "Iteration: 128\n",
      "Loss: 2.0528024601409363\n",
      "Iteration: 129\n",
      "Loss: 2.050173913532633\n",
      "Iteration: 130\n",
      "Loss: 2.0475574037608837\n",
      "Iteration: 131\n",
      "Loss: 2.044952791501588\n",
      "Iteration: 132\n",
      "Loss: 2.0423599401095887\n",
      "Iteration: 133\n",
      "Loss: 2.039778715611651\n",
      "Iteration: 134\n",
      "Loss: 2.037208986696537\n",
      "Iteration: 135\n",
      "Loss: 2.0346506247021616\n",
      "Iteration: 136\n",
      "Loss: 2.0321035035998354\n",
      "Iteration: 137\n",
      "Loss: 2.029567499975657\n",
      "Iteration: 138\n",
      "Loss: 2.0270424930091395\n",
      "Iteration: 139\n",
      "Loss: 2.024528364449165\n",
      "Iteration: 140\n",
      "Loss: 2.022024998587412\n",
      "Iteration: 141\n",
      "Loss: 2.0195322822293913\n",
      "Iteration: 142\n",
      "Loss: 2.0170501046632445\n",
      "Iteration: 143\n",
      "Loss: 2.014578357626491\n",
      "Iteration: 144\n",
      "Loss: 2.012116935270868\n",
      "Iteration: 145\n",
      "Loss: 2.0096657341254636\n",
      "Iteration: 146\n",
      "Loss: 2.0072246530583064\n",
      "Iteration: 147\n",
      "Loss: 2.0047935932365823\n",
      "Iteration: 148\n",
      "Loss: 2.0023724580856586\n",
      "Iteration: 149\n",
      "Loss: 1.9999611532470598\n",
      "Iteration: 150\n",
      "Loss: 1.997559586535561\n",
      "Iteration: 151\n",
      "Loss: 1.9951676678955346\n",
      "Iteration: 152\n",
      "Loss: 1.9927853093566874\n",
      "Iteration: 153\n",
      "Loss: 1.9904124249893058\n",
      "Iteration: 154\n",
      "Loss: 1.9880489308591194\n",
      "Iteration: 155\n",
      "Loss: 1.9856947449818807\n",
      "Iteration: 156\n",
      "Loss: 1.9833497872777448\n",
      "Iteration: 157\n",
      "Loss: 1.9810139795255226\n",
      "Iteration: 158\n",
      "Loss: 1.978687245316869\n",
      "Iteration: 159\n",
      "Loss: 1.9763695100104575\n",
      "Iteration: 160\n",
      "Loss: 1.9740607006861832\n",
      "Iteration: 161\n",
      "Loss: 1.9717607460994242\n",
      "Iteration: 162\n",
      "Loss: 1.9694695766353871\n",
      "Iteration: 163\n",
      "Loss: 1.9671871242635564\n",
      "Iteration: 164\n",
      "Loss: 1.9649133224922548\n",
      "Iteration: 165\n",
      "Loss: 1.962648106323331\n",
      "Iteration: 166\n",
      "Loss: 1.960391412206973\n",
      "Iteration: 167\n",
      "Loss: 1.9581431779966558\n",
      "Iteration: 168\n",
      "Loss: 1.9559033429042274\n",
      "Iteration: 169\n",
      "Loss: 1.9536718474551376\n",
      "Iteration: 170\n",
      "Loss: 1.9514486334438166\n",
      "Iteration: 171\n",
      "Loss: 1.9492336438892155\n",
      "Iteration: 172\n",
      "Loss: 1.947026822990524\n",
      "Iteration: 173\n",
      "Loss: 1.9448281160830792\n",
      "Iteration: 174\n",
      "Loss: 1.942637469594498\n",
      "Iteration: 175\n",
      "Loss: 1.940454831001052\n",
      "Iteration: 176\n",
      "Loss: 1.9382801487843346\n",
      "Iteration: 177\n",
      "Loss: 1.936113372388243\n",
      "Iteration: 178\n",
      "Loss: 1.933954452176339\n",
      "Iteration: 179\n",
      "Loss: 1.9318033393896243\n",
      "Iteration: 180\n",
      "Loss: 1.9296599861048012\n",
      "Iteration: 181\n",
      "Loss: 1.9275243451930653\n",
      "Iteration: 182\n",
      "Loss: 1.9253963702795052\n",
      "Iteration: 183\n",
      "Loss: 1.9232760157031652\n",
      "Iteration: 184\n",
      "Loss: 1.9211632364778535\n",
      "Iteration: 185\n",
      "Loss: 1.9190579882537513\n",
      "Iteration: 186\n",
      "Loss: 1.916960227279905\n",
      "Iteration: 187\n",
      "Loss: 1.9148699103676654\n",
      "Iteration: 188\n",
      "Loss: 1.9127869948551486\n",
      "Iteration: 189\n",
      "Loss: 1.9107114385727768\n",
      "Iteration: 190\n",
      "Loss: 1.9086431998099815\n",
      "Iteration: 191\n",
      "Loss: 1.906582237283107\n",
      "Iteration: 192\n",
      "Loss: 1.9045285101045937\n",
      "Iteration: 193\n",
      "Loss: 1.9024819777534825\n",
      "Iteration: 194\n",
      "Loss: 1.900442600047291\n",
      "Iteration: 195\n",
      "Loss: 1.8984103371153038\n",
      "Iteration: 196\n",
      "Loss: 1.896385149373317\n",
      "Iteration: 197\n",
      "Loss: 1.8943669974998603\n",
      "Iteration: 198\n",
      "Loss: 1.892355842413928\n",
      "Iteration: 199\n",
      "Loss: 1.8903516452542282\n",
      "Iteration: 200\n",
      "Loss: 1.8883543673599705\n",
      "Iteration: 201\n",
      "Loss: 1.886363970253182\n",
      "Iteration: 202\n",
      "Loss: 1.8843804156225594\n",
      "Iteration: 203\n",
      "Loss: 1.8824036653088396\n",
      "Iteration: 204\n",
      "Loss: 1.880433681291675\n",
      "Iteration: 205\n",
      "Loss: 1.878470425677984\n",
      "Iteration: 206\n",
      "Loss: 1.8765138606917566\n",
      "Iteration: 207\n",
      "Loss: 1.8745639486652637\n",
      "Iteration: 208\n",
      "Loss: 1.8726206520316429\n",
      "Iteration: 209\n",
      "Loss: 1.8706839333188037\n",
      "Iteration: 210\n",
      "Loss: 1.8687537551446058\n",
      "Iteration: 211\n",
      "Loss: 1.8668300802132498\n",
      "Iteration: 212\n",
      "Loss: 1.8649128713128253\n",
      "Iteration: 213\n",
      "Loss: 1.863002091313947\n",
      "Iteration: 214\n",
      "Loss: 1.8610977031694214\n",
      "Iteration: 215\n",
      "Loss: 1.8591996699148712\n",
      "Iteration: 216\n",
      "Loss: 1.8573079546702447\n",
      "Iteration: 217\n",
      "Loss: 1.85542252064215\n",
      "Iteration: 218\n",
      "Loss: 1.853543331126935\n",
      "Iteration: 219\n",
      "Loss: 1.8516703495144464\n",
      "Iteration: 220\n",
      "Loss: 1.8498035392923973\n",
      "Iteration: 221\n",
      "Loss: 1.8479428640512727\n",
      "Iteration: 222\n",
      "Loss: 1.846088287489711\n",
      "Iteration: 223\n",
      "Loss: 1.8442397734202887\n",
      "Iteration: 224\n",
      "Loss: 1.8423972857756472\n",
      "Iteration: 225\n",
      "Loss: 1.8405607886149107\n",
      "Iteration: 226\n",
      "Loss: 1.8387302461303159\n",
      "Iteration: 227\n",
      "Loss: 1.8369056226540204\n",
      "Iteration: 228\n",
      "Loss: 1.8350868826650224\n",
      "Iteration: 229\n",
      "Loss: 1.833273990796149\n",
      "Iteration: 230\n",
      "Loss: 1.8314669118410705\n",
      "Iteration: 231\n",
      "Loss: 1.829665610761284\n",
      "Iteration: 232\n",
      "Loss: 1.8278700526930478\n",
      "Iteration: 233\n",
      "Loss: 1.8260802029542145\n",
      "Iteration: 234\n",
      "Loss: 1.8242960270509336\n",
      "Iteration: 235\n",
      "Loss: 1.8225174906842034\n",
      "Iteration: 236\n",
      "Loss: 1.8207445597562288\n",
      "Iteration: 237\n",
      "Loss: 1.8189772003765832\n",
      "Iteration: 238\n",
      "Loss: 1.817215378868131\n",
      "Iteration: 239\n",
      "Loss: 1.8154590617727149\n",
      "Iteration: 240\n",
      "Loss: 1.8137082158565752\n",
      "Iteration: 241\n",
      "Loss: 1.811962808115499\n",
      "Iteration: 242\n",
      "Loss: 1.810222805779688\n",
      "Iteration: 243\n",
      "Loss: 1.8084881763183307\n",
      "Iteration: 244\n",
      "Loss: 1.806758887443882\n",
      "Iteration: 245\n",
      "Loss: 1.805034907116037\n",
      "Iteration: 246\n",
      "Loss: 1.803316203545403\n",
      "Iteration: 247\n",
      "Loss: 1.8016027451968695\n",
      "Iteration: 248\n",
      "Loss: 1.7998945007926679\n",
      "Iteration: 249\n",
      "Loss: 1.798191439315133\n",
      "Iteration: 250\n",
      "Loss: 1.7964935300091678\n",
      "Iteration: 251\n",
      "Loss: 1.7948007423844046\n",
      "Iteration: 252\n",
      "Loss: 1.7931130462170883\n",
      "Iteration: 253\n",
      "Loss: 1.791430411551663\n",
      "Iteration: 254\n",
      "Loss: 1.7897528087020926\n",
      "Iteration: 255\n",
      "Loss: 1.7880802082529037\n",
      "Iteration: 256\n",
      "Loss: 1.7864125810599685\n",
      "Iteration: 257\n",
      "Loss: 1.784749898251036\n",
      "Iteration: 258\n",
      "Loss: 1.7830921312260142\n",
      "Iteration: 259\n",
      "Loss: 1.7814392516570197\n",
      "Iteration: 260\n",
      "Loss: 1.7797912314881978\n",
      "Iteration: 261\n",
      "Loss: 1.7781480429353256\n",
      "Iteration: 262\n",
      "Loss: 1.776509658485207\n",
      "Iteration: 263\n",
      "Loss: 1.7748760508948724\n",
      "Iteration: 264\n",
      "Loss: 1.7732471931905842\n",
      "Iteration: 265\n",
      "Loss: 1.7716230586666686\n",
      "Iteration: 266\n",
      "Loss: 1.7700036208841772\n",
      "Iteration: 267\n",
      "Loss: 1.768388853669385\n",
      "Iteration: 268\n",
      "Loss: 1.766778731112146\n",
      "Iteration: 269\n",
      "Loss: 1.765173227564101\n",
      "Iteration: 270\n",
      "Loss: 1.7635723176367595\n",
      "Iteration: 271\n",
      "Loss: 1.7619759761994551\n",
      "Iteration: 272\n",
      "Loss: 1.7603841783771903\n",
      "Iteration: 273\n",
      "Loss: 1.7587968995483743\n",
      "Iteration: 274\n",
      "Loss: 1.7572141153424634\n",
      "Iteration: 275\n",
      "Loss: 1.7556358016375149\n",
      "Iteration: 276\n",
      "Loss: 1.7540619345576556\n",
      "Iteration: 277\n",
      "Loss: 1.7524924904704782\n",
      "Iteration: 278\n",
      "Loss: 1.7509274459843662\n",
      "Iteration: 279\n",
      "Loss: 1.74936677794576\n",
      "Iteration: 280\n",
      "Loss: 1.7478104634363645\n",
      "Iteration: 281\n",
      "Loss: 1.7462584797703042\n",
      "Iteration: 282\n",
      "Loss: 1.7447108044912338\n",
      "Iteration: 283\n",
      "Loss: 1.7431674153694054\n",
      "Iteration: 284\n",
      "Loss: 1.7416282903986937\n",
      "Iteration: 285\n",
      "Loss: 1.7400934077935895\n",
      "Iteration: 286\n",
      "Loss: 1.738562745986159\n",
      "Iteration: 287\n",
      "Loss: 1.7370362836229685\n",
      "Iteration: 288\n",
      "Loss: 1.7355139995619864\n",
      "Iteration: 289\n",
      "Loss: 1.7339958728694522\n",
      "Iteration: 290\n",
      "Loss: 1.7324818828167243\n",
      "Iteration: 291\n",
      "Loss: 1.7309720088770997\n",
      "Iteration: 292\n",
      "Loss: 1.7294662307226125\n",
      "Iteration: 293\n",
      "Loss: 1.7279645282208087\n",
      "Iteration: 294\n",
      "Loss: 1.7264668814314992\n",
      "Iteration: 295\n",
      "Loss: 1.7249732706034957\n",
      "Iteration: 296\n",
      "Loss: 1.7234836761713224\n",
      "Iteration: 297\n",
      "Loss: 1.7219980787519162\n",
      "Iteration: 298\n",
      "Loss: 1.72051645914131\n",
      "Iteration: 299\n",
      "Loss: 1.7190387983113022\n",
      "Iteration: 300\n",
      "Loss: 1.7175650774061197\n",
      "Iteration: 301\n",
      "Loss: 1.716095277739074\n",
      "Iteration: 302\n",
      "Loss: 1.7146293807892186\n",
      "Iteration: 303\n",
      "Loss: 1.7131673681980064\n",
      "Iteration: 304\n",
      "Loss: 1.7117092217659626\n",
      "Iteration: 305\n",
      "Loss: 1.7102549234493702\n",
      "Iteration: 306\n",
      "Loss: 1.70880445535698\n",
      "Iteration: 307\n",
      "Loss: 1.7073577997467537\n",
      "Iteration: 308\n",
      "Loss: 1.7059149390226453\n",
      "Iteration: 309\n",
      "Loss: 1.7044758557314361\n",
      "Iteration: 310\n",
      "Loss: 1.7030405325596203\n",
      "Iteration: 311\n",
      "Loss: 1.701608952330369\n",
      "Iteration: 312\n",
      "Loss: 1.7001810980005665\n",
      "Iteration: 313\n",
      "Loss: 1.69875695265794\n",
      "Iteration: 314\n",
      "Loss: 1.6973364995182878\n",
      "Iteration: 315\n",
      "Loss: 1.6959197219228164\n",
      "Iteration: 316\n",
      "Loss: 1.6945066033356\n",
      "Iteration: 317\n",
      "Loss: 1.6930971273411686\n",
      "Iteration: 318\n",
      "Loss: 1.6916912776422321\n",
      "Iteration: 319\n",
      "Loss: 1.6902890380575544\n",
      "Iteration: 320\n",
      "Loss: 1.6888903925199785\n",
      "Iteration: 321\n",
      "Loss: 1.6874953250746094\n",
      "Iteration: 322\n",
      "Loss: 1.6861038198771665\n",
      "Iteration: 323\n",
      "Loss: 1.684715861192497\n",
      "Iteration: 324\n",
      "Loss: 1.6833314333932616\n",
      "Iteration: 325\n",
      "Loss: 1.681950520958794\n",
      "Iteration: 326\n",
      "Loss: 1.6805731084741218\n",
      "Iteration: 327\n",
      "Loss: 1.6791991806291642\n",
      "Iteration: 328\n",
      "Loss: 1.6778287222180834\n",
      "Iteration: 329\n",
      "Loss: 1.6764617181388004\n",
      "Iteration: 330\n",
      "Loss: 1.6750981533926588\n",
      "Iteration: 331\n",
      "Loss: 1.6737380130842332\n",
      "Iteration: 332\n",
      "Loss: 1.6723812824212694\n",
      "Iteration: 333\n",
      "Loss: 1.6710279467147493\n",
      "Iteration: 334\n",
      "Loss: 1.669677991379069\n",
      "Iteration: 335\n",
      "Loss: 1.6683314019323134\n",
      "Iteration: 336\n",
      "Loss: 1.6669881639966222\n",
      "Iteration: 337\n",
      "Loss: 1.6656482632986283\n",
      "Iteration: 338\n",
      "Loss: 1.664311685669956\n",
      "Iteration: 339\n",
      "Loss: 1.66297841704777\n",
      "Iteration: 340\n",
      "Loss: 1.6616484434753573\n",
      "Iteration: 341\n",
      "Loss: 1.6603217511027282\n",
      "Iteration: 342\n",
      "Loss: 1.6589983261872305\n",
      "Iteration: 343\n",
      "Loss: 1.6576781550941584\n",
      "Iteration: 344\n",
      "Loss: 1.6563612242973476\n",
      "Iteration: 345\n",
      "Loss: 1.655047520379747\n",
      "Iteration: 346\n",
      "Loss: 1.6537370300339547\n",
      "Iteration: 347\n",
      "Loss: 1.6524297400627153\n",
      "Iteration: 348\n",
      "Loss: 1.6511256373793641\n",
      "Iteration: 349\n",
      "Loss: 1.6498247090082203\n",
      "Iteration: 350\n",
      "Loss: 1.6485269420849167\n",
      "Iteration: 351\n",
      "Loss: 1.6472323238566653\n",
      "Iteration: 352\n",
      "Loss: 1.6459408416824575\n",
      "Iteration: 353\n",
      "Loss: 1.6446524830331948\n",
      "Iteration: 354\n",
      "Loss: 1.643367235491747\n",
      "Iteration: 355\n",
      "Loss: 1.642085086752945\n",
      "Iteration: 356\n",
      "Loss: 1.6408060246234972\n",
      "Iteration: 357\n",
      "Loss: 1.6395300370218435\n",
      "Iteration: 358\n",
      "Loss: 1.6382571119779386\n",
      "Iteration: 359\n",
      "Loss: 1.6369872376329706\n",
      "Iteration: 360\n",
      "Loss: 1.6357204022390157\n",
      "Iteration: 361\n",
      "Loss: 1.6344565941586326\n",
      "Iteration: 362\n",
      "Loss: 1.6331958018643953\n",
      "Iteration: 363\n",
      "Loss: 1.6319380139383672\n",
      "Iteration: 364\n",
      "Loss: 1.6306832190715201\n",
      "Iteration: 365\n",
      "Loss: 1.6294314060630932\n",
      "Iteration: 366\n",
      "Loss: 1.6281825638198966\n",
      "Iteration: 367\n",
      "Loss: 1.626936681355556\n",
      "Iteration: 368\n",
      "Loss: 1.6256937477896962\n",
      "Iteration: 369\n",
      "Loss: 1.6244537523470646\n",
      "Iteration: 370\n",
      "Loss: 1.6232166843565845\n",
      "Iteration: 371\n",
      "Loss: 1.6219825332503404\n",
      "Iteration: 372\n",
      "Loss: 1.6207512885624855\n",
      "Iteration: 373\n",
      "Loss: 1.6195229399280688\n",
      "Iteration: 374\n",
      "Loss: 1.6182974770817702\n",
      "Iteration: 375\n",
      "Loss: 1.6170748898565406\n",
      "Iteration: 376\n",
      "Loss: 1.6158551681821363\n",
      "Iteration: 377\n",
      "Loss: 1.6146383020835429\n",
      "Iteration: 378\n",
      "Loss: 1.613424281679272\n",
      "Iteration: 379\n",
      "Loss: 1.612213097179533\n",
      "Iteration: 380\n",
      "Loss: 1.6110047388842617\n",
      "Iteration: 381\n",
      "Loss: 1.6097991971810066\n",
      "Iteration: 382\n",
      "Loss: 1.6085964625426572\n",
      "Iteration: 383\n",
      "Loss: 1.6073965255250144\n",
      "Iteration: 384\n",
      "Loss: 1.6061993767641929\n",
      "Iteration: 385\n",
      "Loss: 1.6050050069738548\n",
      "Iteration: 386\n",
      "Loss: 1.6038134069422707\n",
      "Iteration: 387\n",
      "Loss: 1.6026245675292068\n",
      "Iteration: 388\n",
      "Loss: 1.6014384796626375\n",
      "Iteration: 389\n",
      "Loss: 1.6002551343352909\n",
      "Iteration: 390\n",
      "Loss: 1.5990745226010286\n",
      "Iteration: 391\n",
      "Loss: 1.597896635571063\n",
      "Iteration: 392\n",
      "Loss: 1.5967214644100347\n",
      "Iteration: 393\n",
      "Loss: 1.5955490003319446\n",
      "Iteration: 394\n",
      "Loss: 1.5943792345959684\n",
      "Iteration: 395\n",
      "Loss: 1.5932121585021626\n",
      "Iteration: 396\n",
      "Loss: 1.5920477633870806\n",
      "Iteration: 397\n",
      "Loss: 1.5908860406193228\n",
      "Iteration: 398\n",
      "Loss: 1.5897269815950388\n",
      "Iteration: 399\n",
      "Loss: 1.5885705777334087\n",
      "Iteration: 400\n",
      "Loss: 1.5874168204721275\n",
      "Iteration: 401\n",
      "Loss: 1.5862657012629133\n",
      "Iteration: 402\n",
      "Loss: 1.5851172115670835\n",
      "Iteration: 403\n",
      "Loss: 1.5839713428512068\n",
      "Iteration: 404\n",
      "Loss: 1.5828280865828726\n",
      "Iteration: 405\n",
      "Loss: 1.5816874342266014\n",
      "Iteration: 406\n",
      "Loss: 1.5805493772399306\n",
      "Iteration: 407\n",
      "Loss: 1.5794139070696946\n",
      "Iteration: 408\n",
      "Loss: 1.578281015148534\n",
      "Iteration: 409\n",
      "Loss: 1.577150692891658\n",
      "Iteration: 410\n",
      "Loss: 1.5760229316938836\n",
      "Iteration: 411\n",
      "Loss: 1.574897722926976\n",
      "Iteration: 412\n",
      "Loss: 1.5737750579373129\n",
      "Iteration: 413\n",
      "Loss: 1.5726549280438893\n",
      "Iteration: 414\n",
      "Loss: 1.5715373245366817\n",
      "Iteration: 415\n",
      "Loss: 1.5704222386753883\n",
      "Iteration: 416\n",
      "Loss: 1.5693096616885516\n",
      "Iteration: 417\n",
      "Loss: 1.568199584773079\n",
      "Iteration: 418\n",
      "Loss: 1.5670919990941676\n",
      "Iteration: 419\n",
      "Loss: 1.56598689578563\n",
      "Iteration: 420\n",
      "Loss: 1.5648842659506326\n",
      "Iteration: 421\n",
      "Loss: 1.5637841006628395\n",
      "Iteration: 422\n",
      "Loss: 1.5626863909679536\n",
      "Iteration: 423\n",
      "Loss: 1.5615911278856576\n",
      "Iteration: 424\n",
      "Loss: 1.5604983024119394\n",
      "Iteration: 425\n",
      "Loss: 1.5594079055217855\n",
      "Iteration: 426\n",
      "Loss: 1.5583199281722404\n",
      "Iteration: 427\n",
      "Loss: 1.5572343613058015\n",
      "Iteration: 428\n",
      "Loss: 1.556151195854143\n",
      "Iteration: 429\n",
      "Loss: 1.5550704227421366\n",
      "Iteration: 430\n",
      "Loss: 1.5539920328921588\n",
      "Iteration: 431\n",
      "Loss: 1.5529160172286538\n",
      "Iteration: 432\n",
      "Loss: 1.5518423666829244\n",
      "Iteration: 433\n",
      "Loss: 1.5507710721981365\n",
      "Iteration: 434\n",
      "Loss: 1.5497021247344986\n",
      "Iteration: 435\n",
      "Loss: 1.5486355152745923\n",
      "Iteration: 436\n",
      "Loss: 1.5475712348288315\n",
      "Iteration: 437\n",
      "Loss: 1.546509274441015\n",
      "Iteration: 438\n",
      "Loss: 1.545449625193943\n",
      "Iteration: 439\n",
      "Loss: 1.5443922782150803\n",
      "Iteration: 440\n",
      "Loss: 1.5433372246822223\n",
      "Iteration: 441\n",
      "Loss: 1.5422844558291484\n",
      "Iteration: 442\n",
      "Loss: 1.5412339629512288\n",
      "Iteration: 443\n",
      "Loss: 1.540185737410963\n",
      "Iteration: 444\n",
      "Loss: 1.539139770643416\n",
      "Iteration: 445\n",
      "Loss: 1.5380960541615352\n",
      "Iteration: 446\n",
      "Loss: 1.5370545795613206\n",
      "Iteration: 447\n",
      "Loss: 1.5360153385268238\n",
      "Iteration: 448\n",
      "Loss: 1.534978322834955\n",
      "Iteration: 449\n",
      "Loss: 1.5339435243600765\n",
      "Iteration: 450\n",
      "Loss: 1.532910935078365\n",
      "Iteration: 451\n",
      "Loss: 1.5318805470719203\n",
      "Iteration: 452\n",
      "Loss: 1.5308523525326105\n",
      "Iteration: 453\n",
      "Loss: 1.5298263437656283\n",
      "Iteration: 454\n",
      "Loss: 1.5288025131927527\n",
      "Iteration: 455\n",
      "Loss: 1.5277808533553012\n",
      "Iteration: 456\n",
      "Loss: 1.5267613569167593\n",
      "Iteration: 457\n",
      "Loss: 1.5257440166650804\n",
      "Iteration: 458\n",
      "Loss: 1.5247288255146443\n",
      "Iteration: 459\n",
      "Loss: 1.523715776507876\n",
      "Iteration: 460\n",
      "Loss: 1.5227048628165067\n",
      "Iteration: 461\n",
      "Loss: 1.5216960777424897\n",
      "Iteration: 462\n",
      "Loss: 1.5206894147185548\n",
      "Iteration: 463\n",
      "Loss: 1.5196848673084093\n",
      "Iteration: 464\n",
      "Loss: 1.518682429206584\n",
      "Iteration: 465\n",
      "Loss: 1.5176820942379294\n",
      "Iteration: 466\n",
      "Loss: 1.5166838563567606\n",
      "Iteration: 467\n",
      "Loss: 1.5156877096456611\n",
      "Iteration: 468\n",
      "Loss: 1.5146936483139528\n",
      "Iteration: 469\n",
      "Loss: 1.5137016666958325\n",
      "Iteration: 470\n",
      "Loss: 1.5127117592481965\n",
      "Iteration: 471\n",
      "Loss: 1.5117239205481492\n",
      "Iteration: 472\n",
      "Loss: 1.5107381452902213\n",
      "Iteration: 473\n",
      "Loss: 1.5097544282832973\n",
      "Iteration: 474\n",
      "Loss: 1.508772764447273\n",
      "Iteration: 475\n",
      "Loss: 1.5077931488094565\n",
      "Iteration: 476\n",
      "Loss: 1.5068155765007238\n",
      "Iteration: 477\n",
      "Loss: 1.505840042751447\n",
      "Iteration: 478\n",
      "Loss: 1.504866542887213\n",
      "Iteration: 479\n",
      "Loss: 1.5038950723243458\n",
      "Iteration: 480\n",
      "Loss: 1.5029256265652509\n",
      "Iteration: 481\n",
      "Loss: 1.5019582011935997\n",
      "Iteration: 482\n",
      "Loss: 1.5009927918693737\n",
      "Iteration: 483\n",
      "Loss: 1.5000293943237812\n",
      "Iteration: 484\n",
      "Loss: 1.4990680043540707\n",
      "Iteration: 485\n",
      "Loss: 1.498108617818259\n",
      "Iteration: 486\n",
      "Loss: 1.4971512306297852\n",
      "Iteration: 487\n",
      "Loss: 1.496195838752122\n",
      "Iteration: 488\n",
      "Loss: 1.495242438193352\n",
      "Iteration: 489\n",
      "Loss: 1.4942910250007322\n",
      "Iteration: 490\n",
      "Loss: 1.493341595255268\n",
      "Iteration: 491\n",
      "Loss: 1.4923941450663063\n",
      "Iteration: 492\n",
      "Loss: 1.4914486705661805\n",
      "Iteration: 493\n",
      "Loss: 1.4905051679049075\n",
      "Iteration: 494\n",
      "Loss: 1.4895636332449693\n",
      "Iteration: 495\n",
      "Loss: 1.4886240627561906\n",
      "Iteration: 496\n",
      "Loss: 1.4876864526107276\n",
      "Iteration: 497\n",
      "Loss: 1.4867507989781863\n",
      "Iteration: 498\n",
      "Loss: 1.485817098020887\n",
      "Iteration: 499\n",
      "Loss: 1.4848853458892923\n",
      "Iteration: 500\n",
      "Loss: 1.4839555387176033\n",
      "Iteration: 501\n",
      "Loss: 1.4830276726195513\n",
      "Iteration: 502\n",
      "Loss: 1.4821017436843884\n",
      "Iteration: 503\n",
      "Loss: 1.481177747973092\n",
      "Iteration: 504\n",
      "Loss: 1.4802556815147927\n",
      "Iteration: 505\n",
      "Loss: 1.4793355403034418\n",
      "Iteration: 506\n",
      "Loss: 1.478417320294719\n",
      "Iteration: 507\n",
      "Loss: 1.477501017403189\n",
      "Iteration: 508\n",
      "Loss: 1.476586627499722\n",
      "Iteration: 509\n",
      "Loss: 1.475674146409172\n",
      "Iteration: 510\n",
      "Loss: 1.4747635699083261\n",
      "Iteration: 511\n",
      "Loss: 1.4738548937241216\n",
      "Iteration: 512\n",
      "Loss: 1.4729481135321305\n",
      "Iteration: 513\n",
      "Loss: 1.472043224955321\n",
      "Iteration: 514\n",
      "Loss: 1.4711402235630828\n",
      "Iteration: 515\n",
      "Loss: 1.4702391048705206\n",
      "Iteration: 516\n",
      "Loss: 1.469339864338008\n",
      "Iteration: 517\n",
      "Loss: 1.4684424973710015\n",
      "Iteration: 518\n",
      "Loss: 1.4675469993200965\n",
      "Iteration: 519\n",
      "Loss: 1.4666533654813334\n",
      "Iteration: 520\n",
      "Loss: 1.4657615910967314\n",
      "Iteration: 521\n",
      "Loss: 1.4648716713550438\n",
      "Iteration: 522\n",
      "Loss: 1.4639836013927288\n",
      "Iteration: 523\n",
      "Loss: 1.463097376295116\n",
      "Iteration: 524\n",
      "Loss: 1.462212991097764\n",
      "Iteration: 525\n",
      "Loss: 1.4613304407879877\n",
      "Iteration: 526\n",
      "Loss: 1.460449720306547\n",
      "Iteration: 527\n",
      "Loss: 1.4595708245494845\n",
      "Iteration: 528\n",
      "Loss: 1.4586937483700888\n",
      "Iteration: 529\n",
      "Loss: 1.4578184865809816\n",
      "Iteration: 530\n",
      "Loss: 1.4569450339563008\n",
      "Iteration: 531\n",
      "Loss: 1.4560733852339747\n",
      "Iteration: 532\n",
      "Loss: 1.4552035351180688\n",
      "Iteration: 533\n",
      "Loss: 1.4543354782811895\n",
      "Iteration: 534\n",
      "Loss: 1.4534692093669357\n",
      "Iteration: 535\n",
      "Loss: 1.4526047229923797\n",
      "Iteration: 536\n",
      "Loss: 1.4517420137505632\n",
      "Iteration: 537\n",
      "Loss: 1.4508810762130073\n",
      "Iteration: 538\n",
      "Loss: 1.4500219049322063\n",
      "Iteration: 539\n",
      "Loss: 1.4491644944441122\n",
      "Iteration: 540\n",
      "Loss: 1.4483088392705874\n",
      "Iteration: 541\n",
      "Loss: 1.4474549339218192\n",
      "Iteration: 542\n",
      "Loss: 1.4466027728986899\n",
      "Iteration: 543\n",
      "Loss: 1.4457523506950918\n",
      "Iteration: 544\n",
      "Loss: 1.4449036618001785\n",
      "Iteration: 545\n",
      "Loss: 1.4440567007005496\n",
      "Iteration: 546\n",
      "Loss: 1.4432114618823593\n",
      "Iteration: 547\n",
      "Loss: 1.4423679398333482\n",
      "Iteration: 548\n",
      "Loss: 1.4415261290447898\n",
      "Iteration: 549\n",
      "Loss: 1.4406860240133503\n",
      "Iteration: 550\n",
      "Loss: 1.4398476192428609\n",
      "Iteration: 551\n",
      "Loss: 1.4390109092459973\n",
      "Iteration: 552\n",
      "Loss: 1.4381758885458682\n",
      "Iteration: 553\n",
      "Loss: 1.4373425516775094\n",
      "Iteration: 554\n",
      "Loss: 1.436510893189283\n",
      "Iteration: 555\n",
      "Loss: 1.435680907644191\n",
      "Iteration: 556\n",
      "Loss: 1.4348525896210886\n",
      "Iteration: 557\n",
      "Loss: 1.4340259337158134\n",
      "Iteration: 558\n",
      "Loss: 1.4332009345422232\n",
      "Iteration: 559\n",
      "Loss: 1.4323775867331499\n",
      "Iteration: 560\n",
      "Loss: 1.4315558849412655\n",
      "Iteration: 561\n",
      "Loss: 1.4307358238398737\n",
      "Iteration: 562\n",
      "Loss: 1.429917398123617\n",
      "Iteration: 563\n",
      "Loss: 1.429100602509114\n",
      "Iteration: 564\n",
      "Loss: 1.4282854317355176\n",
      "Iteration: 565\n",
      "Loss: 1.4274718805650175\n",
      "Iteration: 566\n",
      "Loss: 1.4266599437832617\n",
      "Iteration: 567\n",
      "Loss: 1.4258496161997276\n",
      "Iteration: 568\n",
      "Loss: 1.4250408926480316\n",
      "Iteration: 569\n",
      "Loss: 1.4242337679861798\n",
      "Iteration: 570\n",
      "Loss: 1.4234282370967697\n",
      "Iteration: 571\n",
      "Loss: 1.4226242948871426\n",
      "Iteration: 572\n",
      "Loss: 1.4218219362894882\n",
      "Iteration: 573\n",
      "Loss: 1.4210211562609028\n",
      "Iteration: 574\n",
      "Loss: 1.4202219497834148\n",
      "Iteration: 575\n",
      "Loss: 1.4194243118639593\n",
      "Iteration: 576\n",
      "Loss: 1.4186282375343262\n",
      "Iteration: 577\n",
      "Loss: 1.4178337218510657\n",
      "Iteration: 578\n",
      "Loss: 1.4170407598953687\n",
      "Iteration: 579\n",
      "Loss: 1.4162493467729071\n",
      "Iteration: 580\n",
      "Loss: 1.4154594776136502\n",
      "Iteration: 581\n",
      "Loss: 1.4146711475716516\n",
      "Iteration: 582\n",
      "Loss: 1.4138843518248063\n",
      "Iteration: 583\n",
      "Loss: 1.4130990855745813\n",
      "Iteration: 584\n",
      "Loss: 1.412315344045725\n",
      "Iteration: 585\n",
      "Loss: 1.4115331224859435\n",
      "Iteration: 586\n",
      "Loss: 1.4107524161655607\n",
      "Iteration: 587\n",
      "Loss: 1.40997322037715\n",
      "Iteration: 588\n",
      "Loss: 1.4091955304351431\n",
      "Iteration: 589\n",
      "Loss: 1.4084193416754163\n",
      "Iteration: 590\n",
      "Loss: 1.4076446494548558\n",
      "Iteration: 591\n",
      "Loss: 1.4068714491508971\n",
      "Iteration: 592\n",
      "Loss: 1.4060997361610503\n",
      "Iteration: 593\n",
      "Loss: 1.4053295059023934\n",
      "Iteration: 594\n",
      "Loss: 1.4045607538110556\n",
      "Iteration: 595\n",
      "Loss: 1.4037934753416752\n",
      "Iteration: 596\n",
      "Loss: 1.4030276659668381\n",
      "Iteration: 597\n",
      "Loss: 1.402263321176499\n",
      "Iteration: 598\n",
      "Loss: 1.4015004364773869\n",
      "Iteration: 599\n",
      "Loss: 1.4007390073923884\n",
      "Iteration: 600\n",
      "Loss: 1.3999790294599213\n",
      "Iteration: 601\n",
      "Loss: 1.3992204982332892\n",
      "Iteration: 602\n",
      "Loss: 1.3984634092800257\n",
      "Iteration: 603\n",
      "Loss: 1.397707758181224\n",
      "Iteration: 604\n",
      "Loss: 1.3969535405308589\n",
      "Iteration: 605\n",
      "Loss: 1.3962007519350972\n",
      "Iteration: 606\n",
      "Loss: 1.3954493880116046\n",
      "Iteration: 607\n",
      "Loss: 1.3946994443888416\n",
      "Iteration: 608\n",
      "Loss: 1.3939509167053634\n",
      "Iteration: 609\n",
      "Loss: 1.3932038006091103\n",
      "Iteration: 610\n",
      "Loss: 1.392458091756705\n",
      "Iteration: 611\n",
      "Loss: 1.3917137858127477\n",
      "Iteration: 612\n",
      "Loss: 1.3909708784491204\n",
      "Iteration: 613\n",
      "Loss: 1.3902293653442925\n",
      "Iteration: 614\n",
      "Loss: 1.3894892421826395\n",
      "Iteration: 615\n",
      "Loss: 1.3887505046537714\n",
      "Iteration: 616\n",
      "Loss: 1.3880131484518716\n",
      "Iteration: 617\n",
      "Loss: 1.3872771692750552\n",
      "Iteration: 618\n",
      "Loss: 1.386542562824741\n",
      "Iteration: 619\n",
      "Loss: 1.385809324805042\n",
      "Iteration: 620\n",
      "Loss: 1.3850774509221788\n",
      "Iteration: 621\n",
      "Loss: 1.3843469368839165\n",
      "Iteration: 622\n",
      "Loss: 1.3836177783990198\n",
      "Iteration: 623\n",
      "Loss: 1.3828899711767397\n",
      "Iteration: 624\n",
      "Loss: 1.3821635109263268\n",
      "Iteration: 625\n",
      "Loss: 1.3814383933565686\n",
      "Iteration: 626\n",
      "Loss: 1.3807146141753601\n",
      "Iteration: 627\n",
      "Loss: 1.379992169089305\n",
      "Iteration: 628\n",
      "Loss: 1.3792710538033461\n",
      "Iteration: 629\n",
      "Loss: 1.3785512640204287\n",
      "Iteration: 630\n",
      "Loss: 1.377832795441198\n",
      "Iteration: 631\n",
      "Loss: 1.3771156437637266\n",
      "Iteration: 632\n",
      "Loss: 1.3763998046832802\n",
      "Iteration: 633\n",
      "Loss: 1.37568527389211\n",
      "Iteration: 634\n",
      "Loss: 1.3749720470792848\n",
      "Iteration: 635\n",
      "Loss: 1.3742601199305535\n",
      "Iteration: 636\n",
      "Loss: 1.373549488128241\n",
      "Iteration: 637\n",
      "Loss: 1.3728401473511767\n",
      "Iteration: 638\n",
      "Loss: 1.372132093274653\n",
      "Iteration: 639\n",
      "Loss: 1.3714253215704204\n",
      "Iteration: 640\n",
      "Loss: 1.3707198279067065\n",
      "Iteration: 641\n",
      "Loss: 1.3700156079482722\n",
      "Iteration: 642\n",
      "Loss: 1.3693126573564873\n",
      "Iteration: 643\n",
      "Loss: 1.3686109717894432\n",
      "Iteration: 644\n",
      "Loss: 1.3679105469020865\n",
      "Iteration: 645\n",
      "Loss: 1.3672113783463786\n",
      "Iteration: 646\n",
      "Loss: 1.3665134617714825\n",
      "Iteration: 647\n",
      "Loss: 1.365816792823968\n",
      "Iteration: 648\n",
      "Loss: 1.3651213671480427\n",
      "Iteration: 649\n",
      "Loss: 1.3644271803858015\n",
      "Iteration: 650\n",
      "Loss: 1.3637342281774945\n",
      "Iteration: 651\n",
      "Loss: 1.363042506161813\n",
      "Iteration: 652\n",
      "Loss: 1.362352009976193\n",
      "Iteration: 653\n",
      "Loss: 1.3616627352571304\n",
      "Iteration: 654\n",
      "Loss: 1.3609746776405147\n",
      "Iteration: 655\n",
      "Loss: 1.3602878327619687\n",
      "Iteration: 656\n",
      "Loss: 1.359602196257205\n",
      "Iteration: 657\n",
      "Loss: 1.3589177637623897\n",
      "Iteration: 658\n",
      "Loss: 1.3582345309145119\n",
      "Iteration: 659\n",
      "Loss: 1.3575524933517664\n",
      "Iteration: 660\n",
      "Loss: 1.3568716467139368\n",
      "Iteration: 661\n",
      "Loss: 1.3561919866427872\n",
      "Iteration: 662\n",
      "Loss: 1.355513508782456\n",
      "Iteration: 663\n",
      "Loss: 1.3548362087798533\n",
      "Iteration: 664\n",
      "Loss: 1.3541600822850597\n",
      "Iteration: 665\n",
      "Loss: 1.353485124951728\n",
      "Iteration: 666\n",
      "Loss: 1.3528113324374829\n",
      "Iteration: 667\n",
      "Loss: 1.3521387004043204\n",
      "Iteration: 668\n",
      "Loss: 1.3514672245190074\n",
      "Iteration: 669\n",
      "Loss: 1.350796900453478\n",
      "Iteration: 670\n",
      "Loss: 1.3501277238852265\n",
      "Iteration: 671\n",
      "Loss: 1.349459690497699\n",
      "Iteration: 672\n",
      "Loss: 1.348792795980679\n",
      "Iteration: 673\n",
      "Loss: 1.3481270360306725\n",
      "Iteration: 674\n",
      "Loss: 1.3474624063512812\n",
      "Iteration: 675\n",
      "Loss: 1.3467989026535785\n",
      "Iteration: 676\n",
      "Loss: 1.3461365206564753\n",
      "Iteration: 677\n",
      "Loss: 1.3454752560870793\n",
      "Iteration: 678\n",
      "Loss: 1.344815104681052\n",
      "Iteration: 679\n",
      "Loss: 1.3441560621829554\n",
      "Iteration: 680\n",
      "Loss: 1.3434981243465947\n",
      "Iteration: 681\n",
      "Loss: 1.3428412869353519\n",
      "Iteration: 682\n",
      "Loss: 1.342185545722513\n",
      "Iteration: 683\n",
      "Loss: 1.3415308964915904\n",
      "Iteration: 684\n",
      "Loss: 1.3408773350366352\n",
      "Iteration: 685\n",
      "Loss: 1.3402248571625412\n",
      "Iteration: 686\n",
      "Loss: 1.3395734586853465\n",
      "Iteration: 687\n",
      "Loss: 1.3389231354325228\n",
      "Iteration: 688\n",
      "Loss: 1.3382738832432568\n",
      "Iteration: 689\n",
      "Loss: 1.3376256979687307\n",
      "Iteration: 690\n",
      "Loss: 1.336978575472386\n",
      "Iteration: 691\n",
      "Loss: 1.3363325116301874\n",
      "Iteration: 692\n",
      "Loss: 1.335687502330874\n",
      "Iteration: 693\n",
      "Loss: 1.335043543476206\n",
      "Iteration: 694\n",
      "Loss: 1.3344006309812046\n",
      "Iteration: 695\n",
      "Loss: 1.3337587607743802\n",
      "Iteration: 696\n",
      "Loss: 1.3331179287979578\n",
      "Iteration: 697\n",
      "Loss: 1.3324781310080938\n",
      "Iteration: 698\n",
      "Loss: 1.3318393633750842\n",
      "Iteration: 699\n",
      "Loss: 1.3312016218835672\n",
      "Iteration: 700\n",
      "Loss: 1.3305649025327178\n",
      "Iteration: 701\n",
      "Loss: 1.3299292013364363\n",
      "Iteration: 702\n",
      "Loss: 1.3292945143235293\n",
      "Iteration: 703\n",
      "Loss: 1.3286608375378826\n",
      "Iteration: 704\n",
      "Loss: 1.3280281670386307\n",
      "Iteration: 705\n",
      "Loss: 1.3273964989003153\n",
      "Iteration: 706\n",
      "Loss: 1.3267658292130402\n",
      "Iteration: 707\n",
      "Loss: 1.32613615408262\n",
      "Iteration: 708\n",
      "Loss: 1.3255074696307172\n",
      "Iteration: 709\n",
      "Loss: 1.3248797719949805\n",
      "Iteration: 710\n",
      "Loss: 1.3242530573291702\n",
      "Iteration: 711\n",
      "Loss: 1.3236273218032812\n",
      "Iteration: 712\n",
      "Loss: 1.3230025616036574\n",
      "Iteration: 713\n",
      "Loss: 1.3223787729331007\n",
      "Iteration: 714\n",
      "Loss: 1.3217559520109765\n",
      "Iteration: 715\n",
      "Loss: 1.3211340950733073\n",
      "Iteration: 716\n",
      "Loss: 1.3205131983728655\n",
      "Iteration: 717\n",
      "Loss: 1.3198932581792584\n",
      "Iteration: 718\n",
      "Loss: 1.319274270779006\n",
      "Iteration: 719\n",
      "Loss: 1.3186562324756173\n",
      "Iteration: 720\n",
      "Loss: 1.3180391395896547\n",
      "Iteration: 721\n",
      "Loss: 1.3174229884587971\n",
      "Iteration: 722\n",
      "Loss: 1.3168077754378968\n",
      "Iteration: 723\n",
      "Loss: 1.3161934968990305\n",
      "Iteration: 724\n",
      "Loss: 1.3155801492315427\n",
      "Iteration: 725\n",
      "Loss: 1.3149677288420865\n",
      "Iteration: 726\n",
      "Loss: 1.3143562321546587\n",
      "Iteration: 727\n",
      "Loss: 1.3137456556106288\n",
      "Iteration: 728\n",
      "Loss: 1.3131359956687614\n",
      "Iteration: 729\n",
      "Loss: 1.312527248805238\n",
      "Iteration: 730\n",
      "Loss: 1.3119194115136668\n",
      "Iteration: 731\n",
      "Loss: 1.3113124803050948\n",
      "Iteration: 732\n",
      "Loss: 1.3107064517080103\n",
      "Iteration: 733\n",
      "Loss: 1.310101322268341\n",
      "Iteration: 734\n",
      "Loss: 1.3094970885494508\n",
      "Iteration: 735\n",
      "Loss: 1.308893747132126\n",
      "Iteration: 736\n",
      "Loss: 1.3082912946145628\n",
      "Iteration: 737\n",
      "Loss: 1.3076897276123465\n",
      "Iteration: 738\n",
      "Loss: 1.3070890427584276\n",
      "Iteration: 739\n",
      "Loss: 1.3064892367030945\n",
      "Iteration: 740\n",
      "Loss: 1.3058903061139402\n",
      "Iteration: 741\n",
      "Loss: 1.3052922476758257\n",
      "Iteration: 742\n",
      "Loss: 1.304695058090839\n",
      "Iteration: 743\n",
      "Loss: 1.3040987340782526\n",
      "Iteration: 744\n",
      "Loss: 1.303503272374473\n",
      "Iteration: 745\n",
      "Loss: 1.3029086697329888\n",
      "Iteration: 746\n",
      "Loss: 1.302314922924317\n",
      "Iteration: 747\n",
      "Loss: 1.3017220287359421\n",
      "Iteration: 748\n",
      "Loss: 1.301129983972255\n",
      "Iteration: 749\n",
      "Loss: 1.3005387854544868\n",
      "Iteration: 750\n",
      "Loss: 1.2999484300206392\n",
      "Iteration: 751\n",
      "Loss: 1.2993589145254145\n",
      "Iteration: 752\n",
      "Loss: 1.298770235840141\n",
      "Iteration: 753\n",
      "Loss: 1.2981823908526935\n",
      "Iteration: 754\n",
      "Loss: 1.2975953764674166\n",
      "Iteration: 755\n",
      "Loss: 1.2970091896050395\n",
      "Iteration: 756\n",
      "Loss: 1.2964238272025927\n",
      "Iteration: 757\n",
      "Loss: 1.2958392862133206\n",
      "Iteration: 758\n",
      "Loss: 1.2952555636065926\n",
      "Iteration: 759\n",
      "Loss: 1.2946726563678126\n",
      "Iteration: 760\n",
      "Loss: 1.2940905614983247\n",
      "Iteration: 761\n",
      "Loss: 1.2935092760153195\n",
      "Iteration: 762\n",
      "Loss: 1.2929287969517382\n",
      "Iteration: 763\n",
      "Loss: 1.2923491213561746\n",
      "Iteration: 764\n",
      "Loss: 1.291770246292775\n",
      "Iteration: 765\n",
      "Loss: 1.2911921688411399\n",
      "Iteration: 766\n",
      "Loss: 1.2906148860962217\n",
      "Iteration: 767\n",
      "Loss: 1.2900383951682222\n",
      "Iteration: 768\n",
      "Loss: 1.2894626931824893\n",
      "Iteration: 769\n",
      "Loss: 1.2888877772794154\n",
      "Iteration: 770\n",
      "Loss: 1.2883136446143286\n",
      "Iteration: 771\n",
      "Loss: 1.2877402923573926\n",
      "Iteration: 772\n",
      "Loss: 1.2871677176934977\n",
      "Iteration: 773\n",
      "Loss: 1.2865959178221567\n",
      "Iteration: 774\n",
      "Loss: 1.2860248899573998\n",
      "Iteration: 775\n",
      "Loss: 1.285454631327667\n",
      "Iteration: 776\n",
      "Loss: 1.284885139175703\n",
      "Iteration: 777\n",
      "Loss: 1.284316410758453\n",
      "Iteration: 778\n",
      "Loss: 1.2837484433469546\n",
      "Iteration: 779\n",
      "Loss: 1.283181234226235\n",
      "Iteration: 780\n",
      "Loss: 1.282614780695204\n",
      "Iteration: 781\n",
      "Loss: 1.2820490800665523\n",
      "Iteration: 782\n",
      "Loss: 1.2814841296666464\n",
      "Iteration: 783\n",
      "Loss: 1.2809199268354254\n",
      "Iteration: 784\n",
      "Loss: 1.2803564689262996\n",
      "Iteration: 785\n",
      "Loss: 1.2797937533060477\n",
      "Iteration: 786\n",
      "Loss: 1.2792317773547182\n",
      "Iteration: 787\n",
      "Loss: 1.2786705384655264\n",
      "Iteration: 788\n",
      "Loss: 1.2781100340447604\n",
      "Iteration: 789\n",
      "Loss: 1.277550261511679\n",
      "Iteration: 790\n",
      "Loss: 1.2769912182984173\n",
      "Iteration: 791\n",
      "Loss: 1.2764329018498912\n",
      "Iteration: 792\n",
      "Loss: 1.2758753096237017\n",
      "Iteration: 793\n",
      "Loss: 1.2753184390900438\n",
      "Iteration: 794\n",
      "Loss: 1.2747622877316143\n",
      "Iteration: 795\n",
      "Loss: 1.2742068530435189\n",
      "Iteration: 796\n",
      "Loss: 1.2736521325331862\n",
      "Iteration: 797\n",
      "Loss: 1.2730981237202788\n",
      "Iteration: 798\n",
      "Loss: 1.272544824136606\n",
      "Iteration: 799\n",
      "Loss: 1.2719922313260392\n",
      "Iteration: 800\n",
      "Loss: 1.2714403428444288\n",
      "Iteration: 801\n",
      "Loss: 1.2708891562595215\n",
      "Iteration: 802\n",
      "Loss: 1.2703386691508796\n",
      "Iteration: 803\n",
      "Loss: 1.269788879109801\n",
      "Iteration: 804\n",
      "Loss: 1.2692397837392433\n",
      "Iteration: 805\n",
      "Loss: 1.2686913806537448\n",
      "Iteration: 806\n",
      "Loss: 1.2681436674793511\n",
      "Iteration: 807\n",
      "Loss: 1.2675966418535414\n",
      "Iteration: 808\n",
      "Loss: 1.2670503014251562\n",
      "Iteration: 809\n",
      "Loss: 1.2665046438543266\n",
      "Iteration: 810\n",
      "Loss: 1.2659596668124056\n",
      "Iteration: 811\n",
      "Loss: 1.2654153679818994\n",
      "Iteration: 812\n",
      "Loss: 1.2648717450564029\n",
      "Iteration: 813\n",
      "Loss: 1.2643287957405318\n",
      "Iteration: 814\n",
      "Loss: 1.2637865177498628\n",
      "Iteration: 815\n",
      "Loss: 1.2632449088108684\n",
      "Iteration: 816\n",
      "Loss: 1.262703966660857\n",
      "Iteration: 817\n",
      "Loss: 1.2621636890479142\n",
      "Iteration: 818\n",
      "Loss: 1.2616240737308437\n",
      "Iteration: 819\n",
      "Loss: 1.2610851184791119\n",
      "Iteration: 820\n",
      "Loss: 1.2605468210727895\n",
      "Iteration: 821\n",
      "Loss: 1.260009179302501\n",
      "Iteration: 822\n",
      "Loss: 1.259472190969368\n",
      "Iteration: 823\n",
      "Loss: 1.2589358538849598\n",
      "Iteration: 824\n",
      "Loss: 1.2584001658712418\n",
      "Iteration: 825\n",
      "Loss: 1.2578651247605268\n",
      "Iteration: 826\n",
      "Loss: 1.2573307283954245\n",
      "Iteration: 827\n",
      "Loss: 1.2567969746287964\n",
      "Iteration: 828\n",
      "Loss: 1.2562638613237094\n",
      "Iteration: 829\n",
      "Loss: 1.2557313863533894\n",
      "Iteration: 830\n",
      "Loss: 1.2551995476011784\n",
      "Iteration: 831\n",
      "Loss: 1.2546683429604903\n",
      "Iteration: 832\n",
      "Loss: 1.2541377703347707\n",
      "Iteration: 833\n",
      "Loss: 1.2536078276374538\n",
      "Iteration: 834\n",
      "Loss: 1.2530785127919228\n",
      "Iteration: 835\n",
      "Loss: 1.2525498237314712\n",
      "Iteration: 836\n",
      "Loss: 1.2520217583992632\n",
      "Iteration: 837\n",
      "Loss: 1.2514943147482984\n",
      "Iteration: 838\n",
      "Loss: 1.2509674907413713\n",
      "Iteration: 839\n",
      "Loss: 1.2504412843510384\n",
      "Iteration: 840\n",
      "Loss: 1.2499156935595834\n",
      "Iteration: 841\n",
      "Loss: 1.2493907163589795\n",
      "Iteration: 842\n",
      "Loss: 1.24886635075086\n",
      "Iteration: 843\n",
      "Loss: 1.2483425947464801\n",
      "Iteration: 844\n",
      "Loss: 1.247819446366691\n",
      "Iteration: 845\n",
      "Loss: 1.2472969036419024\n",
      "Iteration: 846\n",
      "Loss: 1.246774964612055\n",
      "Iteration: 847\n",
      "Loss: 1.246253627326589\n",
      "Iteration: 848\n",
      "Loss: 1.2457328898444147\n",
      "Iteration: 849\n",
      "Loss: 1.2452127502338828\n",
      "Iteration: 850\n",
      "Loss: 1.2446932065727556\n",
      "Iteration: 851\n",
      "Loss: 1.2441742569481802\n",
      "Iteration: 852\n",
      "Loss: 1.2436558994566598\n",
      "Iteration: 853\n",
      "Loss: 1.243138132204027\n",
      "Iteration: 854\n",
      "Loss: 1.2426209533054156\n",
      "Iteration: 855\n",
      "Loss: 1.242104360885238\n",
      "Iteration: 856\n",
      "Loss: 1.2415883530771563\n",
      "Iteration: 857\n",
      "Loss: 1.2410729280240584\n",
      "Iteration: 858\n",
      "Loss: 1.2405580838780321\n",
      "Iteration: 859\n",
      "Loss: 1.2400438188003422\n",
      "Iteration: 860\n",
      "Loss: 1.2395301309614042\n",
      "Iteration: 861\n",
      "Loss: 1.2390170185407634\n",
      "Iteration: 862\n",
      "Loss: 1.238504479727069\n",
      "Iteration: 863\n",
      "Loss: 1.2379925127180516\n",
      "Iteration: 864\n",
      "Loss: 1.2374811157205003\n",
      "Iteration: 865\n",
      "Loss: 1.236970286950242\n",
      "Iteration: 866\n",
      "Loss: 1.2364600246321162\n",
      "Iteration: 867\n",
      "Loss: 1.2359503269999546\n",
      "Iteration: 868\n",
      "Loss: 1.2354411922965596\n",
      "Iteration: 869\n",
      "Loss: 1.2349326187736833\n",
      "Iteration: 870\n",
      "Loss: 1.234424604692003\n",
      "Iteration: 871\n",
      "Loss: 1.2339171483211047\n",
      "Iteration: 872\n",
      "Loss: 1.233410247939459\n",
      "Iteration: 873\n",
      "Loss: 1.2329039018344012\n",
      "Iteration: 874\n",
      "Loss: 1.2323981083021116\n",
      "Iteration: 875\n",
      "Loss: 1.2318928656475938\n",
      "Iteration: 876\n",
      "Loss: 1.2313881721846556\n",
      "Iteration: 877\n",
      "Loss: 1.2308840262358873\n",
      "Iteration: 878\n",
      "Loss: 1.2303804261326445\n",
      "Iteration: 879\n",
      "Loss: 1.2298773702150252\n",
      "Iteration: 880\n",
      "Loss: 1.2293748568318514\n",
      "Iteration: 881\n",
      "Loss: 1.2288728843406493\n",
      "Iteration: 882\n",
      "Loss: 1.2283714511076294\n",
      "Iteration: 883\n",
      "Loss: 1.227870555507667\n",
      "Iteration: 884\n",
      "Loss: 1.2273701959242824\n",
      "Iteration: 885\n",
      "Loss: 1.2268703707496214\n",
      "Iteration: 886\n",
      "Loss: 1.2263710783844368\n",
      "Iteration: 887\n",
      "Loss: 1.2258723172380666\n",
      "Iteration: 888\n",
      "Loss: 1.225374085728416\n",
      "Iteration: 889\n",
      "Loss: 1.2248763822819382\n",
      "Iteration: 890\n",
      "Loss: 1.2243792053336142\n",
      "Iteration: 891\n",
      "Loss: 1.2238825533269317\n",
      "Iteration: 892\n",
      "Loss: 1.2233864247138684\n",
      "Iteration: 893\n",
      "Loss: 1.2228908179548692\n",
      "Iteration: 894\n",
      "Loss: 1.2223957315188294\n",
      "Iteration: 895\n",
      "Loss: 1.2219011638830715\n",
      "Iteration: 896\n",
      "Loss: 1.2214071135333275\n",
      "Iteration: 897\n",
      "Loss: 1.220913578963718\n",
      "Iteration: 898\n",
      "Loss: 1.2204205586767314\n",
      "Iteration: 899\n",
      "Loss: 1.2199280511832045\n",
      "Iteration: 900\n",
      "Loss: 1.219436055002301\n",
      "Iteration: 901\n",
      "Loss: 1.2189445686614913\n",
      "Iteration: 902\n",
      "Loss: 1.2184535906965308\n",
      "Iteration: 903\n",
      "Loss: 1.2179631196514404\n",
      "Iteration: 904\n",
      "Loss: 1.2174731540784836\n",
      "Iteration: 905\n",
      "Loss: 1.2169836925381456\n",
      "Iteration: 906\n",
      "Loss: 1.2164947335991119\n",
      "Iteration: 907\n",
      "Loss: 1.216006275838246\n",
      "Iteration: 908\n",
      "Loss: 1.2155183178405664\n",
      "Iteration: 909\n",
      "Loss: 1.2150308581992262\n",
      "Iteration: 910\n",
      "Loss: 1.2145438955154888\n",
      "Iteration: 911\n",
      "Loss: 1.2140574283987058\n",
      "Iteration: 912\n",
      "Loss: 1.2135714554662926\n",
      "Iteration: 913\n",
      "Loss: 1.2130859753437064\n",
      "Iteration: 914\n",
      "Loss: 1.21260098666442\n",
      "Iteration: 915\n",
      "Loss: 1.2121164880699027\n",
      "Iteration: 916\n",
      "Loss: 1.2116324782095889\n",
      "Iteration: 917\n",
      "Loss: 1.211148955740859\n",
      "Iteration: 918\n",
      "Loss: 1.2106659193290115\n",
      "Iteration: 919\n",
      "Loss: 1.2101833676472382\n",
      "Iteration: 920\n",
      "Loss: 1.2097012993765992\n",
      "Iteration: 921\n",
      "Loss: 1.209219713205996\n",
      "Iteration: 922\n",
      "Loss: 1.208738607832144\n",
      "Iteration: 923\n",
      "Loss: 1.208257981959548\n",
      "Iteration: 924\n",
      "Loss: 1.2077778343004737\n",
      "Iteration: 925\n",
      "Loss: 1.2072981635749194\n",
      "Iteration: 926\n",
      "Loss: 1.20681896851059\n",
      "Iteration: 927\n",
      "Loss: 1.2063402478428662\n",
      "Iteration: 928\n",
      "Loss: 1.2058620003147777\n",
      "Iteration: 929\n",
      "Loss: 1.2053842246769741\n",
      "Iteration: 930\n",
      "Loss: 1.2049069196876934\n",
      "Iteration: 931\n",
      "Loss: 1.2044300841127353\n",
      "Iteration: 932\n",
      "Loss: 1.2039537167254268\n",
      "Iteration: 933\n",
      "Loss: 1.203477816306596\n",
      "Iteration: 934\n",
      "Loss: 1.2030023816445377\n",
      "Iteration: 935\n",
      "Loss: 1.202527411534984\n",
      "Iteration: 936\n",
      "Loss: 1.2020529047810715\n",
      "Iteration: 937\n",
      "Loss: 1.2015788601933088\n",
      "Iteration: 938\n",
      "Loss: 1.2011052765895458\n",
      "Iteration: 939\n",
      "Loss: 1.2006321527949384\n",
      "Iteration: 940\n",
      "Loss: 1.200159487641917\n",
      "Iteration: 941\n",
      "Loss: 1.1996872799701526\n",
      "Iteration: 942\n",
      "Loss: 1.1992155286265218\n",
      "Iteration: 943\n",
      "Loss: 1.1987442324650739\n",
      "Iteration: 944\n",
      "Loss: 1.1982733903469962\n",
      "Iteration: 945\n",
      "Loss: 1.1978030011405778\n",
      "Iteration: 946\n",
      "Loss: 1.1973330637211759\n",
      "Iteration: 947\n",
      "Loss: 1.1968635769711795\n",
      "Iteration: 948\n",
      "Loss: 1.1963945397799736\n",
      "Iteration: 949\n",
      "Loss: 1.1959259510439049\n",
      "Iteration: 950\n",
      "Loss: 1.195457809666243\n",
      "Iteration: 951\n",
      "Loss: 1.1949901145571449\n",
      "Iteration: 952\n",
      "Loss: 1.1945228646336195\n",
      "Iteration: 953\n",
      "Loss: 1.1940560588194902\n",
      "Iteration: 954\n",
      "Loss: 1.1935896960453565\n",
      "Iteration: 955\n",
      "Loss: 1.1931237752485582\n",
      "Iteration: 956\n",
      "Loss: 1.1926582953731386\n",
      "Iteration: 957\n",
      "Loss: 1.1921932553698067\n",
      "Iteration: 958\n",
      "Loss: 1.191728654195898\n",
      "Iteration: 959\n",
      "Loss: 1.1912644908153396\n",
      "Iteration: 960\n",
      "Loss: 1.1908007641986111\n",
      "Iteration: 961\n",
      "Loss: 1.190337473322708\n",
      "Iteration: 962\n",
      "Loss: 1.1898746171711032\n",
      "Iteration: 963\n",
      "Loss: 1.1894121947337095\n",
      "Iteration: 964\n",
      "Loss: 1.188950205006843\n",
      "Iteration: 965\n",
      "Loss: 1.1884886469931857\n",
      "Iteration: 966\n",
      "Loss: 1.1880275197017467\n",
      "Iteration: 967\n",
      "Loss: 1.1875668221478275\n",
      "Iteration: 968\n",
      "Loss: 1.1871065533529825\n",
      "Iteration: 969\n",
      "Loss: 1.1866467123449842\n",
      "Iteration: 970\n",
      "Loss: 1.186187298157786\n",
      "Iteration: 971\n",
      "Loss: 1.1857283098314848\n",
      "Iteration: 972\n",
      "Loss: 1.1852697464122879\n",
      "Iteration: 973\n",
      "Loss: 1.1848116069524728\n",
      "Iteration: 974\n",
      "Loss: 1.184353890510357\n",
      "Iteration: 975\n",
      "Loss: 1.1838965961502588\n",
      "Iteration: 976\n",
      "Loss: 1.1834397229424645\n",
      "Iteration: 977\n",
      "Loss: 1.1829832699631937\n",
      "Iteration: 978\n",
      "Loss: 1.1825272362945651\n",
      "Iteration: 979\n",
      "Loss: 1.1820716210245652\n",
      "Iteration: 980\n",
      "Loss: 1.1816164232470112\n",
      "Iteration: 981\n",
      "Loss: 1.181161642061523\n",
      "Iteration: 982\n",
      "Loss: 1.1807072765734894\n",
      "Iteration: 983\n",
      "Loss: 1.180253325894035\n",
      "Iteration: 984\n",
      "Loss: 1.179799789139994\n",
      "Iteration: 985\n",
      "Loss: 1.1793466654338756\n",
      "Iteration: 986\n",
      "Loss: 1.178893953903836\n",
      "Iteration: 987\n",
      "Loss: 1.1784416536836502\n",
      "Iteration: 988\n",
      "Loss: 1.1779897639126828\n",
      "Iteration: 989\n",
      "Loss: 1.177538283735862\n",
      "Iteration: 990\n",
      "Loss: 1.1770872123036495\n",
      "Iteration: 991\n",
      "Loss: 1.1766365487720185\n",
      "Iteration: 992\n",
      "Loss: 1.1761862923024238\n",
      "Iteration: 993\n",
      "Loss: 1.1757364420617809\n",
      "Iteration: 994\n",
      "Loss: 1.1752869972224405\n",
      "Iteration: 995\n",
      "Loss: 1.1748379569621639\n",
      "Iteration: 996\n",
      "Loss: 1.1743893204641025\n",
      "Iteration: 997\n",
      "Loss: 1.1739410869167752\n",
      "Iteration: 998\n",
      "Loss: 1.1734932555140487\n",
      "Iteration: 999\n",
      "Loss: 1.173045825455114\n"
     ]
    }
   ],
   "source": [
    "#  train model with relative n_iters\n",
    "W1, b1, W2, b2, losses = fit(X_train_scaled, y_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 58.44%\n",
      "Iteration:541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-1d4613750e1e>:3: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "predictions = predict(X_test, W1, b1, W2, b2)\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Iteration:{iteration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================================#\n",
    "# Fit with relative n_iters\n",
    "#============================================#\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "def fit(X, y, n_hidden=10, lr=0.3):\n",
    "    n_samples, n_features = X.shape\n",
    "    n_classes = y.shape[1]\n",
    "\n",
    "    # Initialize weights and biases\n",
    "    W1 = np.random.randn(n_features, n_hidden)\n",
    "    b1 = np.zeros((1, n_hidden))\n",
    "    W2 = np.random.randn(n_hidden, n_classes)\n",
    "    b2 = np.zeros((1, n_classes))\n",
    "\n",
    "    losses = []\n",
    "    prev_loss = float('inf')  # previous loss\n",
    "    prev_loss = 1  # previous loss\n",
    "    relative_changes = deque(maxlen=10)  # Store the last 10 relative changes\n",
    "\n",
    "    iteration = 0  # Counter to track the number of iterations\n",
    "\n",
    "    while True:\n",
    "        # Feedforward\n",
    "        Z1 = np.dot(X, W1) + b1\n",
    "        A1 = sigmoid(Z1)\n",
    "        Z2 = np.dot(A1, W2) + b2\n",
    "        A2 = softmax(Z2)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = -np.mean(np.sum(y * np.log(A2 + 1e-9), axis=1))\n",
    "        losses.append(loss)\n",
    "\n",
    "        # Calculate the relative change in loss if it's not the first iteration\n",
    "        if prev_loss != float('inf'):\n",
    "            relative_change = abs(prev_loss - loss) / prev_loss\n",
    "            relative_changes.append(relative_change)\n",
    "            print(f\"Iteration: {iteration}\")\n",
    "            print(f\"Loss: {loss}\")\n",
    "            print(f\"Relative change: {100 * abs(prev_loss - loss) / prev_loss:.6f}%\")\n",
    "            \n",
    "        # Stop if we've tracked 10 relative changes and the maximum change is less than 10%\n",
    "        if 100 *(abs(prev_loss - loss) / prev_loss) < 0.02:\n",
    "            return W1, b1, W2, b2, losses, iteration\n",
    "            break\n",
    "\n",
    "        # Backpropagation\n",
    "        dZ2 = A2 - y\n",
    "        dW2 = (1 / n_samples) * np.dot(A1.T, dZ2)\n",
    "        db2 = (1 / n_samples) * np.sum(dZ2, axis=0)\n",
    "        dZ1 = np.dot(dZ2, W2.T) * sigmoid_derivative(Z1)\n",
    "        dW1 = (1 / n_samples) * np.dot(X.T, dZ1)\n",
    "        db1 = (1 / n_samples) * np.sum(dZ1, axis=0)\n",
    "\n",
    "        # Update weights and biases\n",
    "        W2 -= lr * dW2\n",
    "        b2 -= lr * db2\n",
    "        W1 -= lr * dW1\n",
    "        b1 -= lr * db1\n",
    "\n",
    "        # Update prev_loss for next iteration\n",
    "        prev_loss = loss\n",
    "        \n",
    "        # Increase the iteration counter\n",
    "        iteration += 1       \n",
    "        \n",
    "\n",
    "    return W1, b1, W2, b2, losses, iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, W1, b1, W2, b2):\n",
    "    Z1 = np.dot(X, W1) + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    A2 = softmax(Z2)\n",
    "    return np.argmax(A2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Loss: 4.339704705437051\n",
      "Relative change: 333.970471%\n",
      "Iteration: 1\n",
      "Loss: 3.998110248897458\n",
      "Relative change: 7.871376%\n",
      "Iteration: 2\n",
      "Loss: 3.713698030686802\n",
      "Relative change: 7.113666%\n",
      "Iteration: 3\n",
      "Loss: 3.4843936500376205\n",
      "Relative change: 6.174556%\n",
      "Iteration: 4\n",
      "Loss: 3.3045795752446048\n",
      "Relative change: 5.160556%\n",
      "Iteration: 5\n",
      "Loss: 3.1656370834112963\n",
      "Relative change: 4.204544%\n",
      "Iteration: 6\n",
      "Loss: 3.0584005862176347\n",
      "Relative change: 3.387517%\n",
      "Iteration: 7\n",
      "Loss: 2.9749198621878445\n",
      "Relative change: 2.729555%\n",
      "Iteration: 8\n",
      "Loss: 2.909025112642457\n",
      "Relative change: 2.215009%\n",
      "Iteration: 9\n",
      "Loss: 2.8561708769718255\n",
      "Relative change: 1.816905%\n",
      "Iteration: 10\n",
      "Loss: 2.8130589490964715\n",
      "Relative change: 1.509431%\n",
      "Iteration: 11\n",
      "Loss: 2.7772871812099686\n",
      "Relative change: 1.271632%\n",
      "Iteration: 12\n",
      "Loss: 2.7470876604644388\n",
      "Relative change: 1.087375%\n",
      "Iteration: 13\n",
      "Loss: 2.72114540512671\n",
      "Relative change: 0.944355%\n",
      "Iteration: 14\n",
      "Loss: 2.698474317852203\n",
      "Relative change: 0.833145%\n",
      "Iteration: 15\n",
      "Loss: 2.6783309973436173\n",
      "Relative change: 0.746471%\n",
      "Iteration: 16\n",
      "Loss: 2.660153376815798\n",
      "Relative change: 0.678692%\n",
      "Iteration: 17\n",
      "Loss: 2.6435159965705646\n",
      "Relative change: 0.625429%\n",
      "Iteration: 18\n",
      "Loss: 2.6280967837352596\n",
      "Relative change: 0.583284%\n",
      "Iteration: 19\n",
      "Loss: 2.613652045225365\n",
      "Relative change: 0.549627%\n",
      "Iteration: 20\n",
      "Loss: 2.599997478375983\n",
      "Relative change: 0.522432%\n",
      "Iteration: 21\n",
      "Loss: 2.5869936851923105\n",
      "Relative change: 0.500146%\n",
      "Iteration: 22\n",
      "Loss: 2.574535114791219\n",
      "Relative change: 0.481585%\n",
      "Iteration: 23\n",
      "Loss: 2.562541645736388\n",
      "Relative change: 0.465850%\n",
      "Iteration: 24\n",
      "Loss: 2.550952209517991\n",
      "Relative change: 0.452263%\n",
      "Iteration: 25\n",
      "Loss: 2.539719986994607\n",
      "Relative change: 0.440315%\n",
      "Iteration: 26\n",
      "Loss: 2.528808807901885\n",
      "Relative change: 0.429621%\n",
      "Iteration: 27\n",
      "Loss: 2.5181904616362774\n",
      "Relative change: 0.419895%\n",
      "Iteration: 28\n",
      "Loss: 2.5078426881334677\n",
      "Relative change: 0.410921%\n",
      "Iteration: 29\n",
      "Loss: 2.4977476653232062\n",
      "Relative change: 0.402538%\n",
      "Iteration: 30\n",
      "Loss: 2.4878908530443256\n",
      "Relative change: 0.394628%\n",
      "Iteration: 31\n",
      "Loss: 2.4782600964332184\n",
      "Relative change: 0.387105%\n",
      "Iteration: 32\n",
      "Loss: 2.46884492930164\n",
      "Relative change: 0.379910%\n",
      "Iteration: 33\n",
      "Loss: 2.4596360424769728\n",
      "Relative change: 0.373004%\n",
      "Iteration: 34\n",
      "Loss: 2.4506248928364367\n",
      "Relative change: 0.366361%\n",
      "Iteration: 35\n",
      "Loss: 2.4418034315800696\n",
      "Relative change: 0.359968%\n",
      "Iteration: 36\n",
      "Loss: 2.4331639306635116\n",
      "Relative change: 0.353816%\n",
      "Iteration: 37\n",
      "Loss: 2.424698885965435\n",
      "Relative change: 0.347903%\n",
      "Iteration: 38\n",
      "Loss: 2.4164009748385924\n",
      "Relative change: 0.342224%\n",
      "Iteration: 39\n",
      "Loss: 2.408263045559836\n",
      "Relative change: 0.336779%\n",
      "Iteration: 40\n",
      "Loss: 2.4002781188347\n",
      "Relative change: 0.331564%\n",
      "Iteration: 41\n",
      "Loss: 2.392439387269342\n",
      "Relative change: 0.326576%\n",
      "Iteration: 42\n",
      "Loss: 2.384740205774221\n",
      "Relative change: 0.321813%\n",
      "Iteration: 43\n",
      "Loss: 2.3771740717673664\n",
      "Relative change: 0.317273%\n",
      "Iteration: 44\n",
      "Loss: 2.369734597421631\n",
      "Relative change: 0.312955%\n",
      "Iteration: 45\n",
      "Loss: 2.3624154771802153\n",
      "Relative change: 0.308858%\n",
      "Iteration: 46\n",
      "Loss: 2.355210453356359\n",
      "Relative change: 0.304985%\n",
      "Iteration: 47\n",
      "Loss: 2.3481132818716044\n",
      "Relative change: 0.301339%\n",
      "Iteration: 48\n",
      "Loss: 2.3411176996843097\n",
      "Relative change: 0.297924%\n",
      "Iteration: 49\n",
      "Loss: 2.3342173954482512\n",
      "Relative change: 0.294744%\n",
      "Iteration: 50\n",
      "Loss: 2.3274059855185896\n",
      "Relative change: 0.291807%\n",
      "Iteration: 51\n",
      "Loss: 2.32067699863835\n",
      "Relative change: 0.289120%\n",
      "Iteration: 52\n",
      "Loss: 2.314023874261595\n",
      "Relative change: 0.286689%\n",
      "Iteration: 53\n",
      "Loss: 2.307439980538705\n",
      "Relative change: 0.284521%\n",
      "Iteration: 54\n",
      "Loss: 2.3009186565034434\n",
      "Relative change: 0.282622%\n",
      "Iteration: 55\n",
      "Loss: 2.2944532765399304\n",
      "Relative change: 0.280991%\n",
      "Iteration: 56\n",
      "Loss: 2.288037323309344\n",
      "Relative change: 0.279629%\n",
      "Iteration: 57\n",
      "Loss: 2.281664442642945\n",
      "Relative change: 0.278530%\n",
      "Iteration: 58\n",
      "Loss: 2.2753284503614397\n",
      "Relative change: 0.277692%\n",
      "Iteration: 59\n",
      "Loss: 2.2690232756880535\n",
      "Relative change: 0.277111%\n",
      "Iteration: 60\n",
      "Loss: 2.2627428584061904\n",
      "Relative change: 0.276789%\n",
      "Iteration: 61\n",
      "Loss: 2.2564810536785394\n",
      "Relative change: 0.276735%\n",
      "Iteration: 62\n",
      "Loss: 2.250231619060073\n",
      "Relative change: 0.276955%\n",
      "Iteration: 63\n",
      "Loss: 2.243988346826797\n",
      "Relative change: 0.277450%\n",
      "Iteration: 64\n",
      "Loss: 2.237745359958703\n",
      "Relative change: 0.278209%\n",
      "Iteration: 65\n",
      "Loss: 2.23149752818877\n",
      "Relative change: 0.279202%\n",
      "Iteration: 66\n",
      "Loss: 2.225240909197404\n",
      "Relative change: 0.280378%\n",
      "Iteration: 67\n",
      "Loss: 2.2189731055202206\n",
      "Relative change: 0.281669%\n",
      "Iteration: 68\n",
      "Loss: 2.2126934589423386\n",
      "Relative change: 0.282998%\n",
      "Iteration: 69\n",
      "Loss: 2.206403061766946\n",
      "Relative change: 0.284287%\n",
      "Iteration: 70\n",
      "Loss: 2.2001046112266867\n",
      "Relative change: 0.285462%\n",
      "Iteration: 71\n",
      "Loss: 2.19380215185531\n",
      "Relative change: 0.286462%\n",
      "Iteration: 72\n",
      "Loss: 2.1875007672781854\n",
      "Relative change: 0.287236%\n",
      "Iteration: 73\n",
      "Loss: 2.181206322878384\n",
      "Relative change: 0.287746%\n",
      "Iteration: 74\n",
      "Loss: 2.1749253761030927\n",
      "Relative change: 0.287957%\n",
      "Iteration: 75\n",
      "Loss: 2.1686652788048546\n",
      "Relative change: 0.287830%\n",
      "Iteration: 76\n",
      "Loss: 2.1624343153950893\n",
      "Relative change: 0.287318%\n",
      "Iteration: 77\n",
      "Loss: 2.156241607801598\n",
      "Relative change: 0.286377%\n",
      "Iteration: 78\n",
      "Loss: 2.150096619946514\n",
      "Relative change: 0.284986%\n",
      "Iteration: 79\n",
      "Loss: 2.1440083705455266\n",
      "Relative change: 0.283162%\n",
      "Iteration: 80\n",
      "Loss: 2.1379846826975584\n",
      "Relative change: 0.280954%\n",
      "Iteration: 81\n",
      "Loss: 2.132031768570289\n",
      "Relative change: 0.278436%\n",
      "Iteration: 82\n",
      "Loss: 2.1261542228997703\n",
      "Relative change: 0.275678%\n",
      "Iteration: 83\n",
      "Loss: 2.1203552980410887\n",
      "Relative change: 0.272742%\n",
      "Iteration: 84\n",
      "Loss: 2.1146372807421994\n",
      "Relative change: 0.269673%\n",
      "Iteration: 85\n",
      "Loss: 2.10900183734914\n",
      "Relative change: 0.266497%\n",
      "Iteration: 86\n",
      "Loss: 2.1034502484804185\n",
      "Relative change: 0.263233%\n",
      "Iteration: 87\n",
      "Loss: 2.0979834909201247\n",
      "Relative change: 0.259895%\n",
      "Iteration: 88\n",
      "Loss: 2.0926021636110894\n",
      "Relative change: 0.256500%\n",
      "Iteration: 89\n",
      "Loss: 2.087306305054963\n",
      "Relative change: 0.253075%\n",
      "Iteration: 90\n",
      "Loss: 2.0820951920838566\n",
      "Relative change: 0.249657%\n",
      "Iteration: 91\n",
      "Loss: 2.0769672170831566\n",
      "Relative change: 0.246289%\n",
      "Iteration: 92\n",
      "Loss: 2.0719199036991194\n",
      "Relative change: 0.243014%\n",
      "Iteration: 93\n",
      "Loss: 2.0669500606479176\n",
      "Relative change: 0.239867%\n",
      "Iteration: 94\n",
      "Loss: 2.0620540232334457\n",
      "Relative change: 0.236873%\n",
      "Iteration: 95\n",
      "Loss: 2.05722791424018\n",
      "Relative change: 0.234044%\n",
      "Iteration: 96\n",
      "Loss: 2.0524678689630083\n",
      "Relative change: 0.231382%\n",
      "Iteration: 97\n",
      "Loss: 2.0477701975487332\n",
      "Relative change: 0.228879%\n",
      "Iteration: 98\n",
      "Loss: 2.0431314847511373\n",
      "Relative change: 0.226525%\n",
      "Iteration: 99\n",
      "Loss: 2.038548642695934\n",
      "Relative change: 0.224305%\n",
      "Iteration: 100\n",
      "Loss: 2.034018934604421\n",
      "Relative change: 0.222203%\n",
      "Iteration: 101\n",
      "Loss: 2.029539981144591\n",
      "Relative change: 0.220202%\n",
      "Iteration: 102\n",
      "Loss: 2.0251097531167397\n",
      "Relative change: 0.218287%\n",
      "Iteration: 103\n",
      "Loss: 2.020726549758708\n",
      "Relative change: 0.216443%\n",
      "Iteration: 104\n",
      "Loss: 2.0163889622856965\n",
      "Relative change: 0.214655%\n",
      "Iteration: 105\n",
      "Loss: 2.012095825322693\n",
      "Relative change: 0.212912%\n",
      "Iteration: 106\n",
      "Loss: 2.0078461617973558\n",
      "Relative change: 0.211206%\n",
      "Iteration: 107\n",
      "Loss: 2.003639127908004\n",
      "Relative change: 0.209530%\n",
      "Iteration: 108\n",
      "Loss: 1.9994739638022174\n",
      "Relative change: 0.207880%\n",
      "Iteration: 109\n",
      "Loss: 1.9953499534129535\n",
      "Relative change: 0.206255%\n",
      "Iteration: 110\n",
      "Loss: 1.9912663945094258\n",
      "Relative change: 0.204654%\n",
      "Iteration: 111\n",
      "Loss: 1.9872225781456931\n",
      "Relative change: 0.203078%\n",
      "Iteration: 112\n",
      "Loss: 1.9832177756371514\n",
      "Relative change: 0.201528%\n",
      "Iteration: 113\n",
      "Loss: 1.979251230941467\n",
      "Relative change: 0.200006%\n",
      "Iteration: 114\n",
      "Loss: 1.9753221566437917\n",
      "Relative change: 0.198513%\n",
      "Iteration: 115\n",
      "Loss: 1.9714297323469507\n",
      "Relative change: 0.197053%\n",
      "Iteration: 116\n",
      "Loss: 1.9675731048741119\n",
      "Relative change: 0.195626%\n",
      "Iteration: 117\n",
      "Loss: 1.963751390129121\n",
      "Relative change: 0.194235%\n",
      "Iteration: 118\n",
      "Loss: 1.9599636766637245\n",
      "Relative change: 0.192882%\n",
      "Iteration: 119\n",
      "Loss: 1.9562090309924296\n",
      "Relative change: 0.191567%\n",
      "Iteration: 120\n",
      "Loss: 1.9524865045442348\n",
      "Relative change: 0.190293%\n",
      "Iteration: 121\n",
      "Loss: 1.9487951419337024\n",
      "Relative change: 0.189060%\n",
      "Iteration: 122\n",
      "Loss: 1.945133990053917\n",
      "Relative change: 0.187867%\n",
      "Iteration: 123\n",
      "Loss: 1.9415021073975383\n",
      "Relative change: 0.186716%\n",
      "Iteration: 124\n",
      "Loss: 1.9378985730191196\n",
      "Relative change: 0.185605%\n",
      "Iteration: 125\n",
      "Loss: 1.9343224946470479\n",
      "Relative change: 0.184534%\n",
      "Iteration: 126\n",
      "Loss: 1.930773015600733\n",
      "Relative change: 0.183500%\n",
      "Iteration: 127\n",
      "Loss: 1.9272493203285883\n",
      "Relative change: 0.182502%\n",
      "Iteration: 128\n",
      "Loss: 1.9237506385244647\n",
      "Relative change: 0.181538%\n",
      "Iteration: 129\n",
      "Loss: 1.9202762478880486\n",
      "Relative change: 0.180605%\n",
      "Iteration: 130\n",
      "Loss: 1.9168254756641192\n",
      "Relative change: 0.179702%\n",
      "Iteration: 131\n",
      "Loss: 1.9133976991305388\n",
      "Relative change: 0.178826%\n",
      "Iteration: 132\n",
      "Loss: 1.909992345213417\n",
      "Relative change: 0.177974%\n",
      "Iteration: 133\n",
      "Loss: 1.9066088893990039\n",
      "Relative change: 0.177145%\n",
      "Iteration: 134\n",
      "Loss: 1.903246854093443\n",
      "Relative change: 0.176336%\n",
      "Iteration: 135\n",
      "Loss: 1.899905806559594\n",
      "Relative change: 0.175545%\n",
      "Iteration: 136\n",
      "Loss: 1.8965853565386575\n",
      "Relative change: 0.174769%\n",
      "Iteration: 137\n",
      "Loss: 1.89328515364533\n",
      "Relative change: 0.174008%\n",
      "Iteration: 138\n",
      "Loss: 1.890004884609179\n",
      "Relative change: 0.173258%\n",
      "Iteration: 139\n",
      "Loss: 1.8867442704216544\n",
      "Relative change: 0.172519%\n",
      "Iteration: 140\n",
      "Loss: 1.8835030634368397\n",
      "Relative change: 0.171788%\n",
      "Iteration: 141\n",
      "Loss: 1.8802810444641769\n",
      "Relative change: 0.171065%\n",
      "Iteration: 142\n",
      "Loss: 1.8770780198825083\n",
      "Relative change: 0.170348%\n",
      "Iteration: 143\n",
      "Loss: 1.873893818796922\n",
      "Relative change: 0.169636%\n",
      "Iteration: 144\n",
      "Loss: 1.8707282902531868\n",
      "Relative change: 0.168928%\n",
      "Iteration: 145\n",
      "Loss: 1.8675813005194049\n",
      "Relative change: 0.168223%\n",
      "Iteration: 146\n",
      "Loss: 1.8644527304410816\n",
      "Relative change: 0.167520%\n",
      "Iteration: 147\n",
      "Loss: 1.8613424728742627\n",
      "Relative change: 0.166819%\n",
      "Iteration: 148\n",
      "Loss: 1.8582504302015688\n",
      "Relative change: 0.166119%\n",
      "Iteration: 149\n",
      "Loss: 1.8551765119375163\n",
      "Relative change: 0.165420%\n",
      "Iteration: 150\n",
      "Loss: 1.8521206324319848\n",
      "Relative change: 0.164722%\n",
      "Iteration: 151\n",
      "Loss: 1.849082708683518\n",
      "Relative change: 0.164024%\n",
      "Iteration: 152\n",
      "Loss: 1.846062658276729\n",
      "Relative change: 0.163327%\n",
      "Iteration: 153\n",
      "Loss: 1.8430603974600253\n",
      "Relative change: 0.162630%\n",
      "Iteration: 154\n",
      "Loss: 1.8400758393807624\n",
      "Relative change: 0.161935%\n",
      "Iteration: 155\n",
      "Loss: 1.8371088924946337\n",
      "Relative change: 0.161240%\n",
      "Iteration: 156\n",
      "Loss: 1.834159459164519\n",
      "Relative change: 0.160548%\n",
      "Iteration: 157\n",
      "Loss: 1.8312274344613295\n",
      "Relative change: 0.159857%\n",
      "Iteration: 158\n",
      "Loss: 1.828312705175678\n",
      "Relative change: 0.159168%\n",
      "Iteration: 159\n",
      "Loss: 1.8254151490448787\n",
      "Relative change: 0.158483%\n",
      "Iteration: 160\n",
      "Loss: 1.8225346341950717\n",
      "Relative change: 0.157801%\n",
      "Iteration: 161\n",
      "Loss: 1.8196710187935212\n",
      "Relative change: 0.157123%\n",
      "Iteration: 162\n",
      "Loss: 1.8168241509016507\n",
      "Relative change: 0.156450%\n",
      "Iteration: 163\n",
      "Loss: 1.8139938685153858\n",
      "Relative change: 0.155782%\n",
      "Iteration: 164\n",
      "Loss: 1.8111799997760667\n",
      "Relative change: 0.155120%\n",
      "Iteration: 165\n",
      "Loss: 1.8083823633326848\n",
      "Relative change: 0.154465%\n",
      "Iteration: 166\n",
      "Loss: 1.8056007688345204\n",
      "Relative change: 0.153817%\n",
      "Iteration: 167\n",
      "Loss: 1.8028350175324417\n",
      "Relative change: 0.153176%\n",
      "Iteration: 168\n",
      "Loss: 1.8000849029670571\n",
      "Relative change: 0.152544%\n",
      "Iteration: 169\n",
      "Loss: 1.7973502117225468\n",
      "Relative change: 0.151920%\n",
      "Iteration: 170\n",
      "Loss: 1.794630724226184\n",
      "Relative change: 0.151305%\n",
      "Iteration: 171\n",
      "Loss: 1.7919262155751787\n",
      "Relative change: 0.150700%\n",
      "Iteration: 172\n",
      "Loss: 1.7892364563744054\n",
      "Relative change: 0.150104%\n",
      "Iteration: 173\n",
      "Loss: 1.786561213570683\n",
      "Relative change: 0.149519%\n",
      "Iteration: 174\n",
      "Loss: 1.7839002512714643\n",
      "Relative change: 0.148943%\n",
      "Iteration: 175\n",
      "Loss: 1.7812533315379555\n",
      "Relative change: 0.148378%\n",
      "Iteration: 176\n",
      "Loss: 1.778620215144771\n",
      "Relative change: 0.147824%\n",
      "Iteration: 177\n",
      "Loss: 1.7760006623001507\n",
      "Relative change: 0.147280%\n",
      "Iteration: 178\n",
      "Loss: 1.7733944333225322\n",
      "Relative change: 0.146747%\n",
      "Iteration: 179\n",
      "Loss: 1.7708012892707978\n",
      "Relative change: 0.146225%\n",
      "Iteration: 180\n",
      "Loss: 1.7682209925268515\n",
      "Relative change: 0.145714%\n",
      "Iteration: 181\n",
      "Loss: 1.7656533073302918\n",
      "Relative change: 0.145213%\n",
      "Iteration: 182\n",
      "Loss: 1.7630980002658436\n",
      "Relative change: 0.144723%\n",
      "Iteration: 183\n",
      "Loss: 1.7605548407049183\n",
      "Relative change: 0.144244%\n",
      "Iteration: 184\n",
      "Loss: 1.7580236012032027\n",
      "Relative change: 0.143775%\n",
      "Iteration: 185\n",
      "Loss: 1.7555040578565546\n",
      "Relative change: 0.143317%\n",
      "Iteration: 186\n",
      "Loss: 1.7529959906177006\n",
      "Relative change: 0.142869%\n",
      "Iteration: 187\n",
      "Loss: 1.7504991835763808\n",
      "Relative change: 0.142431%\n",
      "Iteration: 188\n",
      "Loss: 1.7480134252055817\n",
      "Relative change: 0.142003%\n",
      "Iteration: 189\n",
      "Loss: 1.7455385085764799\n",
      "Relative change: 0.141585%\n",
      "Iteration: 190\n",
      "Loss: 1.7430742315445897\n",
      "Relative change: 0.141176%\n",
      "Iteration: 191\n",
      "Loss: 1.7406203969094831\n",
      "Relative change: 0.140776%\n",
      "Iteration: 192\n",
      "Loss: 1.7381768125502697\n",
      "Relative change: 0.140386%\n",
      "Iteration: 193\n",
      "Loss: 1.73574329153883\n",
      "Relative change: 0.140004%\n",
      "Iteration: 194\n",
      "Loss: 1.7333196522326364\n",
      "Relative change: 0.139631%\n",
      "Iteration: 195\n",
      "Loss: 1.7309057183487622\n",
      "Relative change: 0.139267%\n",
      "Iteration: 196\n",
      "Loss: 1.7285013190205514\n",
      "Relative change: 0.138910%\n",
      "Iteration: 197\n",
      "Loss: 1.7261062888382288\n",
      "Relative change: 0.138561%\n",
      "Iteration: 198\n",
      "Loss: 1.723720467874595\n",
      "Relative change: 0.138220%\n",
      "Iteration: 199\n",
      "Loss: 1.7213437016968267\n",
      "Relative change: 0.137886%\n",
      "Iteration: 200\n",
      "Loss: 1.718975841365296\n",
      "Relative change: 0.137559%\n",
      "Iteration: 201\n",
      "Loss: 1.7166167434202255\n",
      "Relative change: 0.137239%\n",
      "Iteration: 202\n",
      "Loss: 1.7142662698569355\n",
      "Relative change: 0.136925%\n",
      "Iteration: 203\n",
      "Loss: 1.7119242880903622\n",
      "Relative change: 0.136617%\n",
      "Iteration: 204\n",
      "Loss: 1.709590670909494\n",
      "Relative change: 0.136315%\n",
      "Iteration: 205\n",
      "Loss: 1.7072652964223205\n",
      "Relative change: 0.136019%\n",
      "Iteration: 206\n",
      "Loss: 1.704948047991855\n",
      "Relative change: 0.135729%\n",
      "Iteration: 207\n",
      "Loss: 1.7026388141637592\n",
      "Relative change: 0.135443%\n",
      "Iteration: 208\n",
      "Loss: 1.700337488586063\n",
      "Relative change: 0.135162%\n",
      "Iteration: 209\n",
      "Loss: 1.6980439699214434\n",
      "Relative change: 0.134886%\n",
      "Iteration: 210\n",
      "Loss: 1.6957581617524897\n",
      "Relative change: 0.134614%\n",
      "Iteration: 211\n",
      "Loss: 1.6934799724803415\n",
      "Relative change: 0.134346%\n",
      "Iteration: 212\n",
      "Loss: 1.6912093152170642\n",
      "Relative change: 0.134082%\n",
      "Iteration: 213\n",
      "Loss: 1.6889461076720786\n",
      "Relative change: 0.133822%\n",
      "Iteration: 214\n",
      "Loss: 1.6866902720329495\n",
      "Relative change: 0.133565%\n",
      "Iteration: 215\n",
      "Loss: 1.6844417348407934\n",
      "Relative change: 0.133311%\n",
      "Iteration: 216\n",
      "Loss: 1.6822004268605768\n",
      "Relative change: 0.133059%\n",
      "Iteration: 217\n",
      "Loss: 1.6799662829465485\n",
      "Relative change: 0.132811%\n",
      "Iteration: 218\n",
      "Loss: 1.677739241903074\n",
      "Relative change: 0.132565%\n",
      "Iteration: 219\n",
      "Loss: 1.6755192463411595\n",
      "Relative change: 0.132321%\n",
      "Iteration: 220\n",
      "Loss: 1.6733062425309655\n",
      "Relative change: 0.132079%\n",
      "Iteration: 221\n",
      "Loss: 1.6711001802506824\n",
      "Relative change: 0.131839%\n",
      "Iteration: 222\n",
      "Loss: 1.6689010126321597\n",
      "Relative change: 0.131600%\n",
      "Iteration: 223\n",
      "Loss: 1.6667086960037587\n",
      "Relative change: 0.131363%\n",
      "Iteration: 224\n",
      "Loss: 1.6645231897309258\n",
      "Relative change: 0.131127%\n",
      "Iteration: 225\n",
      "Loss: 1.662344456055049\n",
      "Relative change: 0.130892%\n",
      "Iteration: 226\n",
      "Loss: 1.6601724599311878\n",
      "Relative change: 0.130659%\n",
      "Iteration: 227\n",
      "Loss: 1.6580071688652849\n",
      "Relative change: 0.130426%\n",
      "Iteration: 228\n",
      "Loss: 1.6558485527514737\n",
      "Relative change: 0.130193%\n",
      "Iteration: 229\n",
      "Loss: 1.6536965837100834\n",
      "Relative change: 0.129962%\n",
      "Iteration: 230\n",
      "Loss: 1.6515512359268856\n",
      "Relative change: 0.129730%\n",
      "Iteration: 231\n",
      "Loss: 1.6494124854941055\n",
      "Relative change: 0.129499%\n",
      "Iteration: 232\n",
      "Loss: 1.6472803102535987\n",
      "Relative change: 0.129269%\n",
      "Iteration: 233\n",
      "Loss: 1.6451546896425604\n",
      "Relative change: 0.129038%\n",
      "Iteration: 234\n",
      "Loss: 1.6430356045420051\n",
      "Relative change: 0.128808%\n",
      "Iteration: 235\n",
      "Loss: 1.6409230371281955\n",
      "Relative change: 0.128577%\n",
      "Iteration: 236\n",
      "Loss: 1.6388169707271039\n",
      "Relative change: 0.128346%\n",
      "Iteration: 237\n",
      "Loss: 1.6367173896719442\n",
      "Relative change: 0.128116%\n",
      "Iteration: 238\n",
      "Loss: 1.634624279163761\n",
      "Relative change: 0.127885%\n",
      "Iteration: 239\n",
      "Loss: 1.6325376251350607\n",
      "Relative change: 0.127653%\n",
      "Iteration: 240\n",
      "Loss: 1.6304574141164756\n",
      "Relative change: 0.127422%\n",
      "Iteration: 241\n",
      "Loss: 1.6283836331065107\n",
      "Relative change: 0.127190%\n",
      "Iteration: 242\n",
      "Loss: 1.6263162694444753\n",
      "Relative change: 0.126958%\n",
      "Iteration: 243\n",
      "Loss: 1.6242553106868072\n",
      "Relative change: 0.126726%\n",
      "Iteration: 244\n",
      "Loss: 1.622200744487085\n",
      "Relative change: 0.126493%\n",
      "Iteration: 245\n",
      "Loss: 1.6201525584801482\n",
      "Relative change: 0.126260%\n",
      "Iteration: 246\n",
      "Loss: 1.6181107401708443\n",
      "Relative change: 0.126026%\n",
      "Iteration: 247\n",
      "Loss: 1.6160752768280056\n",
      "Relative change: 0.125793%\n",
      "Iteration: 248\n",
      "Loss: 1.6140461553843501\n",
      "Relative change: 0.125559%\n",
      "Iteration: 249\n",
      "Loss: 1.6120233623430307\n",
      "Relative change: 0.125324%\n",
      "Iteration: 250\n",
      "Loss: 1.610006883691582\n",
      "Relative change: 0.125090%\n",
      "Iteration: 251\n",
      "Loss: 1.6079967048239825\n",
      "Relative change: 0.124855%\n",
      "Iteration: 252\n",
      "Loss: 1.605992810471508\n",
      "Relative change: 0.124621%\n",
      "Iteration: 253\n",
      "Loss: 1.603995184642946\n",
      "Relative change: 0.124386%\n",
      "Iteration: 254\n",
      "Loss: 1.6020038105746393\n",
      "Relative change: 0.124151%\n",
      "Iteration: 255\n",
      "Loss: 1.600018670690663\n",
      "Relative change: 0.123916%\n",
      "Iteration: 256\n",
      "Loss: 1.598039746573299\n",
      "Relative change: 0.123681%\n",
      "Iteration: 257\n",
      "Loss: 1.5960670189437753\n",
      "Relative change: 0.123447%\n",
      "Iteration: 258\n",
      "Loss: 1.594100467653084\n",
      "Relative change: 0.123212%\n",
      "Iteration: 259\n",
      "Loss: 1.5921400716825036\n",
      "Relative change: 0.122978%\n",
      "Iteration: 260\n",
      "Loss: 1.590185809153294\n",
      "Relative change: 0.122744%\n",
      "Iteration: 261\n",
      "Loss: 1.588237657344889\n",
      "Relative change: 0.122511%\n",
      "Iteration: 262\n",
      "Loss: 1.5862955927207856\n",
      "Relative change: 0.122278%\n",
      "Iteration: 263\n",
      "Loss: 1.584359590961218\n",
      "Relative change: 0.122045%\n",
      "Iteration: 264\n",
      "Loss: 1.5824296270016434\n",
      "Relative change: 0.121814%\n",
      "Iteration: 265\n",
      "Loss: 1.5805056750759943\n",
      "Relative change: 0.121582%\n",
      "Iteration: 266\n",
      "Loss: 1.5785877087636435\n",
      "Relative change: 0.121351%\n",
      "Iteration: 267\n",
      "Loss: 1.5766757010390036\n",
      "Relative change: 0.121121%\n",
      "Iteration: 268\n",
      "Loss: 1.5747696243227132\n",
      "Relative change: 0.120892%\n",
      "Iteration: 269\n",
      "Loss: 1.5728694505333867\n",
      "Relative change: 0.120664%\n",
      "Iteration: 270\n",
      "Loss: 1.5709751511389447\n",
      "Relative change: 0.120436%\n",
      "Iteration: 271\n",
      "Loss: 1.5690866972066178\n",
      "Relative change: 0.120209%\n",
      "Iteration: 272\n",
      "Loss: 1.5672040594507661\n",
      "Relative change: 0.119983%\n",
      "Iteration: 273\n",
      "Loss: 1.565327208277752\n",
      "Relative change: 0.119758%\n",
      "Iteration: 274\n",
      "Loss: 1.5634561138271763\n",
      "Relative change: 0.119534%\n",
      "Iteration: 275\n",
      "Loss: 1.5615907460088847\n",
      "Relative change: 0.119311%\n",
      "Iteration: 276\n",
      "Loss: 1.5597310745352586\n",
      "Relative change: 0.119088%\n",
      "Iteration: 277\n",
      "Loss: 1.557877068948403\n",
      "Relative change: 0.118867%\n",
      "Iteration: 278\n",
      "Loss: 1.5560286986419667\n",
      "Relative change: 0.118647%\n",
      "Iteration: 279\n",
      "Loss: 1.55418593287745\n",
      "Relative change: 0.118427%\n",
      "Iteration: 280\n",
      "Loss: 1.5523487407950014\n",
      "Relative change: 0.118209%\n",
      "Iteration: 281\n",
      "Loss: 1.550517091418818\n",
      "Relative change: 0.117992%\n",
      "Iteration: 282\n",
      "Loss: 1.5486909536574336\n",
      "Relative change: 0.117776%\n",
      "Iteration: 283\n",
      "Loss: 1.5468702962993048\n",
      "Relative change: 0.117561%\n",
      "Iteration: 284\n",
      "Loss: 1.5450550880042504\n",
      "Relative change: 0.117347%\n",
      "Iteration: 285\n",
      "Loss: 1.5432452972914341\n",
      "Relative change: 0.117134%\n",
      "Iteration: 286\n",
      "Loss: 1.5414408925246852\n",
      "Relative change: 0.116923%\n",
      "Iteration: 287\n",
      "Loss: 1.53964184189607\n",
      "Relative change: 0.116712%\n",
      "Iteration: 288\n",
      "Loss: 1.53784811340868\n",
      "Relative change: 0.116503%\n",
      "Iteration: 289\n",
      "Loss: 1.5360596748596598\n",
      "Relative change: 0.116295%\n",
      "Iteration: 290\n",
      "Loss: 1.534276493824506\n",
      "Relative change: 0.116088%\n",
      "Iteration: 291\n",
      "Loss: 1.53249853764364\n",
      "Relative change: 0.115882%\n",
      "Iteration: 292\n",
      "Loss: 1.5307257734122108\n",
      "Relative change: 0.115678%\n",
      "Iteration: 293\n",
      "Loss: 1.5289581679739819\n",
      "Relative change: 0.115475%\n",
      "Iteration: 294\n",
      "Loss: 1.5271956879200366\n",
      "Relative change: 0.115273%\n",
      "Iteration: 295\n",
      "Loss: 1.5254382995929003\n",
      "Relative change: 0.115073%\n",
      "Iteration: 296\n",
      "Loss: 1.5236859690964861\n",
      "Relative change: 0.114874%\n",
      "Iteration: 297\n",
      "Loss: 1.5219386623121225\n",
      "Relative change: 0.114676%\n",
      "Iteration: 298\n",
      "Loss: 1.5201963449207048\n",
      "Relative change: 0.114480%\n",
      "Iteration: 299\n",
      "Loss: 1.5184589824308508\n",
      "Relative change: 0.114285%\n",
      "Iteration: 300\n",
      "Loss: 1.5167265402127608\n",
      "Relative change: 0.114092%\n",
      "Iteration: 301\n",
      "Loss: 1.5149989835373143\n",
      "Relative change: 0.113900%\n",
      "Iteration: 302\n",
      "Loss: 1.5132762776198123\n",
      "Relative change: 0.113710%\n",
      "Iteration: 303\n",
      "Loss: 1.5115583876676462\n",
      "Relative change: 0.113521%\n",
      "Iteration: 304\n",
      "Loss: 1.5098452789310999\n",
      "Relative change: 0.113334%\n",
      "Iteration: 305\n",
      "Loss: 1.5081369167564198\n",
      "Relative change: 0.113148%\n",
      "Iteration: 306\n",
      "Loss: 1.5064332666402696\n",
      "Relative change: 0.112964%\n",
      "Iteration: 307\n",
      "Loss: 1.5047342942846689\n",
      "Relative change: 0.112781%\n",
      "Iteration: 308\n",
      "Loss: 1.503039965651551\n",
      "Relative change: 0.112600%\n",
      "Iteration: 309\n",
      "Loss: 1.501350247016097\n",
      "Relative change: 0.112420%\n",
      "Iteration: 310\n",
      "Loss: 1.4996651050180807\n",
      "Relative change: 0.112242%\n",
      "Iteration: 311\n",
      "Loss: 1.4979845067105222\n",
      "Relative change: 0.112065%\n",
      "Iteration: 312\n",
      "Loss: 1.4963084196050316\n",
      "Relative change: 0.111889%\n",
      "Iteration: 313\n",
      "Loss: 1.4946368117133269\n",
      "Relative change: 0.111715%\n",
      "Iteration: 314\n",
      "Loss: 1.4929696515844926\n",
      "Relative change: 0.111543%\n",
      "Iteration: 315\n",
      "Loss: 1.491306908337651\n",
      "Relative change: 0.111372%\n",
      "Iteration: 316\n",
      "Loss: 1.4896485516898204\n",
      "Relative change: 0.111202%\n",
      "Iteration: 317\n",
      "Loss: 1.48799455197882\n",
      "Relative change: 0.111033%\n",
      "Iteration: 318\n",
      "Loss: 1.4863448801811807\n",
      "Relative change: 0.110865%\n",
      "Iteration: 319\n",
      "Loss: 1.4846995079251077\n",
      "Relative change: 0.110699%\n",
      "Iteration: 320\n",
      "Loss: 1.4830584074986188\n",
      "Relative change: 0.110534%\n",
      "Iteration: 321\n",
      "Loss: 1.4814215518530691\n",
      "Relative change: 0.110370%\n",
      "Iteration: 322\n",
      "Loss: 1.4797889146023162\n",
      "Relative change: 0.110207%\n",
      "Iteration: 323\n",
      "Loss: 1.4781604700178803\n",
      "Relative change: 0.110046%\n",
      "Iteration: 324\n",
      "Loss: 1.4765361930204524\n",
      "Relative change: 0.109885%\n",
      "Iteration: 325\n",
      "Loss: 1.4749160591682022\n",
      "Relative change: 0.109725%\n",
      "Iteration: 326\n",
      "Loss: 1.4733000446423168\n",
      "Relative change: 0.109567%\n",
      "Iteration: 327\n",
      "Loss: 1.4716881262302615\n",
      "Relative change: 0.109409%\n",
      "Iteration: 328\n",
      "Loss: 1.4700802813072438\n",
      "Relative change: 0.109252%\n",
      "Iteration: 329\n",
      "Loss: 1.4684764878163696\n",
      "Relative change: 0.109096%\n",
      "Iteration: 330\n",
      "Loss: 1.4668767242479688\n",
      "Relative change: 0.108940%\n",
      "Iteration: 331\n",
      "Loss: 1.4652809696185518\n",
      "Relative change: 0.108786%\n",
      "Iteration: 332\n",
      "Loss: 1.4636892034498248\n",
      "Relative change: 0.108632%\n",
      "Iteration: 333\n",
      "Loss: 1.4621014057481583\n",
      "Relative change: 0.108479%\n",
      "Iteration: 334\n",
      "Loss: 1.4605175569848623\n",
      "Relative change: 0.108327%\n",
      "Iteration: 335\n",
      "Loss: 1.4589376380775547\n",
      "Relative change: 0.108175%\n",
      "Iteration: 336\n",
      "Loss: 1.4573616303728758\n",
      "Relative change: 0.108024%\n",
      "Iteration: 337\n",
      "Loss: 1.4557895156307121\n",
      "Relative change: 0.107874%\n",
      "Iteration: 338\n",
      "Loss: 1.4542212760100546\n",
      "Relative change: 0.107724%\n",
      "Iteration: 339\n",
      "Loss: 1.4526568940565185\n",
      "Relative change: 0.107575%\n",
      "Iteration: 340\n",
      "Loss: 1.4510963526915135\n",
      "Relative change: 0.107427%\n",
      "Iteration: 341\n",
      "Loss: 1.4495396352029686\n",
      "Relative change: 0.107279%\n",
      "Iteration: 342\n",
      "Loss: 1.4479867252374643\n",
      "Relative change: 0.107131%\n",
      "Iteration: 343\n",
      "Loss: 1.4464376067935747\n",
      "Relative change: 0.106984%\n",
      "Iteration: 344\n",
      "Loss: 1.444892264216184\n",
      "Relative change: 0.106838%\n",
      "Iteration: 345\n",
      "Loss: 1.4433506821915119\n",
      "Relative change: 0.106692%\n",
      "Iteration: 346\n",
      "Loss: 1.4418128457425694\n",
      "Relative change: 0.106546%\n",
      "Iteration: 347\n",
      "Loss: 1.4402787402247879\n",
      "Relative change: 0.106401%\n",
      "Iteration: 348\n",
      "Loss: 1.438748351321564\n",
      "Relative change: 0.106256%\n",
      "Iteration: 349\n",
      "Loss: 1.4372216650395098\n",
      "Relative change: 0.106112%\n",
      "Iteration: 350\n",
      "Loss: 1.4356986677032524\n",
      "Relative change: 0.105968%\n",
      "Iteration: 351\n",
      "Loss: 1.4341793459496712\n",
      "Relative change: 0.105825%\n",
      "Iteration: 352\n",
      "Loss: 1.4326636867215368\n",
      "Relative change: 0.105681%\n",
      "Iteration: 353\n",
      "Loss: 1.431151677260569\n",
      "Relative change: 0.105538%\n",
      "Iteration: 354\n",
      "Loss: 1.4296433051000024\n",
      "Relative change: 0.105396%\n",
      "Iteration: 355\n",
      "Loss: 1.4281385580567942\n",
      "Relative change: 0.105253%\n",
      "Iteration: 356\n",
      "Loss: 1.4266374242236484\n",
      "Relative change: 0.105111%\n",
      "Iteration: 357\n",
      "Loss: 1.4251398919610694\n",
      "Relative change: 0.104969%\n",
      "Iteration: 358\n",
      "Loss: 1.4236459498896517\n",
      "Relative change: 0.104828%\n",
      "Iteration: 359\n",
      "Loss: 1.4221555868828182\n",
      "Relative change: 0.104686%\n",
      "Iteration: 360\n",
      "Loss: 1.4206687920602012\n",
      "Relative change: 0.104545%\n",
      "Iteration: 361\n",
      "Loss: 1.4191855547818037\n",
      "Relative change: 0.104404%\n",
      "Iteration: 362\n",
      "Loss: 1.4177058646430531\n",
      "Relative change: 0.104263%\n",
      "Iteration: 363\n",
      "Loss: 1.416229711470786\n",
      "Relative change: 0.104123%\n",
      "Iteration: 364\n",
      "Loss: 1.4147570853201394\n",
      "Relative change: 0.103982%\n",
      "Iteration: 365\n",
      "Loss: 1.413287976472265\n",
      "Relative change: 0.103842%\n",
      "Iteration: 366\n",
      "Loss: 1.4118223754327155\n",
      "Relative change: 0.103702%\n",
      "Iteration: 367\n",
      "Loss: 1.4103602729302835\n",
      "Relative change: 0.103561%\n",
      "Iteration: 368\n",
      "Loss: 1.4089016599160447\n",
      "Relative change: 0.103421%\n",
      "Iteration: 369\n",
      "Loss: 1.4074465275622876\n",
      "Relative change: 0.103281%\n",
      "Iteration: 370\n",
      "Loss: 1.4059948672610108\n",
      "Relative change: 0.103141%\n",
      "Iteration: 371\n",
      "Loss: 1.4045466706216343\n",
      "Relative change: 0.103002%\n",
      "Iteration: 372\n",
      "Loss: 1.4031019294675855\n",
      "Relative change: 0.102862%\n",
      "Iteration: 373\n",
      "Loss: 1.4016606358314168\n",
      "Relative change: 0.102722%\n",
      "Iteration: 374\n",
      "Loss: 1.4002227819481559\n",
      "Relative change: 0.102582%\n",
      "Iteration: 375\n",
      "Loss: 1.398788360246616\n",
      "Relative change: 0.102442%\n",
      "Iteration: 376\n",
      "Loss: 1.3973573633384495\n",
      "Relative change: 0.102303%\n",
      "Iteration: 377\n",
      "Loss: 1.3959297840047764\n",
      "Relative change: 0.102163%\n",
      "Iteration: 378\n",
      "Loss: 1.3945056151803057\n",
      "Relative change: 0.102023%\n",
      "Iteration: 379\n",
      "Loss: 1.393084849934907\n",
      "Relative change: 0.101883%\n",
      "Iteration: 380\n",
      "Loss: 1.3916674814526955\n",
      "Relative change: 0.101743%\n",
      "Iteration: 381\n",
      "Loss: 1.3902535030087335\n",
      "Relative change: 0.101603%\n",
      "Iteration: 382\n",
      "Loss: 1.3888429079435487\n",
      "Relative change: 0.101463%\n",
      "Iteration: 383\n",
      "Loss: 1.3874356896357152\n",
      "Relative change: 0.101323%\n",
      "Iteration: 384\n",
      "Loss: 1.386031841472808\n",
      "Relative change: 0.101183%\n",
      "Iteration: 385\n",
      "Loss: 1.3846313568210913\n",
      "Relative change: 0.101043%\n",
      "Iteration: 386\n",
      "Loss: 1.3832342289943358\n",
      "Relative change: 0.100903%\n",
      "Iteration: 387\n",
      "Loss: 1.38184045122219\n",
      "Relative change: 0.100762%\n",
      "Iteration: 388\n",
      "Loss: 1.380450016618541\n",
      "Relative change: 0.100622%\n",
      "Iteration: 389\n",
      "Loss: 1.3790629181503022\n",
      "Relative change: 0.100482%\n",
      "Iteration: 390\n",
      "Loss: 1.3776791486070643\n",
      "Relative change: 0.100341%\n",
      "Iteration: 391\n",
      "Loss: 1.3762987005720055\n",
      "Relative change: 0.100201%\n",
      "Iteration: 392\n",
      "Loss: 1.3749215663944458\n",
      "Relative change: 0.100061%\n",
      "Iteration: 393\n",
      "Loss: 1.3735477381643744\n",
      "Relative change: 0.099920%\n",
      "Iteration: 394\n",
      "Loss: 1.3721772076892387\n",
      "Relative change: 0.099780%\n",
      "Iteration: 395\n",
      "Loss: 1.3708099664732336\n",
      "Relative change: 0.099640%\n",
      "Iteration: 396\n",
      "Loss: 1.36944600569927\n",
      "Relative change: 0.099500%\n",
      "Iteration: 397\n",
      "Loss: 1.3680853162137525\n",
      "Relative change: 0.099361%\n",
      "Iteration: 398\n",
      "Loss: 1.3667278885142422\n",
      "Relative change: 0.099221%\n",
      "Iteration: 399\n",
      "Loss: 1.3653737127400252\n",
      "Relative change: 0.099082%\n",
      "Iteration: 400\n",
      "Loss: 1.3640227786655723\n",
      "Relative change: 0.098942%\n",
      "Iteration: 401\n",
      "Loss: 1.3626750756968304\n",
      "Relative change: 0.098804%\n",
      "Iteration: 402\n",
      "Loss: 1.3613305928702428\n",
      "Relative change: 0.098665%\n",
      "Iteration: 403\n",
      "Loss: 1.3599893188543901\n",
      "Relative change: 0.098527%\n",
      "Iteration: 404\n",
      "Loss: 1.3586512419540957\n",
      "Relative change: 0.098389%\n",
      "Iteration: 405\n",
      "Loss: 1.3573163501168444\n",
      "Relative change: 0.098251%\n",
      "Iteration: 406\n",
      "Loss: 1.355984630941338\n",
      "Relative change: 0.098114%\n",
      "Iteration: 407\n",
      "Loss: 1.3546560716880054\n",
      "Relative change: 0.097977%\n",
      "Iteration: 408\n",
      "Loss: 1.3533306592912928\n",
      "Relative change: 0.097841%\n",
      "Iteration: 409\n",
      "Loss: 1.3520083803735334\n",
      "Relative change: 0.097706%\n",
      "Iteration: 410\n",
      "Loss: 1.350689221260224\n",
      "Relative change: 0.097570%\n",
      "Iteration: 411\n",
      "Loss: 1.3493731679965135\n",
      "Relative change: 0.097436%\n",
      "Iteration: 412\n",
      "Loss: 1.348060206364725\n",
      "Relative change: 0.097302%\n",
      "Iteration: 413\n",
      "Loss: 1.3467503219027217\n",
      "Relative change: 0.097168%\n",
      "Iteration: 414\n",
      "Loss: 1.3454434999229536\n",
      "Relative change: 0.097035%\n",
      "Iteration: 415\n",
      "Loss: 1.3441397255319902\n",
      "Relative change: 0.096903%\n",
      "Iteration: 416\n",
      "Loss: 1.3428389836503827\n",
      "Relative change: 0.096771%\n",
      "Iteration: 417\n",
      "Loss: 1.3415412590326818\n",
      "Relative change: 0.096640%\n",
      "Iteration: 418\n",
      "Loss: 1.3402465362874576\n",
      "Relative change: 0.096510%\n",
      "Iteration: 419\n",
      "Loss: 1.3389547998971598\n",
      "Relative change: 0.096381%\n",
      "Iteration: 420\n",
      "Loss: 1.3376660342376843\n",
      "Relative change: 0.096252%\n",
      "Iteration: 421\n",
      "Loss: 1.3363802235975086\n",
      "Relative change: 0.096123%\n",
      "Iteration: 422\n",
      "Loss: 1.3350973521962748\n",
      "Relative change: 0.095996%\n",
      "Iteration: 423\n",
      "Loss: 1.3338174042027129\n",
      "Relative change: 0.095869%\n",
      "Iteration: 424\n",
      "Loss: 1.3325403637518156\n",
      "Relative change: 0.095743%\n",
      "Iteration: 425\n",
      "Loss: 1.3312662149611774\n",
      "Relative change: 0.095618%\n",
      "Iteration: 426\n",
      "Loss: 1.3299949419464434\n",
      "Relative change: 0.095494%\n",
      "Iteration: 427\n",
      "Loss: 1.3287265288358165\n",
      "Relative change: 0.095370%\n",
      "Iteration: 428\n",
      "Loss: 1.3274609597835936\n",
      "Relative change: 0.095247%\n",
      "Iteration: 429\n",
      "Loss: 1.3261982189827128\n",
      "Relative change: 0.095125%\n",
      "Iteration: 430\n",
      "Loss: 1.3249382906763094\n",
      "Relative change: 0.095003%\n",
      "Iteration: 431\n",
      "Loss: 1.3236811591682935\n",
      "Relative change: 0.094882%\n",
      "Iteration: 432\n",
      "Loss: 1.322426808832971\n",
      "Relative change: 0.094762%\n",
      "Iteration: 433\n",
      "Loss: 1.3211752241237429\n",
      "Relative change: 0.094643%\n",
      "Iteration: 434\n",
      "Loss: 1.3199263895809326\n",
      "Relative change: 0.094525%\n",
      "Iteration: 435\n",
      "Loss: 1.318680289838791\n",
      "Relative change: 0.094407%\n",
      "Iteration: 436\n",
      "Loss: 1.3174369096317484\n",
      "Relative change: 0.094290%\n",
      "Iteration: 437\n",
      "Loss: 1.3161962337999888\n",
      "Relative change: 0.094173%\n",
      "Iteration: 438\n",
      "Loss: 1.3149582472944255\n",
      "Relative change: 0.094058%\n",
      "Iteration: 439\n",
      "Loss: 1.3137229351811686\n",
      "Relative change: 0.093943%\n",
      "Iteration: 440\n",
      "Loss: 1.3124902826455886\n",
      "Relative change: 0.093829%\n",
      "Iteration: 441\n",
      "Loss: 1.311260274996077\n",
      "Relative change: 0.093716%\n",
      "Iteration: 442\n",
      "Loss: 1.3100328976676177\n",
      "Relative change: 0.093603%\n",
      "Iteration: 443\n",
      "Loss: 1.308808136225294\n",
      "Relative change: 0.093491%\n",
      "Iteration: 444\n",
      "Loss: 1.307585976367851\n",
      "Relative change: 0.093380%\n",
      "Iteration: 445\n",
      "Loss: 1.3063664039314435\n",
      "Relative change: 0.093269%\n",
      "Iteration: 446\n",
      "Loss: 1.3051494048936978\n",
      "Relative change: 0.093159%\n",
      "Iteration: 447\n",
      "Loss: 1.3039349653782206\n",
      "Relative change: 0.093050%\n",
      "Iteration: 448\n",
      "Loss: 1.3027230716596718\n",
      "Relative change: 0.092941%\n",
      "Iteration: 449\n",
      "Loss: 1.3015137101695227\n",
      "Relative change: 0.092833%\n",
      "Iteration: 450\n",
      "Loss: 1.3003068675026022\n",
      "Relative change: 0.092726%\n",
      "Iteration: 451\n",
      "Loss: 1.2991025304245185\n",
      "Relative change: 0.092619%\n",
      "Iteration: 452\n",
      "Loss: 1.297900685880027\n",
      "Relative change: 0.092513%\n",
      "Iteration: 453\n",
      "Loss: 1.29670132100239\n",
      "Relative change: 0.092408%\n",
      "Iteration: 454\n",
      "Loss: 1.2955044231237443\n",
      "Relative change: 0.092303%\n",
      "Iteration: 455\n",
      "Loss: 1.294309979786471\n",
      "Relative change: 0.092199%\n",
      "Iteration: 456\n",
      "Loss: 1.2931179787555278\n",
      "Relative change: 0.092095%\n",
      "Iteration: 457\n",
      "Loss: 1.291928408031668\n",
      "Relative change: 0.091992%\n",
      "Iteration: 458\n",
      "Loss: 1.2907412558654505\n",
      "Relative change: 0.091890%\n",
      "Iteration: 459\n",
      "Loss: 1.28955651077191\n",
      "Relative change: 0.091788%\n",
      "Iteration: 460\n",
      "Loss: 1.2883741615457385\n",
      "Relative change: 0.091686%\n",
      "Iteration: 461\n",
      "Loss: 1.2871941972767977\n",
      "Relative change: 0.091586%\n",
      "Iteration: 462\n",
      "Loss: 1.2860166073657864\n",
      "Relative change: 0.091485%\n",
      "Iteration: 463\n",
      "Loss: 1.2848413815398596\n",
      "Relative change: 0.091385%\n",
      "Iteration: 464\n",
      "Loss: 1.2836685098680132\n",
      "Relative change: 0.091285%\n",
      "Iteration: 465\n",
      "Loss: 1.2824979827760417\n",
      "Relative change: 0.091186%\n",
      "Iteration: 466\n",
      "Loss: 1.2813297910608952\n",
      "Relative change: 0.091087%\n",
      "Iteration: 467\n",
      "Loss: 1.280163925904289\n",
      "Relative change: 0.090989%\n",
      "Iteration: 468\n",
      "Loss: 1.2790003788854263\n",
      "Relative change: 0.090890%\n",
      "Iteration: 469\n",
      "Loss: 1.2778391419927517\n",
      "Relative change: 0.090793%\n",
      "Iteration: 470\n",
      "Loss: 1.2766802076346546\n",
      "Relative change: 0.090695%\n",
      "Iteration: 471\n",
      "Loss: 1.2755235686491047\n",
      "Relative change: 0.090597%\n",
      "Iteration: 472\n",
      "Loss: 1.2743692183122106\n",
      "Relative change: 0.090500%\n",
      "Iteration: 473\n",
      "Loss: 1.273217150345739\n",
      "Relative change: 0.090403%\n",
      "Iteration: 474\n",
      "Loss: 1.2720673589236442\n",
      "Relative change: 0.090306%\n",
      "Iteration: 475\n",
      "Loss: 1.270919838677686\n",
      "Relative change: 0.090209%\n",
      "Iteration: 476\n",
      "Loss: 1.2697745847022188\n",
      "Relative change: 0.090112%\n",
      "Iteration: 477\n",
      "Loss: 1.2686315925582388\n",
      "Relative change: 0.090015%\n",
      "Iteration: 478\n",
      "Loss: 1.267490858276771\n",
      "Relative change: 0.089918%\n",
      "Iteration: 479\n",
      "Loss: 1.2663523783616562\n",
      "Relative change: 0.089822%\n",
      "Iteration: 480\n",
      "Loss: 1.2652161497917829\n",
      "Relative change: 0.089725%\n",
      "Iteration: 481\n",
      "Loss: 1.2640821700227716\n",
      "Relative change: 0.089627%\n",
      "Iteration: 482\n",
      "Loss: 1.2629504369880742\n",
      "Relative change: 0.089530%\n",
      "Iteration: 483\n",
      "Loss: 1.2618209490994319\n",
      "Relative change: 0.089432%\n",
      "Iteration: 484\n",
      "Loss: 1.260693705246565\n",
      "Relative change: 0.089335%\n",
      "Iteration: 485\n",
      "Loss: 1.2595687047959412\n",
      "Relative change: 0.089237%\n",
      "Iteration: 486\n",
      "Loss: 1.2584459475884324\n",
      "Relative change: 0.089138%\n",
      "Iteration: 487\n",
      "Loss: 1.257325433935623\n",
      "Relative change: 0.089039%\n",
      "Iteration: 488\n",
      "Loss: 1.2562071646145214\n",
      "Relative change: 0.088940%\n",
      "Iteration: 489\n",
      "Loss: 1.255091140860407\n",
      "Relative change: 0.088841%\n",
      "Iteration: 490\n",
      "Loss: 1.2539773643575334\n",
      "Relative change: 0.088741%\n",
      "Iteration: 491\n",
      "Loss: 1.2528658372274388\n",
      "Relative change: 0.088640%\n",
      "Iteration: 492\n",
      "Loss: 1.2517565620146154\n",
      "Relative change: 0.088539%\n",
      "Iteration: 493\n",
      "Loss: 1.2506495416693366\n",
      "Relative change: 0.088437%\n",
      "Iteration: 494\n",
      "Loss: 1.2495447795274945\n",
      "Relative change: 0.088335%\n",
      "Iteration: 495\n",
      "Loss: 1.2484422792873464\n",
      "Relative change: 0.088232%\n",
      "Iteration: 496\n",
      "Loss: 1.2473420449831447\n",
      "Relative change: 0.088129%\n",
      "Iteration: 497\n",
      "Loss: 1.2462440809557005\n",
      "Relative change: 0.088024%\n",
      "Iteration: 498\n",
      "Loss: 1.2451483918200015\n",
      "Relative change: 0.087919%\n",
      "Iteration: 499\n",
      "Loss: 1.2440549824300886\n",
      "Relative change: 0.087814%\n",
      "Iteration: 500\n",
      "Loss: 1.2429638578414686\n",
      "Relative change: 0.087707%\n",
      "Iteration: 501\n",
      "Loss: 1.2418750232714075\n",
      "Relative change: 0.087600%\n",
      "Iteration: 502\n",
      "Loss: 1.24078848405751\n",
      "Relative change: 0.087492%\n",
      "Iteration: 503\n",
      "Loss: 1.239704245615046\n",
      "Relative change: 0.087383%\n",
      "Iteration: 504\n",
      "Loss: 1.2386223133935053\n",
      "Relative change: 0.087273%\n",
      "Iteration: 505\n",
      "Loss: 1.2375426928329012\n",
      "Relative change: 0.087163%\n",
      "Iteration: 506\n",
      "Loss: 1.2364653893203437\n",
      "Relative change: 0.087052%\n",
      "Iteration: 507\n",
      "Loss: 1.2353904081473825\n",
      "Relative change: 0.086940%\n",
      "Iteration: 508\n",
      "Loss: 1.2343177544686141\n",
      "Relative change: 0.086827%\n",
      "Iteration: 509\n",
      "Loss: 1.2332474332620087\n",
      "Relative change: 0.086714%\n",
      "Iteration: 510\n",
      "Loss: 1.232179449291346\n",
      "Relative change: 0.086599%\n",
      "Iteration: 511\n",
      "Loss: 1.231113807071122\n",
      "Relative change: 0.086484%\n",
      "Iteration: 512\n",
      "Loss: 1.2300505108341944\n",
      "Relative change: 0.086369%\n",
      "Iteration: 513\n",
      "Loss: 1.2289895645023943\n",
      "Relative change: 0.086252%\n",
      "Iteration: 514\n",
      "Loss: 1.227930971660241\n",
      "Relative change: 0.086135%\n",
      "Iteration: 515\n",
      "Loss: 1.2268747355318457\n",
      "Relative change: 0.086018%\n",
      "Iteration: 516\n",
      "Loss: 1.2258208589610116\n",
      "Relative change: 0.085899%\n",
      "Iteration: 517\n",
      "Loss: 1.224769344394494\n",
      "Relative change: 0.085780%\n",
      "Iteration: 518\n",
      "Loss: 1.223720193868322\n",
      "Relative change: 0.085661%\n",
      "Iteration: 519\n",
      "Loss: 1.2226734089970424\n",
      "Relative change: 0.085541%\n",
      "Iteration: 520\n",
      "Loss: 1.2216289909657205\n",
      "Relative change: 0.085421%\n",
      "Iteration: 521\n",
      "Loss: 1.220586940524496\n",
      "Relative change: 0.085300%\n",
      "Iteration: 522\n",
      "Loss: 1.2195472579854834\n",
      "Relative change: 0.085179%\n",
      "Iteration: 523\n",
      "Loss: 1.2185099432217985\n",
      "Relative change: 0.085057%\n",
      "Iteration: 524\n",
      "Loss: 1.217474995668484\n",
      "Relative change: 0.084936%\n",
      "Iteration: 525\n",
      "Loss: 1.216442414325123\n",
      "Relative change: 0.084813%\n",
      "Iteration: 526\n",
      "Loss: 1.215412197759926\n",
      "Relative change: 0.084691%\n",
      "Iteration: 527\n",
      "Loss: 1.2143843441151054\n",
      "Relative change: 0.084568%\n",
      "Iteration: 528\n",
      "Loss: 1.2133588511133475\n",
      "Relative change: 0.084446%\n",
      "Iteration: 529\n",
      "Loss: 1.2123357160652284\n",
      "Relative change: 0.084323%\n",
      "Iteration: 530\n",
      "Loss: 1.2113149358774202\n",
      "Relative change: 0.084199%\n",
      "Iteration: 531\n",
      "Loss: 1.210296507061567\n",
      "Relative change: 0.084076%\n",
      "Iteration: 532\n",
      "Loss: 1.2092804257437086\n",
      "Relative change: 0.083953%\n",
      "Iteration: 533\n",
      "Loss: 1.2082666876741657\n",
      "Relative change: 0.083830%\n",
      "Iteration: 534\n",
      "Loss: 1.2072552882377985\n",
      "Relative change: 0.083707%\n",
      "Iteration: 535\n",
      "Loss: 1.2062462224645785\n",
      "Relative change: 0.083583%\n",
      "Iteration: 536\n",
      "Loss: 1.205239485040416\n",
      "Relative change: 0.083460%\n",
      "Iteration: 537\n",
      "Loss: 1.2042350703182119\n",
      "Relative change: 0.083337%\n",
      "Iteration: 538\n",
      "Loss: 1.2032329723291022\n",
      "Relative change: 0.083214%\n",
      "Iteration: 539\n",
      "Loss: 1.2022331847938883\n",
      "Relative change: 0.083092%\n",
      "Iteration: 540\n",
      "Loss: 1.2012357011346544\n",
      "Relative change: 0.082969%\n",
      "Iteration: 541\n",
      "Loss: 1.2002405144865738\n",
      "Relative change: 0.082847%\n",
      "Iteration: 542\n",
      "Loss: 1.199247617709942\n",
      "Relative change: 0.082725%\n",
      "Iteration: 543\n",
      "Loss: 1.1982570034024547\n",
      "Relative change: 0.082603%\n",
      "Iteration: 544\n",
      "Loss: 1.197268663911775\n",
      "Relative change: 0.082481%\n",
      "Iteration: 545\n",
      "Loss: 1.1962825913484425\n",
      "Relative change: 0.082360%\n",
      "Iteration: 546\n",
      "Loss: 1.195298777599155\n",
      "Relative change: 0.082239%\n",
      "Iteration: 547\n",
      "Loss: 1.1943172143404983\n",
      "Relative change: 0.082119%\n",
      "Iteration: 548\n",
      "Loss: 1.1933378930531569\n",
      "Relative change: 0.081998%\n",
      "Iteration: 549\n",
      "Loss: 1.1923608050366596\n",
      "Relative change: 0.081879%\n",
      "Iteration: 550\n",
      "Loss: 1.191385941424713\n",
      "Relative change: 0.081759%\n",
      "Iteration: 551\n",
      "Loss: 1.1904132932011429\n",
      "Relative change: 0.081640%\n",
      "Iteration: 552\n",
      "Loss: 1.189442851216493\n",
      "Relative change: 0.081521%\n",
      "Iteration: 553\n",
      "Loss: 1.1884746062052878\n",
      "Relative change: 0.081403%\n",
      "Iteration: 554\n",
      "Loss: 1.187508548803973\n",
      "Relative change: 0.081285%\n",
      "Iteration: 555\n",
      "Loss: 1.1865446695695336\n",
      "Relative change: 0.081168%\n",
      "Iteration: 556\n",
      "Loss: 1.185582958998772\n",
      "Relative change: 0.081051%\n",
      "Iteration: 557\n",
      "Loss: 1.1846234075482265\n",
      "Relative change: 0.080935%\n",
      "Iteration: 558\n",
      "Loss: 1.1836660056546777\n",
      "Relative change: 0.080819%\n",
      "Iteration: 559\n",
      "Loss: 1.1827107437562112\n",
      "Relative change: 0.080704%\n",
      "Iteration: 560\n",
      "Loss: 1.1817576123137536\n",
      "Relative change: 0.080589%\n",
      "Iteration: 561\n",
      "Loss: 1.180806601833021\n",
      "Relative change: 0.080474%\n",
      "Iteration: 562\n",
      "Loss: 1.1798577028867872\n",
      "Relative change: 0.080360%\n",
      "Iteration: 563\n",
      "Loss: 1.1789109061373741\n",
      "Relative change: 0.080247%\n",
      "Iteration: 564\n",
      "Loss: 1.1779662023592614\n",
      "Relative change: 0.080134%\n",
      "Iteration: 565\n",
      "Loss: 1.1770235824616935\n",
      "Relative change: 0.080021%\n",
      "Iteration: 566\n",
      "Loss: 1.1760830375111626\n",
      "Relative change: 0.079909%\n",
      "Iteration: 567\n",
      "Loss: 1.1751445587536349\n",
      "Relative change: 0.079797%\n",
      "Iteration: 568\n",
      "Loss: 1.1742081376363738\n",
      "Relative change: 0.079686%\n",
      "Iteration: 569\n",
      "Loss: 1.1732737658292234\n",
      "Relative change: 0.079575%\n",
      "Iteration: 570\n",
      "Loss: 1.1723414352451955\n",
      "Relative change: 0.079464%\n",
      "Iteration: 571\n",
      "Loss: 1.171411138060212\n",
      "Relative change: 0.079354%\n",
      "Iteration: 572\n",
      "Loss: 1.1704828667318405\n",
      "Relative change: 0.079244%\n",
      "Iteration: 573\n",
      "Loss: 1.1695566140168847\n",
      "Relative change: 0.079134%\n",
      "Iteration: 574\n",
      "Loss: 1.1686323729876587\n",
      "Relative change: 0.079025%\n",
      "Iteration: 575\n",
      "Loss: 1.1677101370468146\n",
      "Relative change: 0.078916%\n",
      "Iteration: 576\n",
      "Loss: 1.1667898999405752\n",
      "Relative change: 0.078807%\n",
      "Iteration: 577\n",
      "Loss: 1.1658716557702409\n",
      "Relative change: 0.078698%\n",
      "Iteration: 578\n",
      "Loss: 1.1649553990018557\n",
      "Relative change: 0.078590%\n",
      "Iteration: 579\n",
      "Loss: 1.1640411244739155\n",
      "Relative change: 0.078482%\n",
      "Iteration: 580\n",
      "Loss: 1.163128827403038\n",
      "Relative change: 0.078373%\n",
      "Iteration: 581\n",
      "Loss: 1.1622185033875085\n",
      "Relative change: 0.078265%\n",
      "Iteration: 582\n",
      "Loss: 1.1613101484086534\n",
      "Relative change: 0.078157%\n",
      "Iteration: 583\n",
      "Loss: 1.1604037588299956\n",
      "Relative change: 0.078049%\n",
      "Iteration: 584\n",
      "Loss: 1.1594993313941855\n",
      "Relative change: 0.077941%\n",
      "Iteration: 585\n",
      "Loss: 1.1585968632177028\n",
      "Relative change: 0.077833%\n",
      "Iteration: 586\n",
      "Loss: 1.1576963517833612\n",
      "Relative change: 0.077724%\n",
      "Iteration: 587\n",
      "Loss: 1.1567977949306634\n",
      "Relative change: 0.077616%\n",
      "Iteration: 588\n",
      "Loss: 1.1559011908440706\n",
      "Relative change: 0.077507%\n",
      "Iteration: 589\n",
      "Loss: 1.1550065380392776\n",
      "Relative change: 0.077399%\n",
      "Iteration: 590\n",
      "Loss: 1.1541138353476021\n",
      "Relative change: 0.077290%\n",
      "Iteration: 591\n",
      "Loss: 1.153223081898605\n",
      "Relative change: 0.077181%\n",
      "Iteration: 592\n",
      "Loss: 1.1523342771010872\n",
      "Relative change: 0.077071%\n",
      "Iteration: 593\n",
      "Loss: 1.1514474206226082\n",
      "Relative change: 0.076962%\n",
      "Iteration: 594\n",
      "Loss: 1.1505625123676948\n",
      "Relative change: 0.076852%\n",
      "Iteration: 595\n",
      "Loss: 1.149679552454908\n",
      "Relative change: 0.076742%\n",
      "Iteration: 596\n",
      "Loss: 1.1487985411929447\n",
      "Relative change: 0.076631%\n",
      "Iteration: 597\n",
      "Loss: 1.1479194790559626\n",
      "Relative change: 0.076520%\n",
      "Iteration: 598\n",
      "Loss: 1.1470423666583096\n",
      "Relative change: 0.076409%\n",
      "Iteration: 599\n",
      "Loss: 1.1461672047288411\n",
      "Relative change: 0.076297%\n",
      "Iteration: 600\n",
      "Loss: 1.1452939940850184\n",
      "Relative change: 0.076185%\n",
      "Iteration: 601\n",
      "Loss: 1.1444227356069592\n",
      "Relative change: 0.076073%\n",
      "Iteration: 602\n",
      "Loss: 1.1435534302116308\n",
      "Relative change: 0.075960%\n",
      "Iteration: 603\n",
      "Loss: 1.142686078827342\n",
      "Relative change: 0.075847%\n",
      "Iteration: 604\n",
      "Loss: 1.1418206823687167\n",
      "Relative change: 0.075734%\n",
      "Iteration: 605\n",
      "Loss: 1.1409572417122906\n",
      "Relative change: 0.075620%\n",
      "Iteration: 606\n",
      "Loss: 1.1400957576728887\n",
      "Relative change: 0.075505%\n",
      "Iteration: 607\n",
      "Loss: 1.1392362309809194\n",
      "Relative change: 0.075391%\n",
      "Iteration: 608\n",
      "Loss: 1.1383786622607102\n",
      "Relative change: 0.075276%\n",
      "Iteration: 609\n",
      "Loss: 1.1375230520100015\n",
      "Relative change: 0.075160%\n",
      "Iteration: 610\n",
      "Loss: 1.1366694005807068\n",
      "Relative change: 0.075045%\n",
      "Iteration: 611\n",
      "Loss: 1.1358177081610197\n",
      "Relative change: 0.074929%\n",
      "Iteration: 612\n",
      "Loss: 1.1349679747589543\n",
      "Relative change: 0.074812%\n",
      "Iteration: 613\n",
      "Loss: 1.1341202001873776\n",
      "Relative change: 0.074696%\n",
      "Iteration: 614\n",
      "Loss: 1.133274384050577\n",
      "Relative change: 0.074579%\n",
      "Iteration: 615\n",
      "Loss: 1.132430525732411\n",
      "Relative change: 0.074462%\n",
      "Iteration: 616\n",
      "Loss: 1.1315886243860456\n",
      "Relative change: 0.074345%\n",
      "Iteration: 617\n",
      "Loss: 1.1307486789252914\n",
      "Relative change: 0.074227%\n",
      "Iteration: 618\n",
      "Loss: 1.129910688017532\n",
      "Relative change: 0.074109%\n",
      "Iteration: 619\n",
      "Loss: 1.129074650078213\n",
      "Relative change: 0.073992%\n",
      "Iteration: 620\n",
      "Loss: 1.1282405632668646\n",
      "Relative change: 0.073873%\n",
      "Iteration: 621\n",
      "Loss: 1.1274084254846026\n",
      "Relative change: 0.073755%\n",
      "Iteration: 622\n",
      "Loss: 1.1265782343730475\n",
      "Relative change: 0.073637%\n",
      "Iteration: 623\n",
      "Loss: 1.1257499873145986\n",
      "Relative change: 0.073519%\n",
      "Iteration: 624\n",
      "Loss: 1.1249236814339703\n",
      "Relative change: 0.073400%\n",
      "Iteration: 625\n",
      "Loss: 1.1240993136009194\n",
      "Relative change: 0.073282%\n",
      "Iteration: 626\n",
      "Loss: 1.1232768804340534\n",
      "Relative change: 0.073164%\n",
      "Iteration: 627\n",
      "Loss: 1.1224563783056334\n",
      "Relative change: 0.073045%\n",
      "Iteration: 628\n",
      "Loss: 1.1216378033472605\n",
      "Relative change: 0.072927%\n",
      "Iteration: 629\n",
      "Loss: 1.1208211514563435\n",
      "Relative change: 0.072809%\n",
      "Iteration: 630\n",
      "Loss: 1.1200064183032414\n",
      "Relative change: 0.072691%\n",
      "Iteration: 631\n",
      "Loss: 1.1191935993389748\n",
      "Relative change: 0.072573%\n",
      "Iteration: 632\n",
      "Loss: 1.1183826898034\n",
      "Relative change: 0.072455%\n",
      "Iteration: 633\n",
      "Loss: 1.1175736847337423\n",
      "Relative change: 0.072337%\n",
      "Iteration: 634\n",
      "Loss: 1.1167665789733947\n",
      "Relative change: 0.072219%\n",
      "Iteration: 635\n",
      "Loss: 1.115961367180879\n",
      "Relative change: 0.072102%\n",
      "Iteration: 636\n",
      "Loss: 1.115158043838888\n",
      "Relative change: 0.071985%\n",
      "Iteration: 637\n",
      "Loss: 1.1143566032633165\n",
      "Relative change: 0.071868%\n",
      "Iteration: 638\n",
      "Loss: 1.1135570396122105\n",
      "Relative change: 0.071751%\n",
      "Iteration: 639\n",
      "Loss: 1.1127593468945565\n",
      "Relative change: 0.071635%\n",
      "Iteration: 640\n",
      "Loss: 1.111963518978853\n",
      "Relative change: 0.071518%\n",
      "Iteration: 641\n",
      "Loss: 1.1111695496013985\n",
      "Relative change: 0.071402%\n",
      "Iteration: 642\n",
      "Loss: 1.1103774323742552\n",
      "Relative change: 0.071287%\n",
      "Iteration: 643\n",
      "Loss: 1.1095871607928358\n",
      "Relative change: 0.071171%\n",
      "Iteration: 644\n",
      "Loss: 1.1087987282430825\n",
      "Relative change: 0.071056%\n",
      "Iteration: 645\n",
      "Loss: 1.108012128008209\n",
      "Relative change: 0.070942%\n",
      "Iteration: 646\n",
      "Loss: 1.1072273532749806\n",
      "Relative change: 0.070827%\n",
      "Iteration: 647\n",
      "Loss: 1.106444397139517\n",
      "Relative change: 0.070713%\n",
      "Iteration: 648\n",
      "Loss: 1.105663252612609\n",
      "Relative change: 0.070600%\n",
      "Iteration: 649\n",
      "Loss: 1.104883912624542\n",
      "Relative change: 0.070486%\n",
      "Iteration: 650\n",
      "Loss: 1.1041063700294338\n",
      "Relative change: 0.070373%\n",
      "Iteration: 651\n",
      "Loss: 1.1033306176090838\n",
      "Relative change: 0.070261%\n",
      "Iteration: 652\n",
      "Loss: 1.1025566480763538\n",
      "Relative change: 0.070148%\n",
      "Iteration: 653\n",
      "Loss: 1.1017844540780874\n",
      "Relative change: 0.070037%\n",
      "Iteration: 654\n",
      "Loss: 1.1010140281975946\n",
      "Relative change: 0.069925%\n",
      "Iteration: 655\n",
      "Loss: 1.1002453629567164\n",
      "Relative change: 0.069814%\n",
      "Iteration: 656\n",
      "Loss: 1.099478450817499\n",
      "Relative change: 0.069704%\n",
      "Iteration: 657\n",
      "Loss: 1.098713284183505\n",
      "Relative change: 0.069594%\n",
      "Iteration: 658\n",
      "Loss: 1.0979498554007854\n",
      "Relative change: 0.069484%\n",
      "Iteration: 659\n",
      "Loss: 1.0971881567585504\n",
      "Relative change: 0.069375%\n",
      "Iteration: 660\n",
      "Loss: 1.0964281804895677\n",
      "Relative change: 0.069266%\n",
      "Iteration: 661\n",
      "Loss: 1.095669918770317\n",
      "Relative change: 0.069157%\n",
      "Iteration: 662\n",
      "Loss: 1.0949133637209405\n",
      "Relative change: 0.069050%\n",
      "Iteration: 663\n",
      "Loss: 1.0941585074050175\n",
      "Relative change: 0.068942%\n",
      "Iteration: 664\n",
      "Loss: 1.0934053418291951\n",
      "Relative change: 0.068835%\n",
      "Iteration: 665\n",
      "Loss: 1.09265385894271\n",
      "Relative change: 0.068729%\n",
      "Iteration: 666\n",
      "Loss: 1.091904050636829\n",
      "Relative change: 0.068623%\n",
      "Iteration: 667\n",
      "Loss: 1.0911559087442382\n",
      "Relative change: 0.068517%\n",
      "Iteration: 668\n",
      "Loss: 1.0904094250384104\n",
      "Relative change: 0.068412%\n",
      "Iteration: 669\n",
      "Loss: 1.0896645912329728\n",
      "Relative change: 0.068308%\n",
      "Iteration: 670\n",
      "Loss: 1.0889213989811048\n",
      "Relative change: 0.068204%\n",
      "Iteration: 671\n",
      "Loss: 1.0881798398749836\n",
      "Relative change: 0.068100%\n",
      "Iteration: 672\n",
      "Loss: 1.0874399054452992\n",
      "Relative change: 0.067997%\n",
      "Iteration: 673\n",
      "Loss: 1.086701587160858\n",
      "Relative change: 0.067895%\n",
      "Iteration: 674\n",
      "Loss: 1.0859648764282883\n",
      "Relative change: 0.067793%\n",
      "Iteration: 675\n",
      "Loss: 1.0852297645918632\n",
      "Relative change: 0.067692%\n",
      "Iteration: 676\n",
      "Loss: 1.0844962429334515\n",
      "Relative change: 0.067591%\n",
      "Iteration: 677\n",
      "Loss: 1.0837643026726023\n",
      "Relative change: 0.067491%\n",
      "Iteration: 678\n",
      "Loss: 1.0830339349667775\n",
      "Relative change: 0.067392%\n",
      "Iteration: 679\n",
      "Loss: 1.0823051309117278\n",
      "Relative change: 0.067293%\n",
      "Iteration: 680\n",
      "Loss: 1.0815778815420192\n",
      "Relative change: 0.067194%\n",
      "Iteration: 681\n",
      "Loss: 1.0808521778317142\n",
      "Relative change: 0.067097%\n",
      "Iteration: 682\n",
      "Loss: 1.0801280106951954\n",
      "Relative change: 0.067000%\n",
      "Iteration: 683\n",
      "Loss: 1.079405370988141\n",
      "Relative change: 0.066903%\n",
      "Iteration: 684\n",
      "Loss: 1.07868424950864\n",
      "Relative change: 0.066807%\n",
      "Iteration: 685\n",
      "Loss: 1.0779646369984417\n",
      "Relative change: 0.066712%\n",
      "Iteration: 686\n",
      "Loss: 1.0772465241443347\n",
      "Relative change: 0.066617%\n",
      "Iteration: 687\n",
      "Loss: 1.0765299015796461\n",
      "Relative change: 0.066524%\n",
      "Iteration: 688\n",
      "Loss: 1.0758147598858503\n",
      "Relative change: 0.066430%\n",
      "Iteration: 689\n",
      "Loss: 1.0751010895942776\n",
      "Relative change: 0.066338%\n",
      "Iteration: 690\n",
      "Loss: 1.0743888811879145\n",
      "Relative change: 0.066246%\n",
      "Iteration: 691\n",
      "Loss: 1.0736781251032816\n",
      "Relative change: 0.066154%\n",
      "Iteration: 692\n",
      "Loss: 1.0729688117323746\n",
      "Relative change: 0.066064%\n",
      "Iteration: 693\n",
      "Loss: 1.0722609314246645\n",
      "Relative change: 0.065974%\n",
      "Iteration: 694\n",
      "Loss: 1.0715544744891397\n",
      "Relative change: 0.065885%\n",
      "Iteration: 695\n",
      "Loss: 1.0708494311963757\n",
      "Relative change: 0.065796%\n",
      "Iteration: 696\n",
      "Loss: 1.0701457917806292\n",
      "Relative change: 0.065709%\n",
      "Iteration: 697\n",
      "Loss: 1.0694435464419332\n",
      "Relative change: 0.065621%\n",
      "Iteration: 698\n",
      "Loss: 1.0687426853481958\n",
      "Relative change: 0.065535%\n",
      "Iteration: 699\n",
      "Loss: 1.068043198637278\n",
      "Relative change: 0.065449%\n",
      "Iteration: 700\n",
      "Loss: 1.0673450764190513\n",
      "Relative change: 0.065365%\n",
      "Iteration: 701\n",
      "Loss: 1.0666483087774181\n",
      "Relative change: 0.065280%\n",
      "Iteration: 702\n",
      "Loss: 1.065952885772293\n",
      "Relative change: 0.065197%\n",
      "Iteration: 703\n",
      "Loss: 1.065258797441528\n",
      "Relative change: 0.065114%\n",
      "Iteration: 704\n",
      "Loss: 1.0645660338027807\n",
      "Relative change: 0.065032%\n",
      "Iteration: 705\n",
      "Loss: 1.063874584855318\n",
      "Relative change: 0.064951%\n",
      "Iteration: 706\n",
      "Loss: 1.0631844405817459\n",
      "Relative change: 0.064871%\n",
      "Iteration: 707\n",
      "Loss: 1.0624955909496634\n",
      "Relative change: 0.064791%\n",
      "Iteration: 708\n",
      "Loss: 1.0618080259132345\n",
      "Relative change: 0.064712%\n",
      "Iteration: 709\n",
      "Loss: 1.0611217354146771\n",
      "Relative change: 0.064634%\n",
      "Iteration: 710\n",
      "Loss: 1.0604367093856617\n",
      "Relative change: 0.064557%\n",
      "Iteration: 711\n",
      "Loss: 1.0597529377486246\n",
      "Relative change: 0.064480%\n",
      "Iteration: 712\n",
      "Loss: 1.059070410417987\n",
      "Relative change: 0.064404%\n",
      "Iteration: 713\n",
      "Loss: 1.0583891173012918\n",
      "Relative change: 0.064329%\n",
      "Iteration: 714\n",
      "Loss: 1.0577090483002403\n",
      "Relative change: 0.064255%\n",
      "Iteration: 715\n",
      "Loss: 1.0570301933116544\n",
      "Relative change: 0.064182%\n",
      "Iteration: 716\n",
      "Loss: 1.0563525422283442\n",
      "Relative change: 0.064109%\n",
      "Iteration: 717\n",
      "Loss: 1.0556760849399034\n",
      "Relative change: 0.064037%\n",
      "Iteration: 718\n",
      "Loss: 1.0550008113334208\n",
      "Relative change: 0.063966%\n",
      "Iteration: 719\n",
      "Loss: 1.0543267112941281\n",
      "Relative change: 0.063896%\n",
      "Iteration: 720\n",
      "Loss: 1.0536537747059778\n",
      "Relative change: 0.063826%\n",
      "Iteration: 721\n",
      "Loss: 1.0529819914521668\n",
      "Relative change: 0.063757%\n",
      "Iteration: 722\n",
      "Loss: 1.0523113514156084\n",
      "Relative change: 0.063690%\n",
      "Iteration: 723\n",
      "Loss: 1.051641844479365\n",
      "Relative change: 0.063623%\n",
      "Iteration: 724\n",
      "Loss: 1.050973460527047\n",
      "Relative change: 0.063556%\n",
      "Iteration: 725\n",
      "Loss: 1.050306189443193\n",
      "Relative change: 0.063491%\n",
      "Iteration: 726\n",
      "Loss: 1.0496400211136396\n",
      "Relative change: 0.063426%\n",
      "Iteration: 727\n",
      "Loss: 1.048974945425888\n",
      "Relative change: 0.063362%\n",
      "Iteration: 728\n",
      "Loss: 1.0483109522694947\n",
      "Relative change: 0.063299%\n",
      "Iteration: 729\n",
      "Loss: 1.0476480315364773\n",
      "Relative change: 0.063237%\n",
      "Iteration: 730\n",
      "Loss: 1.0469861731217722\n",
      "Relative change: 0.063176%\n",
      "Iteration: 731\n",
      "Loss: 1.04632536692374\n",
      "Relative change: 0.063115%\n",
      "Iteration: 732\n",
      "Loss: 1.0456656028447442\n",
      "Relative change: 0.063055%\n",
      "Iteration: 733\n",
      "Loss: 1.045006870791816\n",
      "Relative change: 0.062996%\n",
      "Iteration: 734\n",
      "Loss: 1.0443491606774171\n",
      "Relative change: 0.062938%\n",
      "Iteration: 735\n",
      "Loss: 1.0436924624203212\n",
      "Relative change: 0.062881%\n",
      "Iteration: 736\n",
      "Loss: 1.0430367659466293\n",
      "Relative change: 0.062825%\n",
      "Iteration: 737\n",
      "Loss: 1.0423820611909298\n",
      "Relative change: 0.062769%\n",
      "Iteration: 738\n",
      "Loss: 1.0417283380976268\n",
      "Relative change: 0.062714%\n",
      "Iteration: 739\n",
      "Loss: 1.0410755866224457\n",
      "Relative change: 0.062660%\n",
      "Iteration: 740\n",
      "Loss: 1.0404237967341365\n",
      "Relative change: 0.062607%\n",
      "Iteration: 741\n",
      "Loss: 1.039772958416383\n",
      "Relative change: 0.062555%\n",
      "Iteration: 742\n",
      "Loss: 1.0391230616699374\n",
      "Relative change: 0.062504%\n",
      "Iteration: 743\n",
      "Loss: 1.0384740965149903\n",
      "Relative change: 0.062453%\n",
      "Iteration: 744\n",
      "Loss: 1.037826052993787\n",
      "Relative change: 0.062403%\n",
      "Iteration: 745\n",
      "Loss: 1.0371789211734994\n",
      "Relative change: 0.062355%\n",
      "Iteration: 746\n",
      "Loss: 1.0365326911493624\n",
      "Relative change: 0.062307%\n",
      "Iteration: 747\n",
      "Loss: 1.03588735304808\n",
      "Relative change: 0.062259%\n",
      "Iteration: 748\n",
      "Loss: 1.0352428970315042\n",
      "Relative change: 0.062213%\n",
      "Iteration: 749\n",
      "Loss: 1.0345993133005875\n",
      "Relative change: 0.062167%\n",
      "Iteration: 750\n",
      "Loss: 1.0339565920996068\n",
      "Relative change: 0.062123%\n",
      "Iteration: 751\n",
      "Loss: 1.0333147237206524\n",
      "Relative change: 0.062079%\n",
      "Iteration: 752\n",
      "Loss: 1.032673698508376\n",
      "Relative change: 0.062036%\n",
      "Iteration: 753\n",
      "Loss: 1.0320335068649806\n",
      "Relative change: 0.061994%\n",
      "Iteration: 754\n",
      "Loss: 1.0313941392554435\n",
      "Relative change: 0.061952%\n",
      "Iteration: 755\n",
      "Loss: 1.0307555862129434\n",
      "Relative change: 0.061912%\n",
      "Iteration: 756\n",
      "Loss: 1.0301178383444773\n",
      "Relative change: 0.061872%\n",
      "Iteration: 757\n",
      "Loss: 1.0294808863366331\n",
      "Relative change: 0.061833%\n",
      "Iteration: 758\n",
      "Loss: 1.0288447209614888\n",
      "Relative change: 0.061795%\n",
      "Iteration: 759\n",
      "Loss: 1.0282093330826052\n",
      "Relative change: 0.061757%\n",
      "Iteration: 760\n",
      "Loss: 1.027574713661071\n",
      "Relative change: 0.061721%\n",
      "Iteration: 761\n",
      "Loss: 1.0269408537615596\n",
      "Relative change: 0.061685%\n",
      "Iteration: 762\n",
      "Loss: 1.0263077445583577\n",
      "Relative change: 0.061650%\n",
      "Iteration: 763\n",
      "Loss: 1.0256753773413094\n",
      "Relative change: 0.061616%\n",
      "Iteration: 764\n",
      "Loss: 1.0250437435216404\n",
      "Relative change: 0.061582%\n",
      "Iteration: 765\n",
      "Loss: 1.024412834637601\n",
      "Relative change: 0.061549%\n",
      "Iteration: 766\n",
      "Loss: 1.023782642359885\n",
      "Relative change: 0.061517%\n",
      "Iteration: 767\n",
      "Loss: 1.0231531584967746\n",
      "Relative change: 0.061486%\n",
      "Iteration: 768\n",
      "Loss: 1.0225243749989545\n",
      "Relative change: 0.061455%\n",
      "Iteration: 769\n",
      "Loss: 1.0218962839639572\n",
      "Relative change: 0.061426%\n",
      "Iteration: 770\n",
      "Loss: 1.0212688776401861\n",
      "Relative change: 0.061396%\n",
      "Iteration: 771\n",
      "Loss: 1.020642148430477\n",
      "Relative change: 0.061368%\n",
      "Iteration: 772\n",
      "Loss: 1.0200160888951597\n",
      "Relative change: 0.061340%\n",
      "Iteration: 773\n",
      "Loss: 1.0193906917545785\n",
      "Relative change: 0.061312%\n",
      "Iteration: 774\n",
      "Loss: 1.0187659498910526\n",
      "Relative change: 0.061286%\n",
      "Iteration: 775\n",
      "Loss: 1.0181418563502391\n",
      "Relative change: 0.061260%\n",
      "Iteration: 776\n",
      "Loss: 1.01751840434189\n",
      "Relative change: 0.061234%\n",
      "Iteration: 777\n",
      "Loss: 1.016895587239984\n",
      "Relative change: 0.061209%\n",
      "Iteration: 778\n",
      "Loss: 1.0162733985822336\n",
      "Relative change: 0.061185%\n",
      "Iteration: 779\n",
      "Loss: 1.015651832068962\n",
      "Relative change: 0.061161%\n",
      "Iteration: 780\n",
      "Loss: 1.0150308815613647\n",
      "Relative change: 0.061138%\n",
      "Iteration: 781\n",
      "Loss: 1.014410541079167\n",
      "Relative change: 0.061115%\n",
      "Iteration: 782\n",
      "Loss: 1.0137908047977024\n",
      "Relative change: 0.061093%\n",
      "Iteration: 783\n",
      "Loss: 1.0131716670444382\n",
      "Relative change: 0.061072%\n",
      "Iteration: 784\n",
      "Loss: 1.0125531222949877\n",
      "Relative change: 0.061050%\n",
      "Iteration: 785\n",
      "Loss: 1.0119351651686472\n",
      "Relative change: 0.061030%\n",
      "Iteration: 786\n",
      "Loss: 1.011317790423507\n",
      "Relative change: 0.061009%\n",
      "Iteration: 787\n",
      "Loss: 1.010700992951186\n",
      "Relative change: 0.060989%\n",
      "Iteration: 788\n",
      "Loss: 1.0100847677712523\n",
      "Relative change: 0.060970%\n",
      "Iteration: 789\n",
      "Loss: 1.0094691100253754\n",
      "Relative change: 0.060951%\n",
      "Iteration: 790\n",
      "Loss: 1.0088540149712877\n",
      "Relative change: 0.060933%\n",
      "Iteration: 791\n",
      "Loss: 1.0082394779766068\n",
      "Relative change: 0.060914%\n",
      "Iteration: 792\n",
      "Loss: 1.0076254945125849\n",
      "Relative change: 0.060897%\n",
      "Iteration: 793\n",
      "Loss: 1.0070120601478476\n",
      "Relative change: 0.060879%\n",
      "Iteration: 794\n",
      "Loss: 1.0063991705421846\n",
      "Relative change: 0.060862%\n",
      "Iteration: 795\n",
      "Loss: 1.0057868214404442\n",
      "Relative change: 0.060846%\n",
      "Iteration: 796\n",
      "Loss: 1.005175008666591\n",
      "Relative change: 0.060829%\n",
      "Iteration: 797\n",
      "Loss: 1.004563728117973\n",
      "Relative change: 0.060813%\n",
      "Iteration: 798\n",
      "Loss: 1.0039529757598387\n",
      "Relative change: 0.060798%\n",
      "Iteration: 799\n",
      "Loss: 1.0033427476201433\n",
      "Relative change: 0.060783%\n",
      "Iteration: 800\n",
      "Loss: 1.0027330397846705\n",
      "Relative change: 0.060768%\n",
      "Iteration: 801\n",
      "Loss: 1.00212384839249\n",
      "Relative change: 0.060753%\n",
      "Iteration: 802\n",
      "Loss: 1.0015151696317663\n",
      "Relative change: 0.060739%\n",
      "Iteration: 803\n",
      "Loss: 1.0009069997359135\n",
      "Relative change: 0.060725%\n",
      "Iteration: 804\n",
      "Loss: 1.0002993349801022\n",
      "Relative change: 0.060711%\n",
      "Iteration: 805\n",
      "Loss: 0.9996921716780945\n",
      "Relative change: 0.060698%\n",
      "Iteration: 806\n",
      "Loss: 0.9990855061793897\n",
      "Relative change: 0.060685%\n",
      "Iteration: 807\n",
      "Loss: 0.9984793348666495\n",
      "Relative change: 0.060673%\n",
      "Iteration: 808\n",
      "Loss: 0.9978736541533678\n",
      "Relative change: 0.060660%\n",
      "Iteration: 809\n",
      "Loss: 0.9972684604817383\n",
      "Relative change: 0.060648%\n",
      "Iteration: 810\n",
      "Loss: 0.9966637503206771\n",
      "Relative change: 0.060637%\n",
      "Iteration: 811\n",
      "Loss: 0.9960595201639443\n",
      "Relative change: 0.060625%\n",
      "Iteration: 812\n",
      "Loss: 0.9954557665283159\n",
      "Relative change: 0.060614%\n",
      "Iteration: 813\n",
      "Loss: 0.9948524859517446\n",
      "Relative change: 0.060603%\n",
      "Iteration: 814\n",
      "Loss: 0.9942496749914616\n",
      "Relative change: 0.060593%\n",
      "Iteration: 815\n",
      "Loss: 0.9936473302219608\n",
      "Relative change: 0.060583%\n",
      "Iteration: 816\n",
      "Loss: 0.9930454482328195\n",
      "Relative change: 0.060573%\n",
      "Iteration: 817\n",
      "Loss: 0.9924440256263038\n",
      "Relative change: 0.060563%\n",
      "Iteration: 818\n",
      "Loss: 0.9918430590147231\n",
      "Relative change: 0.060554%\n",
      "Iteration: 819\n",
      "Loss: 0.9912425450174933\n",
      "Relative change: 0.060545%\n",
      "Iteration: 820\n",
      "Loss: 0.9906424802578847\n",
      "Relative change: 0.060537%\n",
      "Iteration: 821\n",
      "Loss: 0.9900428613594277\n",
      "Relative change: 0.060528%\n",
      "Iteration: 822\n",
      "Loss: 0.9894436849419669\n",
      "Relative change: 0.060520%\n",
      "Iteration: 823\n",
      "Loss: 0.9888449476173589\n",
      "Relative change: 0.060513%\n",
      "Iteration: 824\n",
      "Loss: 0.9882466459848127\n",
      "Relative change: 0.060505%\n",
      "Iteration: 825\n",
      "Loss: 0.9876487766258877\n",
      "Relative change: 0.060498%\n",
      "Iteration: 826\n",
      "Loss: 0.9870513360991702\n",
      "Relative change: 0.060491%\n",
      "Iteration: 827\n",
      "Loss: 0.9864543209346526\n",
      "Relative change: 0.060485%\n",
      "Iteration: 828\n",
      "Loss: 0.9858577276278592\n",
      "Relative change: 0.060479%\n",
      "Iteration: 829\n",
      "Loss: 0.985261552633758\n",
      "Relative change: 0.060473%\n",
      "Iteration: 830\n",
      "Loss: 0.984665792360515\n",
      "Relative change: 0.060467%\n",
      "Iteration: 831\n",
      "Loss: 0.9840704431631495\n",
      "Relative change: 0.060462%\n",
      "Iteration: 832\n",
      "Loss: 0.9834755013371628\n",
      "Relative change: 0.060457%\n",
      "Iteration: 833\n",
      "Loss: 0.9828809631122077\n",
      "Relative change: 0.060453%\n",
      "Iteration: 834\n",
      "Loss: 0.9822868246458887\n",
      "Relative change: 0.060449%\n",
      "Iteration: 835\n",
      "Loss: 0.981693082017773\n",
      "Relative change: 0.060445%\n",
      "Iteration: 836\n",
      "Loss: 0.9810997312237032\n",
      "Relative change: 0.060442%\n",
      "Iteration: 837\n",
      "Loss: 0.9805067681705122\n",
      "Relative change: 0.060439%\n",
      "Iteration: 838\n",
      "Loss: 0.9799141886712309\n",
      "Relative change: 0.060436%\n",
      "Iteration: 839\n",
      "Loss: 0.9793219884408905\n",
      "Relative change: 0.060434%\n",
      "Iteration: 840\n",
      "Loss: 0.9787301630930155\n",
      "Relative change: 0.060432%\n",
      "Iteration: 841\n",
      "Loss: 0.9781387081369035\n",
      "Relative change: 0.060431%\n",
      "Iteration: 842\n",
      "Loss: 0.9775476189757829\n",
      "Relative change: 0.060430%\n",
      "Iteration: 843\n",
      "Loss: 0.9769568909059346\n",
      "Relative change: 0.060430%\n",
      "Iteration: 844\n",
      "Loss: 0.9763665191168532\n",
      "Relative change: 0.060430%\n",
      "Iteration: 845\n",
      "Loss: 0.975776498692519\n",
      "Relative change: 0.060430%\n",
      "Iteration: 846\n",
      "Loss: 0.9751868246138332\n",
      "Relative change: 0.060431%\n",
      "Iteration: 847\n",
      "Loss: 0.974597491762263\n",
      "Relative change: 0.060433%\n",
      "Iteration: 848\n",
      "Loss: 0.9740084949247196\n",
      "Relative change: 0.060435%\n",
      "Iteration: 849\n",
      "Loss: 0.9734198287996875\n",
      "Relative change: 0.060437%\n",
      "Iteration: 850\n",
      "Loss: 0.9728314880045935\n",
      "Relative change: 0.060441%\n",
      "Iteration: 851\n",
      "Loss: 0.9722434670843966\n",
      "Relative change: 0.060444%\n",
      "Iteration: 852\n",
      "Loss: 0.971655760521357\n",
      "Relative change: 0.060448%\n",
      "Iteration: 853\n",
      "Loss: 0.9710683627459247\n",
      "Relative change: 0.060453%\n",
      "Iteration: 854\n",
      "Loss: 0.970481268148673\n",
      "Relative change: 0.060459%\n",
      "Iteration: 855\n",
      "Loss: 0.969894471093184\n",
      "Relative change: 0.060465%\n",
      "Iteration: 856\n",
      "Loss: 0.9693079659297779\n",
      "Relative change: 0.060471%\n",
      "Iteration: 857\n",
      "Loss: 0.9687217470099653\n",
      "Relative change: 0.060478%\n",
      "Iteration: 858\n",
      "Loss: 0.968135808701488\n",
      "Relative change: 0.060486%\n",
      "Iteration: 859\n",
      "Loss: 0.9675501454038052\n",
      "Relative change: 0.060494%\n",
      "Iteration: 860\n",
      "Loss: 0.9669647515638732\n",
      "Relative change: 0.060503%\n",
      "Iteration: 861\n",
      "Loss: 0.9663796216920616\n",
      "Relative change: 0.060512%\n",
      "Iteration: 862\n",
      "Loss: 0.965794750378045\n",
      "Relative change: 0.060522%\n",
      "Iteration: 863\n",
      "Loss: 0.9652101323065087\n",
      "Relative change: 0.060532%\n",
      "Iteration: 864\n",
      "Loss: 0.9646257622725111\n",
      "Relative change: 0.060543%\n",
      "Iteration: 865\n",
      "Loss: 0.964041635196347\n",
      "Relative change: 0.060555%\n",
      "Iteration: 866\n",
      "Loss: 0.9634577461377599\n",
      "Relative change: 0.060567%\n",
      "Iteration: 867\n",
      "Loss: 0.9628740903093697\n",
      "Relative change: 0.060579%\n",
      "Iteration: 868\n",
      "Loss: 0.9622906630891852\n",
      "Relative change: 0.060592%\n",
      "Iteration: 869\n",
      "Loss: 0.9617074600320831\n",
      "Relative change: 0.060606%\n",
      "Iteration: 870\n",
      "Loss: 0.96112447688016\n",
      "Relative change: 0.060620%\n",
      "Iteration: 871\n",
      "Loss: 0.9605417095718666\n",
      "Relative change: 0.060634%\n",
      "Iteration: 872\n",
      "Loss: 0.9599591542498594\n",
      "Relative change: 0.060649%\n",
      "Iteration: 873\n",
      "Loss: 0.9593768072675252\n",
      "Relative change: 0.060664%\n",
      "Iteration: 874\n",
      "Loss: 0.9587946651941381\n",
      "Relative change: 0.060679%\n",
      "Iteration: 875\n",
      "Loss: 0.9582127248186453\n",
      "Relative change: 0.060695%\n",
      "Iteration: 876\n",
      "Loss: 0.9576309831520804\n",
      "Relative change: 0.060711%\n",
      "Iteration: 877\n",
      "Loss: 0.9570494374286243\n",
      "Relative change: 0.060728%\n",
      "Iteration: 878\n",
      "Loss: 0.9564680851053547\n",
      "Relative change: 0.060744%\n",
      "Iteration: 879\n",
      "Loss: 0.9558869238607239\n",
      "Relative change: 0.060761%\n",
      "Iteration: 880\n",
      "Loss: 0.9553059515918325\n",
      "Relative change: 0.060778%\n",
      "Iteration: 881\n",
      "Loss: 0.9547251664105606\n",
      "Relative change: 0.060796%\n",
      "Iteration: 882\n",
      "Loss: 0.954144566638638\n",
      "Relative change: 0.060813%\n",
      "Iteration: 883\n",
      "Loss: 0.953564150801725\n",
      "Relative change: 0.060831%\n",
      "Iteration: 884\n",
      "Loss: 0.9529839176225953\n",
      "Relative change: 0.060849%\n",
      "Iteration: 885\n",
      "Loss: 0.9524038660134946\n",
      "Relative change: 0.060867%\n",
      "Iteration: 886\n",
      "Loss: 0.9518239950677615\n",
      "Relative change: 0.060885%\n",
      "Iteration: 887\n",
      "Loss: 0.9512443040507902\n",
      "Relative change: 0.060903%\n",
      "Iteration: 888\n",
      "Loss: 0.9506647923904059\n",
      "Relative change: 0.060921%\n",
      "Iteration: 889\n",
      "Loss: 0.950085459666734\n",
      "Relative change: 0.060940%\n",
      "Iteration: 890\n",
      "Loss: 0.9495063056016226\n",
      "Relative change: 0.060958%\n",
      "Iteration: 891\n",
      "Loss: 0.9489273300476881\n",
      "Relative change: 0.060976%\n",
      "Iteration: 892\n",
      "Loss: 0.9483485329770444\n",
      "Relative change: 0.060995%\n",
      "Iteration: 893\n",
      "Loss: 0.947769914469772\n",
      "Relative change: 0.061013%\n",
      "Iteration: 894\n",
      "Loss: 0.9471914747021845\n",
      "Relative change: 0.061032%\n",
      "Iteration: 895\n",
      "Loss: 0.9466132139349448\n",
      "Relative change: 0.061050%\n",
      "Iteration: 896\n",
      "Loss: 0.946035132501086\n",
      "Relative change: 0.061068%\n",
      "Iteration: 897\n",
      "Loss: 0.9454572307939872\n",
      "Relative change: 0.061087%\n",
      "Iteration: 898\n",
      "Loss: 0.9448795092553604\n",
      "Relative change: 0.061105%\n",
      "Iteration: 899\n",
      "Loss: 0.9443019683632984\n",
      "Relative change: 0.061123%\n",
      "Iteration: 900\n",
      "Loss: 0.9437246086204435\n",
      "Relative change: 0.061141%\n",
      "Iteration: 901\n",
      "Loss: 0.9431474305423329\n",
      "Relative change: 0.061160%\n",
      "Iteration: 902\n",
      "Loss: 0.9425704346459776\n",
      "Relative change: 0.061178%\n",
      "Iteration: 903\n",
      "Loss: 0.9419936214387443\n",
      "Relative change: 0.061196%\n",
      "Iteration: 904\n",
      "Loss: 0.9414169914075997\n",
      "Relative change: 0.061214%\n",
      "Iteration: 905\n",
      "Loss: 0.9408405450087906\n",
      "Relative change: 0.061232%\n",
      "Iteration: 906\n",
      "Loss: 0.9402642826580341\n",
      "Relative change: 0.061250%\n",
      "Iteration: 907\n",
      "Loss: 0.9396882047212944\n",
      "Relative change: 0.061268%\n",
      "Iteration: 908\n",
      "Loss: 0.9391123115062309\n",
      "Relative change: 0.061286%\n",
      "Iteration: 909\n",
      "Loss: 0.9385366032544056\n",
      "Relative change: 0.061303%\n",
      "Iteration: 910\n",
      "Loss: 0.9379610801343462\n",
      "Relative change: 0.061321%\n",
      "Iteration: 911\n",
      "Loss: 0.9373857422355566\n",
      "Relative change: 0.061339%\n",
      "Iteration: 912\n",
      "Loss: 0.9368105895635886\n",
      "Relative change: 0.061357%\n",
      "Iteration: 913\n",
      "Loss: 0.9362356220362743\n",
      "Relative change: 0.061375%\n",
      "Iteration: 914\n",
      "Loss: 0.935660839481231\n",
      "Relative change: 0.061393%\n",
      "Iteration: 915\n",
      "Loss: 0.9350862416347563\n",
      "Relative change: 0.061411%\n",
      "Iteration: 916\n",
      "Loss: 0.9345118281422164\n",
      "Relative change: 0.061429%\n",
      "Iteration: 917\n",
      "Loss: 0.9339375985600437\n",
      "Relative change: 0.061447%\n",
      "Iteration: 918\n",
      "Loss: 0.9333635523594398\n",
      "Relative change: 0.061465%\n",
      "Iteration: 919\n",
      "Loss: 0.9327896889318855\n",
      "Relative change: 0.061483%\n",
      "Iteration: 920\n",
      "Loss: 0.9322160075965291\n",
      "Relative change: 0.061502%\n",
      "Iteration: 921\n",
      "Loss: 0.931642507609528\n",
      "Relative change: 0.061520%\n",
      "Iteration: 922\n",
      "Loss: 0.9310691881753755\n",
      "Relative change: 0.061539%\n",
      "Iteration: 923\n",
      "Loss: 0.9304960484602429\n",
      "Relative change: 0.061557%\n",
      "Iteration: 924\n",
      "Loss: 0.9299230876073157\n",
      "Relative change: 0.061576%\n",
      "Iteration: 925\n",
      "Loss: 0.9293503047540858\n",
      "Relative change: 0.061595%\n",
      "Iteration: 926\n",
      "Loss: 0.9287776990515169\n",
      "Relative change: 0.061614%\n",
      "Iteration: 927\n",
      "Loss: 0.9282052696849592\n",
      "Relative change: 0.061633%\n",
      "Iteration: 928\n",
      "Loss: 0.9276330158966531\n",
      "Relative change: 0.061652%\n",
      "Iteration: 929\n",
      "Loss: 0.9270609370096131\n",
      "Relative change: 0.061671%\n",
      "Iteration: 930\n",
      "Loss: 0.9264890324526337\n",
      "Relative change: 0.061690%\n",
      "Iteration: 931\n",
      "Loss: 0.9259173017861304\n",
      "Relative change: 0.061709%\n",
      "Iteration: 932\n",
      "Loss: 0.925345744728467\n",
      "Relative change: 0.061729%\n",
      "Iteration: 933\n",
      "Loss: 0.9247743611823955\n",
      "Relative change: 0.061748%\n",
      "Iteration: 934\n",
      "Loss: 0.9242031512611987\n",
      "Relative change: 0.061767%\n",
      "Iteration: 935\n",
      "Loss: 0.923632115314088\n",
      "Relative change: 0.061787%\n",
      "Iteration: 936\n",
      "Loss: 0.9230612539504036\n",
      "Relative change: 0.061806%\n",
      "Iteration: 937\n",
      "Loss: 0.9224905680621406\n",
      "Relative change: 0.061825%\n",
      "Iteration: 938\n",
      "Loss: 0.9219200588443314\n",
      "Relative change: 0.061844%\n",
      "Iteration: 939\n",
      "Loss: 0.921349727812822\n",
      "Relative change: 0.061863%\n",
      "Iteration: 940\n",
      "Loss: 0.920779576819004\n",
      "Relative change: 0.061882%\n",
      "Iteration: 941\n",
      "Loss: 0.9202096080611039\n",
      "Relative change: 0.061901%\n",
      "Iteration: 942\n",
      "Loss: 0.9196398240916722\n",
      "Relative change: 0.061919%\n",
      "Iteration: 943\n",
      "Loss: 0.9190702278209866\n",
      "Relative change: 0.061937%\n",
      "Iteration: 944\n",
      "Loss: 0.9185008225161483\n",
      "Relative change: 0.061954%\n",
      "Iteration: 945\n",
      "Loss: 0.91793161179574\n",
      "Relative change: 0.061972%\n",
      "Iteration: 946\n",
      "Loss: 0.9173625996200024\n",
      "Relative change: 0.061989%\n",
      "Iteration: 947\n",
      "Loss: 0.9167937902765888\n",
      "Relative change: 0.062005%\n",
      "Iteration: 948\n",
      "Loss: 0.9162251883620489\n",
      "Relative change: 0.062021%\n",
      "Iteration: 949\n",
      "Loss: 0.9156567987593124\n",
      "Relative change: 0.062036%\n",
      "Iteration: 950\n",
      "Loss: 0.9150886266115156\n",
      "Relative change: 0.062051%\n",
      "Iteration: 951\n",
      "Loss: 0.9145206772926326\n",
      "Relative change: 0.062065%\n",
      "Iteration: 952\n",
      "Loss: 0.9139529563754337\n",
      "Relative change: 0.062079%\n",
      "Iteration: 953\n",
      "Loss: 0.9133854695973742\n",
      "Relative change: 0.062091%\n",
      "Iteration: 954\n",
      "Loss: 0.9128182228250608\n",
      "Relative change: 0.062104%\n",
      "Iteration: 955\n",
      "Loss: 0.9122512220179916\n",
      "Relative change: 0.062115%\n",
      "Iteration: 956\n",
      "Loss: 0.911684473192263\n",
      "Relative change: 0.062126%\n",
      "Iteration: 957\n",
      "Loss: 0.9111179823849538\n",
      "Relative change: 0.062137%\n",
      "Iteration: 958\n",
      "Loss: 0.9105517556198582\n",
      "Relative change: 0.062146%\n",
      "Iteration: 959\n",
      "Loss: 0.9099857988752067\n",
      "Relative change: 0.062155%\n",
      "Iteration: 960\n",
      "Loss: 0.9094201180539554\n",
      "Relative change: 0.062164%\n",
      "Iteration: 961\n",
      "Loss: 0.9088547189571451\n",
      "Relative change: 0.062171%\n",
      "Iteration: 962\n",
      "Loss: 0.9082896072607609\n",
      "Relative change: 0.062178%\n",
      "Iteration: 963\n",
      "Loss: 0.9077247884964122\n",
      "Relative change: 0.062185%\n",
      "Iteration: 964\n",
      "Loss: 0.9071602680360755\n",
      "Relative change: 0.062191%\n",
      "Iteration: 965\n",
      "Loss: 0.9065960510810261\n",
      "Relative change: 0.062196%\n",
      "Iteration: 966\n",
      "Loss: 0.9060321426550006\n",
      "Relative change: 0.062201%\n",
      "Iteration: 967\n",
      "Loss: 0.905468547601531\n",
      "Relative change: 0.062205%\n",
      "Iteration: 968\n",
      "Loss: 0.9049052705853101\n",
      "Relative change: 0.062208%\n",
      "Iteration: 969\n",
      "Loss: 0.9043423160973707\n",
      "Relative change: 0.062211%\n",
      "Iteration: 970\n",
      "Loss: 0.9037796884637933\n",
      "Relative change: 0.062214%\n",
      "Iteration: 971\n",
      "Loss: 0.9032173918576103\n",
      "Relative change: 0.062216%\n",
      "Iteration: 972\n",
      "Loss: 0.9026554303135211\n",
      "Relative change: 0.062218%\n",
      "Iteration: 973\n",
      "Loss: 0.902093807745017\n",
      "Relative change: 0.062219%\n",
      "Iteration: 974\n",
      "Loss: 0.9015325279634874\n",
      "Relative change: 0.062220%\n",
      "Iteration: 975\n",
      "Loss: 0.9009715946988737\n",
      "Relative change: 0.062220%\n",
      "Iteration: 976\n",
      "Loss: 0.9004110116214433\n",
      "Relative change: 0.062220%\n",
      "Iteration: 977\n",
      "Loss: 0.8998507823642634\n",
      "Relative change: 0.062219%\n",
      "Iteration: 978\n",
      "Loss: 0.8992909105459739\n",
      "Relative change: 0.062218%\n",
      "Iteration: 979\n",
      "Loss: 0.8987313997934837\n",
      "Relative change: 0.062217%\n",
      "Iteration: 980\n",
      "Loss: 0.8981722537642424\n",
      "Relative change: 0.062215%\n",
      "Iteration: 981\n",
      "Loss: 0.8976134761677664\n",
      "Relative change: 0.062213%\n",
      "Iteration: 982\n",
      "Loss: 0.8970550707861414\n",
      "Relative change: 0.062210%\n",
      "Iteration: 983\n",
      "Loss: 0.8964970414932475\n",
      "Relative change: 0.062207%\n",
      "Iteration: 984\n",
      "Loss: 0.8959393922724931\n",
      "Relative change: 0.062203%\n",
      "Iteration: 985\n",
      "Loss: 0.8953821272328789\n",
      "Relative change: 0.062199%\n",
      "Iteration: 986\n",
      "Loss: 0.8948252506232438\n",
      "Relative change: 0.062194%\n",
      "Iteration: 987\n",
      "Loss: 0.8942687668445773\n",
      "Relative change: 0.062189%\n",
      "Iteration: 988\n",
      "Loss: 0.8937126804603174\n",
      "Relative change: 0.062183%\n",
      "Iteration: 989\n",
      "Loss: 0.8931569962045784\n",
      "Relative change: 0.062177%\n",
      "Iteration: 990\n",
      "Loss: 0.892601718988283\n",
      "Relative change: 0.062170%\n",
      "Iteration: 991\n",
      "Loss: 0.8920468539031998\n",
      "Relative change: 0.062163%\n",
      "Iteration: 992\n",
      "Loss: 0.8914924062239118\n",
      "Relative change: 0.062155%\n",
      "Iteration: 993\n",
      "Loss: 0.8909383814077624\n",
      "Relative change: 0.062146%\n",
      "Iteration: 994\n",
      "Loss: 0.8903847850928548\n",
      "Relative change: 0.062136%\n",
      "Iteration: 995\n",
      "Loss: 0.8898316230941867\n",
      "Relative change: 0.062126%\n",
      "Iteration: 996\n",
      "Loss: 0.8892789013980381\n",
      "Relative change: 0.062115%\n",
      "Iteration: 997\n",
      "Loss: 0.8887266261547263\n",
      "Relative change: 0.062104%\n",
      "Iteration: 998\n",
      "Loss: 0.8881748036698781\n",
      "Relative change: 0.062091%\n",
      "Iteration: 999\n",
      "Loss: 0.8876234403943599\n",
      "Relative change: 0.062078%\n",
      "Iteration: 1000\n",
      "Loss: 0.8870725429130328\n",
      "Relative change: 0.062064%\n",
      "Iteration: 1001\n",
      "Loss: 0.8865221179324989\n",
      "Relative change: 0.062050%\n",
      "Iteration: 1002\n",
      "Loss: 0.885972172268013\n",
      "Relative change: 0.062034%\n",
      "Iteration: 1003\n",
      "Loss: 0.8854227128297365\n",
      "Relative change: 0.062018%\n",
      "Iteration: 1004\n",
      "Loss: 0.8848737466085106\n",
      "Relative change: 0.062000%\n",
      "Iteration: 1005\n",
      "Loss: 0.8843252806613265\n",
      "Relative change: 0.061982%\n",
      "Iteration: 1006\n",
      "Loss: 0.883777322096664\n",
      "Relative change: 0.061963%\n",
      "Iteration: 1007\n",
      "Loss: 0.8832298780598659\n",
      "Relative change: 0.061944%\n",
      "Iteration: 1008\n",
      "Loss: 0.8826829557187087\n",
      "Relative change: 0.061923%\n",
      "Iteration: 1009\n",
      "Loss: 0.8821365622493182\n",
      "Relative change: 0.061901%\n",
      "Iteration: 1010\n",
      "Loss: 0.8815907048225737\n",
      "Relative change: 0.061879%\n",
      "Iteration: 1011\n",
      "Loss: 0.8810453905911195\n",
      "Relative change: 0.061856%\n",
      "Iteration: 1012\n",
      "Loss: 0.8805006266771082\n",
      "Relative change: 0.061832%\n",
      "Iteration: 1013\n",
      "Loss: 0.8799564201607639\n",
      "Relative change: 0.061806%\n",
      "Iteration: 1014\n",
      "Loss: 0.8794127780698531\n",
      "Relative change: 0.061781%\n",
      "Iteration: 1015\n",
      "Loss: 0.8788697073701276\n",
      "Relative change: 0.061754%\n",
      "Iteration: 1016\n",
      "Loss: 0.8783272149567853\n",
      "Relative change: 0.061726%\n",
      "Iteration: 1017\n",
      "Loss: 0.8777853076469846\n",
      "Relative change: 0.061698%\n",
      "Iteration: 1018\n",
      "Loss: 0.8772439921734181\n",
      "Relative change: 0.061668%\n",
      "Iteration: 1019\n",
      "Loss: 0.8767032751789505\n",
      "Relative change: 0.061638%\n",
      "Iteration: 1020\n",
      "Loss: 0.8761631632122966\n",
      "Relative change: 0.061607%\n",
      "Iteration: 1021\n",
      "Loss: 0.8756236627247032\n",
      "Relative change: 0.061575%\n",
      "Iteration: 1022\n",
      "Loss: 0.8750847800675984\n",
      "Relative change: 0.061543%\n",
      "Iteration: 1023\n",
      "Loss: 0.8745465214911371\n",
      "Relative change: 0.061509%\n",
      "Iteration: 1024\n",
      "Loss: 0.8740088931435894\n",
      "Relative change: 0.061475%\n",
      "Iteration: 1025\n",
      "Loss: 0.8734719010714905\n",
      "Relative change: 0.061440%\n",
      "Iteration: 1026\n",
      "Loss: 0.87293555122048\n",
      "Relative change: 0.061404%\n",
      "Iteration: 1027\n",
      "Loss: 0.8723998494367506\n",
      "Relative change: 0.061368%\n",
      "Iteration: 1028\n",
      "Loss: 0.8718648014690318\n",
      "Relative change: 0.061331%\n",
      "Iteration: 1029\n",
      "Loss: 0.8713304129710272\n",
      "Relative change: 0.061293%\n",
      "Iteration: 1030\n",
      "Loss: 0.8707966895042423\n",
      "Relative change: 0.061254%\n",
      "Iteration: 1031\n",
      "Loss: 0.8702636365411289\n",
      "Relative change: 0.061214%\n",
      "Iteration: 1032\n",
      "Loss: 0.8697312594684921\n",
      "Relative change: 0.061174%\n",
      "Iteration: 1033\n",
      "Loss: 0.8691995635911032\n",
      "Relative change: 0.061133%\n",
      "Iteration: 1034\n",
      "Loss: 0.8686685541354718\n",
      "Relative change: 0.061092%\n",
      "Iteration: 1035\n",
      "Loss: 0.8681382362537361\n",
      "Relative change: 0.061050%\n",
      "Iteration: 1036\n",
      "Loss: 0.8676086150276349\n",
      "Relative change: 0.061007%\n",
      "Iteration: 1037\n",
      "Loss: 0.8670796954725307\n",
      "Relative change: 0.060963%\n",
      "Iteration: 1038\n",
      "Loss: 0.8665514825414568\n",
      "Relative change: 0.060919%\n",
      "Iteration: 1039\n",
      "Loss: 0.86602398112916\n",
      "Relative change: 0.060874%\n",
      "Iteration: 1040\n",
      "Loss: 0.8654971960761189\n",
      "Relative change: 0.060828%\n",
      "Iteration: 1041\n",
      "Loss: 0.8649711321725153\n",
      "Relative change: 0.060782%\n",
      "Iteration: 1042\n",
      "Loss: 0.8644457941621324\n",
      "Relative change: 0.060735%\n",
      "Iteration: 1043\n",
      "Loss: 0.8639211867461626\n",
      "Relative change: 0.060687%\n",
      "Iteration: 1044\n",
      "Loss: 0.8633973145868966\n",
      "Relative change: 0.060639%\n",
      "Iteration: 1045\n",
      "Loss: 0.8628741823112762\n",
      "Relative change: 0.060590%\n",
      "Iteration: 1046\n",
      "Loss: 0.8623517945142775\n",
      "Relative change: 0.060540%\n",
      "Iteration: 1047\n",
      "Loss: 0.8618301557621132\n",
      "Relative change: 0.060490%\n",
      "Iteration: 1048\n",
      "Loss: 0.8613092705952194\n",
      "Relative change: 0.060439%\n",
      "Iteration: 1049\n",
      "Loss: 0.8607891435310102\n",
      "Relative change: 0.060388%\n",
      "Iteration: 1050\n",
      "Loss: 0.8602697790663789\n",
      "Relative change: 0.060336%\n",
      "Iteration: 1051\n",
      "Loss: 0.8597511816799258\n",
      "Relative change: 0.060283%\n",
      "Iteration: 1052\n",
      "Loss: 0.8592333558339001\n",
      "Relative change: 0.060230%\n",
      "Iteration: 1053\n",
      "Loss: 0.8587163059758404\n",
      "Relative change: 0.060176%\n",
      "Iteration: 1054\n",
      "Loss: 0.8582000365399083\n",
      "Relative change: 0.060121%\n",
      "Iteration: 1055\n",
      "Loss: 0.8576845519479096\n",
      "Relative change: 0.060066%\n",
      "Iteration: 1056\n",
      "Loss: 0.857169856610002\n",
      "Relative change: 0.060010%\n",
      "Iteration: 1057\n",
      "Loss: 0.856655954925096\n",
      "Relative change: 0.059953%\n",
      "Iteration: 1058\n",
      "Loss: 0.8561428512809497\n",
      "Relative change: 0.059896%\n",
      "Iteration: 1059\n",
      "Loss: 0.8556305500539781\n",
      "Relative change: 0.059838%\n",
      "Iteration: 1060\n",
      "Loss: 0.8551190556087803\n",
      "Relative change: 0.059780%\n",
      "Iteration: 1061\n",
      "Loss: 0.8546083722974078\n",
      "Relative change: 0.059721%\n",
      "Iteration: 1062\n",
      "Loss: 0.8540985044583883\n",
      "Relative change: 0.059661%\n",
      "Iteration: 1063\n",
      "Loss: 0.8535894564155262\n",
      "Relative change: 0.059601%\n",
      "Iteration: 1064\n",
      "Loss: 0.8530812324764985\n",
      "Relative change: 0.059540%\n",
      "Iteration: 1065\n",
      "Loss: 0.8525738369312641\n",
      "Relative change: 0.059478%\n",
      "Iteration: 1066\n",
      "Loss: 0.8520672740503134\n",
      "Relative change: 0.059416%\n",
      "Iteration: 1067\n",
      "Loss: 0.8515615480827667\n",
      "Relative change: 0.059353%\n",
      "Iteration: 1068\n",
      "Loss: 0.8510566632543504\n",
      "Relative change: 0.059289%\n",
      "Iteration: 1069\n",
      "Loss: 0.8505526237652558\n",
      "Relative change: 0.059225%\n",
      "Iteration: 1070\n",
      "Loss: 0.8500494337879069\n",
      "Relative change: 0.059160%\n",
      "Iteration: 1071\n",
      "Loss: 0.849547097464643\n",
      "Relative change: 0.059095%\n",
      "Iteration: 1072\n",
      "Loss: 0.8490456189053288\n",
      "Relative change: 0.059029%\n",
      "Iteration: 1073\n",
      "Loss: 0.8485450021849069\n",
      "Relative change: 0.058962%\n",
      "Iteration: 1074\n",
      "Loss: 0.8480452513408959\n",
      "Relative change: 0.058895%\n",
      "Iteration: 1075\n",
      "Loss: 0.8475463703708452\n",
      "Relative change: 0.058827%\n",
      "Iteration: 1076\n",
      "Loss: 0.8470483632297551\n",
      "Relative change: 0.058759%\n",
      "Iteration: 1077\n",
      "Loss: 0.8465512338274619\n",
      "Relative change: 0.058690%\n",
      "Iteration: 1078\n",
      "Loss: 0.8460549860260038\n",
      "Relative change: 0.058620%\n",
      "Iteration: 1079\n",
      "Loss: 0.8455596236369648\n",
      "Relative change: 0.058550%\n",
      "Iteration: 1080\n",
      "Loss: 0.8450651504188083\n",
      "Relative change: 0.058479%\n",
      "Iteration: 1081\n",
      "Loss: 0.8445715700742056\n",
      "Relative change: 0.058407%\n",
      "Iteration: 1082\n",
      "Loss: 0.8440788862473677\n",
      "Relative change: 0.058335%\n",
      "Iteration: 1083\n",
      "Loss: 0.8435871025213874\n",
      "Relative change: 0.058263%\n",
      "Iteration: 1084\n",
      "Loss: 0.8430962224156039\n",
      "Relative change: 0.058190%\n",
      "Iteration: 1085\n",
      "Loss: 0.8426062493830013\n",
      "Relative change: 0.058116%\n",
      "Iteration: 1086\n",
      "Loss: 0.8421171868076506\n",
      "Relative change: 0.058042%\n",
      "Iteration: 1087\n",
      "Loss: 0.8416290380022113\n",
      "Relative change: 0.057967%\n",
      "Iteration: 1088\n",
      "Loss: 0.8411418062055098\n",
      "Relative change: 0.057892%\n",
      "Iteration: 1089\n",
      "Loss: 0.8406554945802042\n",
      "Relative change: 0.057816%\n",
      "Iteration: 1090\n",
      "Loss: 0.8401701062105604\n",
      "Relative change: 0.057739%\n",
      "Iteration: 1091\n",
      "Loss: 0.8396856441003512\n",
      "Relative change: 0.057662%\n",
      "Iteration: 1092\n",
      "Loss: 0.8392021111708973\n",
      "Relative change: 0.057585%\n",
      "Iteration: 1093\n",
      "Loss: 0.8387195102592694\n",
      "Relative change: 0.057507%\n",
      "Iteration: 1094\n",
      "Loss: 0.838237844116667\n",
      "Relative change: 0.057429%\n",
      "Iteration: 1095\n",
      "Loss: 0.8377571154069862\n",
      "Relative change: 0.057350%\n",
      "Iteration: 1096\n",
      "Loss: 0.8372773267055974\n",
      "Relative change: 0.057271%\n",
      "Iteration: 1097\n",
      "Loss: 0.8367984804983369\n",
      "Relative change: 0.057191%\n",
      "Iteration: 1098\n",
      "Loss: 0.8363205791807278\n",
      "Relative change: 0.057111%\n",
      "Iteration: 1099\n",
      "Loss: 0.8358436250574338\n",
      "Relative change: 0.057030%\n",
      "Iteration: 1100\n",
      "Loss: 0.8353676203419497\n",
      "Relative change: 0.056949%\n",
      "Iteration: 1101\n",
      "Loss: 0.8348925671565296\n",
      "Relative change: 0.056868%\n",
      "Iteration: 1102\n",
      "Loss: 0.8344184675323494\n",
      "Relative change: 0.056786%\n",
      "Iteration: 1103\n",
      "Loss: 0.8339453234098961\n",
      "Relative change: 0.056703%\n",
      "Iteration: 1104\n",
      "Loss: 0.8334731366395737\n",
      "Relative change: 0.056621%\n",
      "Iteration: 1105\n",
      "Loss: 0.8330019089825147\n",
      "Relative change: 0.056538%\n",
      "Iteration: 1106\n",
      "Loss: 0.8325316421115756\n",
      "Relative change: 0.056454%\n",
      "Iteration: 1107\n",
      "Loss: 0.8320623376125053\n",
      "Relative change: 0.056371%\n",
      "Iteration: 1108\n",
      "Loss: 0.8315939969852547\n",
      "Relative change: 0.056287%\n",
      "Iteration: 1109\n",
      "Loss: 0.8311266216454141\n",
      "Relative change: 0.056202%\n",
      "Iteration: 1110\n",
      "Loss: 0.8306602129257464\n",
      "Relative change: 0.056118%\n",
      "Iteration: 1111\n",
      "Loss: 0.8301947720777966\n",
      "Relative change: 0.056033%\n",
      "Iteration: 1112\n",
      "Loss: 0.8297303002735459\n",
      "Relative change: 0.055947%\n",
      "Iteration: 1113\n",
      "Loss: 0.8292667986070916\n",
      "Relative change: 0.055862%\n",
      "Iteration: 1114\n",
      "Loss: 0.8288042680963231\n",
      "Relative change: 0.055776%\n",
      "Iteration: 1115\n",
      "Loss: 0.8283427096845719\n",
      "Relative change: 0.055690%\n",
      "Iteration: 1116\n",
      "Loss: 0.8278821242422194\n",
      "Relative change: 0.055603%\n",
      "Iteration: 1117\n",
      "Loss: 0.8274225125682331\n",
      "Relative change: 0.055517%\n",
      "Iteration: 1118\n",
      "Loss: 0.8269638753916257\n",
      "Relative change: 0.055430%\n",
      "Iteration: 1119\n",
      "Loss: 0.826506213372814\n",
      "Relative change: 0.055342%\n",
      "Iteration: 1120\n",
      "Loss: 0.8260495271048713\n",
      "Relative change: 0.055255%\n",
      "Iteration: 1121\n",
      "Loss: 0.8255938171146642\n",
      "Relative change: 0.055167%\n",
      "Iteration: 1122\n",
      "Loss: 0.8251390838638666\n",
      "Relative change: 0.055080%\n",
      "Iteration: 1123\n",
      "Loss: 0.8246853277498531\n",
      "Relative change: 0.054991%\n",
      "Iteration: 1124\n",
      "Loss: 0.8242325491064655\n",
      "Relative change: 0.054903%\n",
      "Iteration: 1125\n",
      "Loss: 0.8237807482046623\n",
      "Relative change: 0.054815%\n",
      "Iteration: 1126\n",
      "Loss: 0.8233299252530537\n",
      "Relative change: 0.054726%\n",
      "Iteration: 1127\n",
      "Loss: 0.8228800803983288\n",
      "Relative change: 0.054637%\n",
      "Iteration: 1128\n",
      "Loss: 0.8224312137255864\n",
      "Relative change: 0.054548%\n",
      "Iteration: 1129\n",
      "Loss: 0.8219833252585816\n",
      "Relative change: 0.054459%\n",
      "Iteration: 1130\n",
      "Loss: 0.8215364149598982\n",
      "Relative change: 0.054370%\n",
      "Iteration: 1131\n",
      "Loss: 0.8210904827310594\n",
      "Relative change: 0.054280%\n",
      "Iteration: 1132\n",
      "Loss: 0.8206455284125956\n",
      "Relative change: 0.054191%\n",
      "Iteration: 1133\n",
      "Loss: 0.8202015517840782\n",
      "Relative change: 0.054101%\n",
      "Iteration: 1134\n",
      "Loss: 0.8197585525641333\n",
      "Relative change: 0.054011%\n",
      "Iteration: 1135\n",
      "Loss: 0.8193165304104537\n",
      "Relative change: 0.053921%\n",
      "Iteration: 1136\n",
      "Loss: 0.8188754849198164\n",
      "Relative change: 0.053831%\n",
      "Iteration: 1137\n",
      "Loss: 0.8184354156281205\n",
      "Relative change: 0.053741%\n",
      "Iteration: 1138\n",
      "Loss: 0.8179963220104564\n",
      "Relative change: 0.053650%\n",
      "Iteration: 1139\n",
      "Loss: 0.8175582034812137\n",
      "Relative change: 0.053560%\n",
      "Iteration: 1140\n",
      "Loss: 0.8171210593942405\n",
      "Relative change: 0.053469%\n",
      "Iteration: 1141\n",
      "Loss: 0.8166848890430554\n",
      "Relative change: 0.053379%\n",
      "Iteration: 1142\n",
      "Loss: 0.8162496916611236\n",
      "Relative change: 0.053288%\n",
      "Iteration: 1143\n",
      "Loss: 0.8158154664221968\n",
      "Relative change: 0.053198%\n",
      "Iteration: 1144\n",
      "Loss: 0.8153822124407224\n",
      "Relative change: 0.053107%\n",
      "Iteration: 1145\n",
      "Loss: 0.814949928772322\n",
      "Relative change: 0.053016%\n",
      "Iteration: 1146\n",
      "Loss: 0.8145186144143403\n",
      "Relative change: 0.052925%\n",
      "Iteration: 1147\n",
      "Loss: 0.8140882683064627\n",
      "Relative change: 0.052834%\n",
      "Iteration: 1148\n",
      "Loss: 0.8136588893314\n",
      "Relative change: 0.052744%\n",
      "Iteration: 1149\n",
      "Loss: 0.8132304763156374\n",
      "Relative change: 0.052653%\n",
      "Iteration: 1150\n",
      "Loss: 0.8128030280302435\n",
      "Relative change: 0.052562%\n",
      "Iteration: 1151\n",
      "Loss: 0.8123765431917355\n",
      "Relative change: 0.052471%\n",
      "Iteration: 1152\n",
      "Loss: 0.8119510204629943\n",
      "Relative change: 0.052380%\n",
      "Iteration: 1153\n",
      "Loss: 0.8115264584542271\n",
      "Relative change: 0.052289%\n",
      "Iteration: 1154\n",
      "Loss: 0.8111028557239692\n",
      "Relative change: 0.052198%\n",
      "Iteration: 1155\n",
      "Loss: 0.8106802107801185\n",
      "Relative change: 0.052107%\n",
      "Iteration: 1156\n",
      "Loss: 0.8102585220810015\n",
      "Relative change: 0.052017%\n",
      "Iteration: 1157\n",
      "Loss: 0.8098377880364597\n",
      "Relative change: 0.051926%\n",
      "Iteration: 1158\n",
      "Loss: 0.8094180070089556\n",
      "Relative change: 0.051835%\n",
      "Iteration: 1159\n",
      "Loss: 0.8089991773146887\n",
      "Relative change: 0.051745%\n",
      "Iteration: 1160\n",
      "Loss: 0.8085812972247199\n",
      "Relative change: 0.051654%\n",
      "Iteration: 1161\n",
      "Loss: 0.8081643649660984\n",
      "Relative change: 0.051563%\n",
      "Iteration: 1162\n",
      "Loss: 0.8077483787229871\n",
      "Relative change: 0.051473%\n",
      "Iteration: 1163\n",
      "Loss: 0.8073333366377812\n",
      "Relative change: 0.051383%\n",
      "Iteration: 1164\n",
      "Loss: 0.8069192368122207\n",
      "Relative change: 0.051292%\n",
      "Iteration: 1165\n",
      "Loss: 0.8065060773084889\n",
      "Relative change: 0.051202%\n",
      "Iteration: 1166\n",
      "Loss: 0.8060938561502975\n",
      "Relative change: 0.051112%\n",
      "Iteration: 1167\n",
      "Loss: 0.8056825713239572\n",
      "Relative change: 0.051022%\n",
      "Iteration: 1168\n",
      "Loss: 0.8052722207794283\n",
      "Relative change: 0.050932%\n",
      "Iteration: 1169\n",
      "Loss: 0.8048628024313541\n",
      "Relative change: 0.050842%\n",
      "Iteration: 1170\n",
      "Loss: 0.8044543141600751\n",
      "Relative change: 0.050753%\n",
      "Iteration: 1171\n",
      "Loss: 0.8040467538126213\n",
      "Relative change: 0.050663%\n",
      "Iteration: 1172\n",
      "Loss: 0.8036401192036859\n",
      "Relative change: 0.050574%\n",
      "Iteration: 1173\n",
      "Loss: 0.803234408116581\n",
      "Relative change: 0.050484%\n",
      "Iteration: 1174\n",
      "Loss: 0.8028296183041668\n",
      "Relative change: 0.050395%\n",
      "Iteration: 1175\n",
      "Loss: 0.8024257474897707\n",
      "Relative change: 0.050306%\n",
      "Iteration: 1176\n",
      "Loss: 0.8020227933680814\n",
      "Relative change: 0.050217%\n",
      "Iteration: 1177\n",
      "Loss: 0.8016207536060282\n",
      "Relative change: 0.050128%\n",
      "Iteration: 1178\n",
      "Loss: 0.8012196258436444\n",
      "Relative change: 0.050040%\n",
      "Iteration: 1179\n",
      "Loss: 0.8008194076949124\n",
      "Relative change: 0.049951%\n",
      "Iteration: 1180\n",
      "Loss: 0.8004200967485983\n",
      "Relative change: 0.049863%\n",
      "Iteration: 1181\n",
      "Loss: 0.8000216905690695\n",
      "Relative change: 0.049775%\n",
      "Iteration: 1182\n",
      "Loss: 0.799624186697104\n",
      "Relative change: 0.049687%\n",
      "Iteration: 1183\n",
      "Loss: 0.7992275826506845\n",
      "Relative change: 0.049599%\n",
      "Iteration: 1184\n",
      "Loss: 0.7988318759257851\n",
      "Relative change: 0.049511%\n",
      "Iteration: 1185\n",
      "Loss: 0.7984370639971483\n",
      "Relative change: 0.049424%\n",
      "Iteration: 1186\n",
      "Loss: 0.7980431443190529\n",
      "Relative change: 0.049336%\n",
      "Iteration: 1187\n",
      "Loss: 0.7976501143260764\n",
      "Relative change: 0.049249%\n",
      "Iteration: 1188\n",
      "Loss: 0.7972579714338476\n",
      "Relative change: 0.049162%\n",
      "Iteration: 1189\n",
      "Loss: 0.7968667130397962\n",
      "Relative change: 0.049076%\n",
      "Iteration: 1190\n",
      "Loss: 0.7964763365238948\n",
      "Relative change: 0.048989%\n",
      "Iteration: 1191\n",
      "Loss: 0.7960868392493969\n",
      "Relative change: 0.048903%\n",
      "Iteration: 1192\n",
      "Loss: 0.7956982185635704\n",
      "Relative change: 0.048816%\n",
      "Iteration: 1193\n",
      "Loss: 0.7953104717984231\n",
      "Relative change: 0.048730%\n",
      "Iteration: 1194\n",
      "Loss: 0.7949235962714298\n",
      "Relative change: 0.048645%\n",
      "Iteration: 1195\n",
      "Loss: 0.7945375892862505\n",
      "Relative change: 0.048559%\n",
      "Iteration: 1196\n",
      "Loss: 0.7941524481334457\n",
      "Relative change: 0.048474%\n",
      "Iteration: 1197\n",
      "Loss: 0.793768170091187\n",
      "Relative change: 0.048388%\n",
      "Iteration: 1198\n",
      "Loss: 0.793384752425965\n",
      "Relative change: 0.048303%\n",
      "Iteration: 1199\n",
      "Loss: 0.7930021923932892\n",
      "Relative change: 0.048219%\n",
      "Iteration: 1200\n",
      "Loss: 0.7926204872383863\n",
      "Relative change: 0.048134%\n",
      "Iteration: 1201\n",
      "Loss: 0.7922396341968916\n",
      "Relative change: 0.048050%\n",
      "Iteration: 1202\n",
      "Loss: 0.7918596304955332\n",
      "Relative change: 0.047966%\n",
      "Iteration: 1203\n",
      "Loss: 0.7914804733528152\n",
      "Relative change: 0.047882%\n",
      "Iteration: 1204\n",
      "Loss: 0.7911021599796887\n",
      "Relative change: 0.047798%\n",
      "Iteration: 1205\n",
      "Loss: 0.7907246875802197\n",
      "Relative change: 0.047715%\n",
      "Iteration: 1206\n",
      "Loss: 0.7903480533522498\n",
      "Relative change: 0.047632%\n",
      "Iteration: 1207\n",
      "Loss: 0.7899722544880463\n",
      "Relative change: 0.047549%\n",
      "Iteration: 1208\n",
      "Loss: 0.7895972881749489\n",
      "Relative change: 0.047466%\n",
      "Iteration: 1209\n",
      "Loss: 0.7892231515960032\n",
      "Relative change: 0.047383%\n",
      "Iteration: 1210\n",
      "Loss: 0.788849841930588\n",
      "Relative change: 0.047301%\n",
      "Iteration: 1211\n",
      "Loss: 0.788477356355034\n",
      "Relative change: 0.047219%\n",
      "Iteration: 1212\n",
      "Loss: 0.7881056920432301\n",
      "Relative change: 0.047137%\n",
      "Iteration: 1213\n",
      "Loss: 0.7877348461672213\n",
      "Relative change: 0.047055%\n",
      "Iteration: 1214\n",
      "Loss: 0.7873648158977962\n",
      "Relative change: 0.046974%\n",
      "Iteration: 1215\n",
      "Loss: 0.7869955984050632\n",
      "Relative change: 0.046893%\n",
      "Iteration: 1216\n",
      "Loss: 0.7866271908590149\n",
      "Relative change: 0.046812%\n",
      "Iteration: 1217\n",
      "Loss: 0.7862595904300834\n",
      "Relative change: 0.046731%\n",
      "Iteration: 1218\n",
      "Loss: 0.785892794289681\n",
      "Relative change: 0.046651%\n",
      "Iteration: 1219\n",
      "Loss: 0.7855267996107296\n",
      "Relative change: 0.046571%\n",
      "Iteration: 1220\n",
      "Loss: 0.78516160356818\n",
      "Relative change: 0.046491%\n",
      "Iteration: 1221\n",
      "Loss: 0.7847972033395159\n",
      "Relative change: 0.046411%\n",
      "Iteration: 1222\n",
      "Loss: 0.7844335961052471\n",
      "Relative change: 0.046331%\n",
      "Iteration: 1223\n",
      "Loss: 0.7840707790493893\n",
      "Relative change: 0.046252%\n",
      "Iteration: 1224\n",
      "Loss: 0.7837087493599302\n",
      "Relative change: 0.046173%\n",
      "Iteration: 1225\n",
      "Loss: 0.783347504229284\n",
      "Relative change: 0.046094%\n",
      "Iteration: 1226\n",
      "Loss: 0.7829870408547318\n",
      "Relative change: 0.046016%\n",
      "Iteration: 1227\n",
      "Loss: 0.7826273564388488\n",
      "Relative change: 0.045937%\n",
      "Iteration: 1228\n",
      "Loss: 0.7822684481899191\n",
      "Relative change: 0.045859%\n",
      "Iteration: 1229\n",
      "Loss: 0.7819103133223356\n",
      "Relative change: 0.045782%\n",
      "Iteration: 1230\n",
      "Loss: 0.7815529490569874\n",
      "Relative change: 0.045704%\n",
      "Iteration: 1231\n",
      "Loss: 0.7811963526216343\n",
      "Relative change: 0.045627%\n",
      "Iteration: 1232\n",
      "Loss: 0.7808405212512675\n",
      "Relative change: 0.045550%\n",
      "Iteration: 1233\n",
      "Loss: 0.7804854521884563\n",
      "Relative change: 0.045473%\n",
      "Iteration: 1234\n",
      "Loss: 0.780131142683683\n",
      "Relative change: 0.045396%\n",
      "Iteration: 1235\n",
      "Loss: 0.7797775899956626\n",
      "Relative change: 0.045320%\n",
      "Iteration: 1236\n",
      "Loss: 0.7794247913916519\n",
      "Relative change: 0.045243%\n",
      "Iteration: 1237\n",
      "Loss: 0.779072744147743\n",
      "Relative change: 0.045168%\n",
      "Iteration: 1238\n",
      "Loss: 0.7787214455491446\n",
      "Relative change: 0.045092%\n",
      "Iteration: 1239\n",
      "Loss: 0.7783708928904521\n",
      "Relative change: 0.045016%\n",
      "Iteration: 1240\n",
      "Loss: 0.7780210834759023\n",
      "Relative change: 0.044941%\n",
      "Iteration: 1241\n",
      "Loss: 0.7776720146196162\n",
      "Relative change: 0.044866%\n",
      "Iteration: 1242\n",
      "Loss: 0.7773236836458312\n",
      "Relative change: 0.044792%\n",
      "Iteration: 1243\n",
      "Loss: 0.776976087889118\n",
      "Relative change: 0.044717%\n",
      "Iteration: 1244\n",
      "Loss: 0.7766292246945866\n",
      "Relative change: 0.044643%\n",
      "Iteration: 1245\n",
      "Loss: 0.7762830914180809\n",
      "Relative change: 0.044569%\n",
      "Iteration: 1246\n",
      "Loss: 0.7759376854263601\n",
      "Relative change: 0.044495%\n",
      "Iteration: 1247\n",
      "Loss: 0.7755930040972687\n",
      "Relative change: 0.044421%\n",
      "Iteration: 1248\n",
      "Loss: 0.7752490448198949\n",
      "Relative change: 0.044348%\n",
      "Iteration: 1249\n",
      "Loss: 0.7749058049947176\n",
      "Relative change: 0.044275%\n",
      "Iteration: 1250\n",
      "Loss: 0.7745632820337421\n",
      "Relative change: 0.044202%\n",
      "Iteration: 1251\n",
      "Loss: 0.7742214733606251\n",
      "Relative change: 0.044129%\n",
      "Iteration: 1252\n",
      "Loss: 0.7738803764107879\n",
      "Relative change: 0.044057%\n",
      "Iteration: 1253\n",
      "Loss: 0.7735399886315197\n",
      "Relative change: 0.043985%\n",
      "Iteration: 1254\n",
      "Loss: 0.7732003074820704\n",
      "Relative change: 0.043913%\n",
      "Iteration: 1255\n",
      "Loss: 0.772861330433733\n",
      "Relative change: 0.043841%\n",
      "Iteration: 1256\n",
      "Loss: 0.7725230549699152\n",
      "Relative change: 0.043769%\n",
      "Iteration: 1257\n",
      "Loss: 0.7721854785862036\n",
      "Relative change: 0.043698%\n",
      "Iteration: 1258\n",
      "Loss: 0.7718485987904149\n",
      "Relative change: 0.043627%\n",
      "Iteration: 1259\n",
      "Loss: 0.771512413102641\n",
      "Relative change: 0.043556%\n",
      "Iteration: 1260\n",
      "Loss: 0.7711769190552823\n",
      "Relative change: 0.043485%\n",
      "Iteration: 1261\n",
      "Loss: 0.7708421141930751\n",
      "Relative change: 0.043415%\n",
      "Iteration: 1262\n",
      "Loss: 0.7705079960731082\n",
      "Relative change: 0.043345%\n",
      "Iteration: 1263\n",
      "Loss: 0.7701745622648302\n",
      "Relative change: 0.043275%\n",
      "Iteration: 1264\n",
      "Loss: 0.7698418103500534\n",
      "Relative change: 0.043205%\n",
      "Iteration: 1265\n",
      "Loss: 0.7695097379229445\n",
      "Relative change: 0.043135%\n",
      "Iteration: 1266\n",
      "Loss: 0.7691783425900114\n",
      "Relative change: 0.043066%\n",
      "Iteration: 1267\n",
      "Loss: 0.7688476219700809\n",
      "Relative change: 0.042997%\n",
      "Iteration: 1268\n",
      "Loss: 0.7685175736942707\n",
      "Relative change: 0.042928%\n",
      "Iteration: 1269\n",
      "Loss: 0.7681881954059532\n",
      "Relative change: 0.042859%\n",
      "Iteration: 1270\n",
      "Loss: 0.7678594847607151\n",
      "Relative change: 0.042790%\n",
      "Iteration: 1271\n",
      "Loss: 0.7675314394263086\n",
      "Relative change: 0.042722%\n",
      "Iteration: 1272\n",
      "Loss: 0.767204057082598\n",
      "Relative change: 0.042654%\n",
      "Iteration: 1273\n",
      "Loss: 0.7668773354215006\n",
      "Relative change: 0.042586%\n",
      "Iteration: 1274\n",
      "Loss: 0.7665512721469225\n",
      "Relative change: 0.042518%\n",
      "Iteration: 1275\n",
      "Loss: 0.766225864974689\n",
      "Relative change: 0.042451%\n",
      "Iteration: 1276\n",
      "Loss: 0.7659011116324699\n",
      "Relative change: 0.042384%\n",
      "Iteration: 1277\n",
      "Loss: 0.7655770098597028\n",
      "Relative change: 0.042316%\n",
      "Iteration: 1278\n",
      "Loss: 0.7652535574075083\n",
      "Relative change: 0.042249%\n",
      "Iteration: 1279\n",
      "Loss: 0.7649307520386047\n",
      "Relative change: 0.042183%\n",
      "Iteration: 1280\n",
      "Loss: 0.7646085915272184\n",
      "Relative change: 0.042116%\n",
      "Iteration: 1281\n",
      "Loss: 0.7642870736589897\n",
      "Relative change: 0.042050%\n",
      "Iteration: 1282\n",
      "Loss: 0.7639661962308772\n",
      "Relative change: 0.041984%\n",
      "Iteration: 1283\n",
      "Loss: 0.7636459570510584\n",
      "Relative change: 0.041918%\n",
      "Iteration: 1284\n",
      "Loss: 0.7633263539388286\n",
      "Relative change: 0.041852%\n",
      "Iteration: 1285\n",
      "Loss: 0.7630073847244966\n",
      "Relative change: 0.041787%\n",
      "Iteration: 1286\n",
      "Loss: 0.7626890472492793\n",
      "Relative change: 0.041721%\n",
      "Iteration: 1287\n",
      "Loss: 0.7623713393651962\n",
      "Relative change: 0.041656%\n",
      "Iteration: 1288\n",
      "Loss: 0.7620542589349566\n",
      "Relative change: 0.041591%\n",
      "Iteration: 1289\n",
      "Loss: 0.7617378038318536\n",
      "Relative change: 0.041527%\n",
      "Iteration: 1290\n",
      "Loss: 0.7614219719396513\n",
      "Relative change: 0.041462%\n",
      "Iteration: 1291\n",
      "Loss: 0.7611067611524729\n",
      "Relative change: 0.041398%\n",
      "Iteration: 1292\n",
      "Loss: 0.7607921693746897\n",
      "Relative change: 0.041333%\n",
      "Iteration: 1293\n",
      "Loss: 0.7604781945208072\n",
      "Relative change: 0.041269%\n",
      "Iteration: 1294\n",
      "Loss: 0.7601648345153532\n",
      "Relative change: 0.041206%\n",
      "Iteration: 1295\n",
      "Loss: 0.7598520872927657\n",
      "Relative change: 0.041142%\n",
      "Iteration: 1296\n",
      "Loss: 0.7595399507972794\n",
      "Relative change: 0.041079%\n",
      "Iteration: 1297\n",
      "Loss: 0.7592284229828156\n",
      "Relative change: 0.041015%\n",
      "Iteration: 1298\n",
      "Loss: 0.7589175018128692\n",
      "Relative change: 0.040952%\n",
      "Iteration: 1299\n",
      "Loss: 0.7586071852604003\n",
      "Relative change: 0.040889%\n",
      "Iteration: 1300\n",
      "Loss: 0.7582974713077234\n",
      "Relative change: 0.040827%\n",
      "Iteration: 1301\n",
      "Loss: 0.7579883579464006\n",
      "Relative change: 0.040764%\n",
      "Iteration: 1302\n",
      "Loss: 0.7576798431771332\n",
      "Relative change: 0.040702%\n",
      "Iteration: 1303\n",
      "Loss: 0.7573719250096574\n",
      "Relative change: 0.040640%\n",
      "Iteration: 1304\n",
      "Loss: 0.7570646014626401\n",
      "Relative change: 0.040578%\n",
      "Iteration: 1305\n",
      "Loss: 0.7567578705635761\n",
      "Relative change: 0.040516%\n",
      "Iteration: 1306\n",
      "Loss: 0.7564517303486887\n",
      "Relative change: 0.040454%\n",
      "Iteration: 1307\n",
      "Loss: 0.7561461788628293\n",
      "Relative change: 0.040393%\n",
      "Iteration: 1308\n",
      "Loss: 0.7558412141593825\n",
      "Relative change: 0.040331%\n",
      "Iteration: 1309\n",
      "Loss: 0.7555368343001707\n",
      "Relative change: 0.040270%\n",
      "Iteration: 1310\n",
      "Loss: 0.7552330373553616\n",
      "Relative change: 0.040209%\n",
      "Iteration: 1311\n",
      "Loss: 0.7549298214033779\n",
      "Relative change: 0.040149%\n",
      "Iteration: 1312\n",
      "Loss: 0.7546271845308089\n",
      "Relative change: 0.040088%\n",
      "Iteration: 1313\n",
      "Loss: 0.7543251248323254\n",
      "Relative change: 0.040028%\n",
      "Iteration: 1314\n",
      "Loss: 0.7540236404105961\n",
      "Relative change: 0.039967%\n",
      "Iteration: 1315\n",
      "Loss: 0.7537227293762062\n",
      "Relative change: 0.039907%\n",
      "Iteration: 1316\n",
      "Loss: 0.7534223898475789\n",
      "Relative change: 0.039847%\n",
      "Iteration: 1317\n",
      "Loss: 0.7531226199508997\n",
      "Relative change: 0.039788%\n",
      "Iteration: 1318\n",
      "Loss: 0.752823417820042\n",
      "Relative change: 0.039728%\n",
      "Iteration: 1319\n",
      "Loss: 0.7525247815964959\n",
      "Relative change: 0.039669%\n",
      "Iteration: 1320\n",
      "Loss: 0.7522267094293007\n",
      "Relative change: 0.039610%\n",
      "Iteration: 1321\n",
      "Loss: 0.7519291994749758\n",
      "Relative change: 0.039551%\n",
      "Iteration: 1322\n",
      "Loss: 0.7516322498974604\n",
      "Relative change: 0.039492%\n",
      "Iteration: 1323\n",
      "Loss: 0.7513358588680485\n",
      "Relative change: 0.039433%\n",
      "Iteration: 1324\n",
      "Loss: 0.7510400245653325\n",
      "Relative change: 0.039374%\n",
      "Iteration: 1325\n",
      "Loss: 0.7507447451751451\n",
      "Relative change: 0.039316%\n",
      "Iteration: 1326\n",
      "Loss: 0.7504500188905058\n",
      "Relative change: 0.039258%\n",
      "Iteration: 1327\n",
      "Loss: 0.7501558439115679\n",
      "Relative change: 0.039200%\n",
      "Iteration: 1328\n",
      "Loss: 0.74986221844557\n",
      "Relative change: 0.039142%\n",
      "Iteration: 1329\n",
      "Loss: 0.7495691407067878\n",
      "Relative change: 0.039084%\n",
      "Iteration: 1330\n",
      "Loss: 0.7492766089164887\n",
      "Relative change: 0.039027%\n",
      "Iteration: 1331\n",
      "Loss: 0.7489846213028888\n",
      "Relative change: 0.038969%\n",
      "Iteration: 1332\n",
      "Loss: 0.748693176101112\n",
      "Relative change: 0.038912%\n",
      "Iteration: 1333\n",
      "Loss: 0.7484022715531506\n",
      "Relative change: 0.038855%\n",
      "Iteration: 1334\n",
      "Loss: 0.7481119059078282\n",
      "Relative change: 0.038798%\n",
      "Iteration: 1335\n",
      "Loss: 0.7478220774207641\n",
      "Relative change: 0.038741%\n",
      "Iteration: 1336\n",
      "Loss: 0.747532784354341\n",
      "Relative change: 0.038685%\n",
      "Iteration: 1337\n",
      "Loss: 0.7472440249776714\n",
      "Relative change: 0.038628%\n",
      "Iteration: 1338\n",
      "Loss: 0.7469557975665689\n",
      "Relative change: 0.038572%\n",
      "Iteration: 1339\n",
      "Loss: 0.7466681004035198\n",
      "Relative change: 0.038516%\n",
      "Iteration: 1340\n",
      "Loss: 0.7463809317776553\n",
      "Relative change: 0.038460%\n",
      "Iteration: 1341\n",
      "Loss: 0.7460942899847259\n",
      "Relative change: 0.038404%\n",
      "Iteration: 1342\n",
      "Loss: 0.7458081733270787\n",
      "Relative change: 0.038349%\n",
      "Iteration: 1343\n",
      "Loss: 0.7455225801136333\n",
      "Relative change: 0.038293%\n",
      "Iteration: 1344\n",
      "Loss: 0.74523750865986\n",
      "Relative change: 0.038238%\n",
      "Iteration: 1345\n",
      "Loss: 0.7449529572877599\n",
      "Relative change: 0.038183%\n",
      "Iteration: 1346\n",
      "Loss: 0.7446689243258459\n",
      "Relative change: 0.038128%\n",
      "Iteration: 1347\n",
      "Loss: 0.7443854081091226\n",
      "Relative change: 0.038073%\n",
      "Iteration: 1348\n",
      "Loss: 0.7441024069790707\n",
      "Relative change: 0.038018%\n",
      "Iteration: 1349\n",
      "Loss: 0.7438199192836286\n",
      "Relative change: 0.037964%\n",
      "Iteration: 1350\n",
      "Loss: 0.7435379433771772\n",
      "Relative change: 0.037909%\n",
      "Iteration: 1351\n",
      "Loss: 0.7432564776205242\n",
      "Relative change: 0.037855%\n",
      "Iteration: 1352\n",
      "Loss: 0.74297552038089\n",
      "Relative change: 0.037801%\n",
      "Iteration: 1353\n",
      "Loss: 0.7426950700318926\n",
      "Relative change: 0.037747%\n",
      "Iteration: 1354\n",
      "Loss: 0.7424151249535343\n",
      "Relative change: 0.037693%\n",
      "Iteration: 1355\n",
      "Loss: 0.7421356835321884\n",
      "Relative change: 0.037640%\n",
      "Iteration: 1356\n",
      "Loss: 0.7418567441605859\n",
      "Relative change: 0.037586%\n",
      "Iteration: 1357\n",
      "Loss: 0.7415783052378022\n",
      "Relative change: 0.037533%\n",
      "Iteration: 1358\n",
      "Loss: 0.7413003651692457\n",
      "Relative change: 0.037480%\n",
      "Iteration: 1359\n",
      "Loss: 0.7410229223666437\n",
      "Relative change: 0.037427%\n",
      "Iteration: 1360\n",
      "Loss: 0.7407459752480318\n",
      "Relative change: 0.037374%\n",
      "Iteration: 1361\n",
      "Loss: 0.740469522237739\n",
      "Relative change: 0.037321%\n",
      "Iteration: 1362\n",
      "Loss: 0.7401935617663771\n",
      "Relative change: 0.037268%\n",
      "Iteration: 1363\n",
      "Loss: 0.7399180922708276\n",
      "Relative change: 0.037216%\n",
      "Iteration: 1364\n",
      "Loss: 0.7396431121942286\n",
      "Relative change: 0.037164%\n",
      "Iteration: 1365\n",
      "Loss: 0.7393686199859618\n",
      "Relative change: 0.037111%\n",
      "Iteration: 1366\n",
      "Loss: 0.7390946141016389\n",
      "Relative change: 0.037059%\n",
      "Iteration: 1367\n",
      "Loss: 0.738821093003089\n",
      "Relative change: 0.037008%\n",
      "Iteration: 1368\n",
      "Loss: 0.7385480551583438\n",
      "Relative change: 0.036956%\n",
      "Iteration: 1369\n",
      "Loss: 0.7382754990416238\n",
      "Relative change: 0.036904%\n",
      "Iteration: 1370\n",
      "Loss: 0.7380034231333233\n",
      "Relative change: 0.036853%\n",
      "Iteration: 1371\n",
      "Loss: 0.7377318259199952\n",
      "Relative change: 0.036802%\n",
      "Iteration: 1372\n",
      "Loss: 0.7374607058943354\n",
      "Relative change: 0.036750%\n",
      "Iteration: 1373\n",
      "Loss: 0.737190061555167\n",
      "Relative change: 0.036699%\n",
      "Iteration: 1374\n",
      "Loss: 0.736919891407423\n",
      "Relative change: 0.036649%\n",
      "Iteration: 1375\n",
      "Loss: 0.7366501939621298\n",
      "Relative change: 0.036598%\n",
      "Iteration: 1376\n",
      "Loss: 0.7363809677363887\n",
      "Relative change: 0.036547%\n",
      "Iteration: 1377\n",
      "Loss: 0.736112211253358\n",
      "Relative change: 0.036497%\n",
      "Iteration: 1378\n",
      "Loss: 0.7358439230422333\n",
      "Relative change: 0.036447%\n",
      "Iteration: 1379\n",
      "Loss: 0.7355761016382295\n",
      "Relative change: 0.036396%\n",
      "Iteration: 1380\n",
      "Loss: 0.7353087455825579\n",
      "Relative change: 0.036346%\n",
      "Iteration: 1381\n",
      "Loss: 0.735041853422408\n",
      "Relative change: 0.036297%\n",
      "Iteration: 1382\n",
      "Loss: 0.7347754237109244\n",
      "Relative change: 0.036247%\n",
      "Iteration: 1383\n",
      "Loss: 0.7345094550071851\n",
      "Relative change: 0.036197%\n",
      "Iteration: 1384\n",
      "Loss: 0.7342439458761789\n",
      "Relative change: 0.036148%\n",
      "Iteration: 1385\n",
      "Loss: 0.733978894888781\n",
      "Relative change: 0.036098%\n",
      "Iteration: 1386\n",
      "Loss: 0.7337143006217303\n",
      "Relative change: 0.036049%\n",
      "Iteration: 1387\n",
      "Loss: 0.7334501616576031\n",
      "Relative change: 0.036000%\n",
      "Iteration: 1388\n",
      "Loss: 0.7331864765847899\n",
      "Relative change: 0.035951%\n",
      "Iteration: 1389\n",
      "Loss: 0.7329232439974668\n",
      "Relative change: 0.035903%\n",
      "Iteration: 1390\n",
      "Loss: 0.7326604624955707\n",
      "Relative change: 0.035854%\n",
      "Iteration: 1391\n",
      "Loss: 0.7323981306847723\n",
      "Relative change: 0.035805%\n",
      "Iteration: 1392\n",
      "Loss: 0.7321362471764472\n",
      "Relative change: 0.035757%\n",
      "Iteration: 1393\n",
      "Loss: 0.7318748105876474\n",
      "Relative change: 0.035709%\n",
      "Iteration: 1394\n",
      "Loss: 0.7316138195410745\n",
      "Relative change: 0.035661%\n",
      "Iteration: 1395\n",
      "Loss: 0.7313532726650478\n",
      "Relative change: 0.035613%\n",
      "Iteration: 1396\n",
      "Loss: 0.7310931685934763\n",
      "Relative change: 0.035565%\n",
      "Iteration: 1397\n",
      "Loss: 0.7308335059658267\n",
      "Relative change: 0.035517%\n",
      "Iteration: 1398\n",
      "Loss: 0.7305742834270939\n",
      "Relative change: 0.035469%\n",
      "Iteration: 1399\n",
      "Loss: 0.7303154996277693\n",
      "Relative change: 0.035422%\n",
      "Iteration: 1400\n",
      "Loss: 0.7300571532238085\n",
      "Relative change: 0.035375%\n",
      "Iteration: 1401\n",
      "Loss: 0.7297992428765991\n",
      "Relative change: 0.035327%\n",
      "Iteration: 1402\n",
      "Loss: 0.7295417672529293\n",
      "Relative change: 0.035280%\n",
      "Iteration: 1403\n",
      "Loss: 0.7292847250249541\n",
      "Relative change: 0.035233%\n",
      "Iteration: 1404\n",
      "Loss: 0.7290281148701623\n",
      "Relative change: 0.035187%\n",
      "Iteration: 1405\n",
      "Loss: 0.7287719354713427\n",
      "Relative change: 0.035140%\n",
      "Iteration: 1406\n",
      "Loss: 0.7285161855165504\n",
      "Relative change: 0.035093%\n",
      "Iteration: 1407\n",
      "Loss: 0.7282608636990735\n",
      "Relative change: 0.035047%\n",
      "Iteration: 1408\n",
      "Loss: 0.7280059687173978\n",
      "Relative change: 0.035001%\n",
      "Iteration: 1409\n",
      "Loss: 0.7277514992751729\n",
      "Relative change: 0.034954%\n",
      "Iteration: 1410\n",
      "Loss: 0.727497454081177\n",
      "Relative change: 0.034908%\n",
      "Iteration: 1411\n",
      "Loss: 0.7272438318492828\n",
      "Relative change: 0.034862%\n",
      "Iteration: 1412\n",
      "Loss: 0.7269906312984219\n",
      "Relative change: 0.034816%\n",
      "Iteration: 1413\n",
      "Loss: 0.7267378511525504\n",
      "Relative change: 0.034771%\n",
      "Iteration: 1414\n",
      "Loss: 0.7264854901406126\n",
      "Relative change: 0.034725%\n",
      "Iteration: 1415\n",
      "Loss: 0.7262335469965072\n",
      "Relative change: 0.034680%\n",
      "Iteration: 1416\n",
      "Loss: 0.7259820204590515\n",
      "Relative change: 0.034634%\n",
      "Iteration: 1417\n",
      "Loss: 0.7257309092719456\n",
      "Relative change: 0.034589%\n",
      "Iteration: 1418\n",
      "Loss: 0.7254802121837378\n",
      "Relative change: 0.034544%\n",
      "Iteration: 1419\n",
      "Loss: 0.7252299279477898\n",
      "Relative change: 0.034499%\n",
      "Iteration: 1420\n",
      "Loss: 0.7249800553222404\n",
      "Relative change: 0.034454%\n",
      "Iteration: 1421\n",
      "Loss: 0.7247305930699711\n",
      "Relative change: 0.034410%\n",
      "Iteration: 1422\n",
      "Loss: 0.7244815399585713\n",
      "Relative change: 0.034365%\n",
      "Iteration: 1423\n",
      "Loss: 0.7242328947603034\n",
      "Relative change: 0.034320%\n",
      "Iteration: 1424\n",
      "Loss: 0.7239846562520672\n",
      "Relative change: 0.034276%\n",
      "Iteration: 1425\n",
      "Loss: 0.7237368232153669\n",
      "Relative change: 0.034232%\n",
      "Iteration: 1426\n",
      "Loss: 0.7234893944362747\n",
      "Relative change: 0.034188%\n",
      "Iteration: 1427\n",
      "Loss: 0.7232423687053986\n",
      "Relative change: 0.034144%\n",
      "Iteration: 1428\n",
      "Loss: 0.7229957448178466\n",
      "Relative change: 0.034100%\n",
      "Iteration: 1429\n",
      "Loss: 0.7227495215731944\n",
      "Relative change: 0.034056%\n",
      "Iteration: 1430\n",
      "Loss: 0.7225036977754501\n",
      "Relative change: 0.034012%\n",
      "Iteration: 1431\n",
      "Loss: 0.7222582722330226\n",
      "Relative change: 0.033969%\n",
      "Iteration: 1432\n",
      "Loss: 0.7220132437586868\n",
      "Relative change: 0.033925%\n",
      "Iteration: 1433\n",
      "Loss: 0.7217686111695524\n",
      "Relative change: 0.033882%\n",
      "Iteration: 1434\n",
      "Loss: 0.7215243732870292\n",
      "Relative change: 0.033839%\n",
      "Iteration: 1435\n",
      "Loss: 0.721280528936797\n",
      "Relative change: 0.033796%\n",
      "Iteration: 1436\n",
      "Loss: 0.7210370769487718\n",
      "Relative change: 0.033753%\n",
      "Iteration: 1437\n",
      "Loss: 0.7207940161570756\n",
      "Relative change: 0.033710%\n",
      "Iteration: 1438\n",
      "Loss: 0.7205513454000031\n",
      "Relative change: 0.033667%\n",
      "Iteration: 1439\n",
      "Loss: 0.720309063519992\n",
      "Relative change: 0.033625%\n",
      "Iteration: 1440\n",
      "Loss: 0.720067169363592\n",
      "Relative change: 0.033582%\n",
      "Iteration: 1441\n",
      "Loss: 0.7198256617814331\n",
      "Relative change: 0.033540%\n",
      "Iteration: 1442\n",
      "Loss: 0.7195845396281972\n",
      "Relative change: 0.033497%\n",
      "Iteration: 1443\n",
      "Loss: 0.7193438017625869\n",
      "Relative change: 0.033455%\n",
      "Iteration: 1444\n",
      "Loss: 0.7191034470472965\n",
      "Relative change: 0.033413%\n",
      "Iteration: 1445\n",
      "Loss: 0.7188634743489823\n",
      "Relative change: 0.033371%\n",
      "Iteration: 1446\n",
      "Loss: 0.718623882538235\n",
      "Relative change: 0.033329%\n",
      "Iteration: 1447\n",
      "Loss: 0.7183846704895495\n",
      "Relative change: 0.033288%\n",
      "Iteration: 1448\n",
      "Loss: 0.7181458370812975\n",
      "Relative change: 0.033246%\n",
      "Iteration: 1449\n",
      "Loss: 0.7179073811957006\n",
      "Relative change: 0.033204%\n",
      "Iteration: 1450\n",
      "Loss: 0.7176693017188011\n",
      "Relative change: 0.033163%\n",
      "Iteration: 1451\n",
      "Loss: 0.717431597540436\n",
      "Relative change: 0.033122%\n",
      "Iteration: 1452\n",
      "Loss: 0.7171942675542093\n",
      "Relative change: 0.033081%\n",
      "Iteration: 1453\n",
      "Loss: 0.7169573106574673\n",
      "Relative change: 0.033039%\n",
      "Iteration: 1454\n",
      "Loss: 0.71672072575127\n",
      "Relative change: 0.032998%\n",
      "Iteration: 1455\n",
      "Loss: 0.7164845117403676\n",
      "Relative change: 0.032958%\n",
      "Iteration: 1456\n",
      "Loss: 0.7162486675331736\n",
      "Relative change: 0.032917%\n",
      "Iteration: 1457\n",
      "Loss: 0.7160131920417403\n",
      "Relative change: 0.032876%\n",
      "Iteration: 1458\n",
      "Loss: 0.7157780841817343\n",
      "Relative change: 0.032836%\n",
      "Iteration: 1459\n",
      "Loss: 0.7155433428724112\n",
      "Relative change: 0.032795%\n",
      "Iteration: 1460\n",
      "Loss: 0.7153089670365931\n",
      "Relative change: 0.032755%\n",
      "Iteration: 1461\n",
      "Loss: 0.7150749556006432\n",
      "Relative change: 0.032715%\n",
      "Iteration: 1462\n",
      "Loss: 0.714841307494444\n",
      "Relative change: 0.032675%\n",
      "Iteration: 1463\n",
      "Loss: 0.7146080216513724\n",
      "Relative change: 0.032635%\n",
      "Iteration: 1464\n",
      "Loss: 0.714375097008279\n",
      "Relative change: 0.032595%\n",
      "Iteration: 1465\n",
      "Loss: 0.7141425325054643\n",
      "Relative change: 0.032555%\n",
      "Iteration: 1466\n",
      "Loss: 0.713910327086658\n",
      "Relative change: 0.032515%\n",
      "Iteration: 1467\n",
      "Loss: 0.7136784796989958\n",
      "Relative change: 0.032476%\n",
      "Iteration: 1468\n",
      "Loss: 0.7134469892929994\n",
      "Relative change: 0.032436%\n",
      "Iteration: 1469\n",
      "Loss: 0.7132158548225542\n",
      "Relative change: 0.032397%\n",
      "Iteration: 1470\n",
      "Loss: 0.7129850752448896\n",
      "Relative change: 0.032358%\n",
      "Iteration: 1471\n",
      "Loss: 0.7127546495205589\n",
      "Relative change: 0.032318%\n",
      "Iteration: 1472\n",
      "Loss: 0.712524576613418\n",
      "Relative change: 0.032279%\n",
      "Iteration: 1473\n",
      "Loss: 0.712294855490607\n",
      "Relative change: 0.032240%\n",
      "Iteration: 1474\n",
      "Loss: 0.7120654851225297\n",
      "Relative change: 0.032202%\n",
      "Iteration: 1475\n",
      "Loss: 0.711836464482836\n",
      "Relative change: 0.032163%\n",
      "Iteration: 1476\n",
      "Loss: 0.7116077925484011\n",
      "Relative change: 0.032124%\n",
      "Iteration: 1477\n",
      "Loss: 0.7113794682993098\n",
      "Relative change: 0.032086%\n",
      "Iteration: 1478\n",
      "Loss: 0.7111514907188355\n",
      "Relative change: 0.032047%\n",
      "Iteration: 1479\n",
      "Loss: 0.7109238587934256\n",
      "Relative change: 0.032009%\n",
      "Iteration: 1480\n",
      "Loss: 0.7106965715126814\n",
      "Relative change: 0.031971%\n",
      "Iteration: 1481\n",
      "Loss: 0.710469627869342\n",
      "Relative change: 0.031933%\n",
      "Iteration: 1482\n",
      "Loss: 0.7102430268592685\n",
      "Relative change: 0.031895%\n",
      "Iteration: 1483\n",
      "Loss: 0.710016767481426\n",
      "Relative change: 0.031857%\n",
      "Iteration: 1484\n",
      "Loss: 0.7097908487378688\n",
      "Relative change: 0.031819%\n",
      "Iteration: 1485\n",
      "Loss: 0.7095652696337232\n",
      "Relative change: 0.031781%\n",
      "Iteration: 1486\n",
      "Loss: 0.709340029177174\n",
      "Relative change: 0.031743%\n",
      "Iteration: 1487\n",
      "Loss: 0.7091151263794478\n",
      "Relative change: 0.031706%\n",
      "Iteration: 1488\n",
      "Loss: 0.708890560254799\n",
      "Relative change: 0.031669%\n",
      "Iteration: 1489\n",
      "Loss: 0.7086663298204944\n",
      "Relative change: 0.031631%\n",
      "Iteration: 1490\n",
      "Loss: 0.7084424340968007\n",
      "Relative change: 0.031594%\n",
      "Iteration: 1491\n",
      "Loss: 0.7082188721069689\n",
      "Relative change: 0.031557%\n",
      "Iteration: 1492\n",
      "Loss: 0.7079956428772222\n",
      "Relative change: 0.031520%\n",
      "Iteration: 1493\n",
      "Loss: 0.7077727454367423\n",
      "Relative change: 0.031483%\n",
      "Iteration: 1494\n",
      "Loss: 0.707550178817656\n",
      "Relative change: 0.031446%\n",
      "Iteration: 1495\n",
      "Loss: 0.7073279420550244\n",
      "Relative change: 0.031409%\n",
      "Iteration: 1496\n",
      "Loss: 0.707106034186828\n",
      "Relative change: 0.031373%\n",
      "Iteration: 1497\n",
      "Loss: 0.706884454253958\n",
      "Relative change: 0.031336%\n",
      "Iteration: 1498\n",
      "Loss: 0.7066632013002027\n",
      "Relative change: 0.031300%\n",
      "Iteration: 1499\n",
      "Loss: 0.7064422743722361\n",
      "Relative change: 0.031263%\n",
      "Iteration: 1500\n",
      "Loss: 0.7062216725196093\n",
      "Relative change: 0.031227%\n",
      "Iteration: 1501\n",
      "Loss: 0.7060013947947369\n",
      "Relative change: 0.031191%\n",
      "Iteration: 1502\n",
      "Loss: 0.705781440252889\n",
      "Relative change: 0.031155%\n",
      "Iteration: 1503\n",
      "Loss: 0.7055618079521809\n",
      "Relative change: 0.031119%\n",
      "Iteration: 1504\n",
      "Loss: 0.7053424969535627\n",
      "Relative change: 0.031083%\n",
      "Iteration: 1505\n",
      "Loss: 0.7051235063208102\n",
      "Relative change: 0.031047%\n",
      "Iteration: 1506\n",
      "Loss: 0.7049048351205172\n",
      "Relative change: 0.031012%\n",
      "Iteration: 1507\n",
      "Loss: 0.704686482422085\n",
      "Relative change: 0.030976%\n",
      "Iteration: 1508\n",
      "Loss: 0.7044684472977156\n",
      "Relative change: 0.030941%\n",
      "Iteration: 1509\n",
      "Loss: 0.7042507288224025\n",
      "Relative change: 0.030905%\n",
      "Iteration: 1510\n",
      "Loss: 0.7040333260739243\n",
      "Relative change: 0.030870%\n",
      "Iteration: 1511\n",
      "Loss: 0.7038162381328359\n",
      "Relative change: 0.030835%\n",
      "Iteration: 1512\n",
      "Loss: 0.7035994640824618\n",
      "Relative change: 0.030800%\n",
      "Iteration: 1513\n",
      "Loss: 0.7033830030088906\n",
      "Relative change: 0.030765%\n",
      "Iteration: 1514\n",
      "Loss: 0.7031668540009662\n",
      "Relative change: 0.030730%\n",
      "Iteration: 1515\n",
      "Loss: 0.7029510161502834\n",
      "Relative change: 0.030695%\n",
      "Iteration: 1516\n",
      "Loss: 0.7027354885511813\n",
      "Relative change: 0.030660%\n",
      "Iteration: 1517\n",
      "Loss: 0.7025202703007373\n",
      "Relative change: 0.030626%\n",
      "Iteration: 1518\n",
      "Loss: 0.7023053604987624\n",
      "Relative change: 0.030591%\n",
      "Iteration: 1519\n",
      "Loss: 0.7020907582477954\n",
      "Relative change: 0.030557%\n",
      "Iteration: 1520\n",
      "Loss: 0.7018764626530988\n",
      "Relative change: 0.030522%\n",
      "Iteration: 1521\n",
      "Loss: 0.7016624728226537\n",
      "Relative change: 0.030488%\n",
      "Iteration: 1522\n",
      "Loss: 0.7014487878671557\n",
      "Relative change: 0.030454%\n",
      "Iteration: 1523\n",
      "Loss: 0.7012354069000105\n",
      "Relative change: 0.030420%\n",
      "Iteration: 1524\n",
      "Loss: 0.701022329037331\n",
      "Relative change: 0.030386%\n",
      "Iteration: 1525\n",
      "Loss: 0.7008095533979323\n",
      "Relative change: 0.030352%\n",
      "Iteration: 1526\n",
      "Loss: 0.7005970791033299\n",
      "Relative change: 0.030318%\n",
      "Iteration: 1527\n",
      "Loss: 0.7003849052777351\n",
      "Relative change: 0.030285%\n",
      "Iteration: 1528\n",
      "Loss: 0.7001730310480538\n",
      "Relative change: 0.030251%\n",
      "Iteration: 1529\n",
      "Loss: 0.6999614555438822\n",
      "Relative change: 0.030218%\n",
      "Iteration: 1530\n",
      "Loss: 0.6997501778975056\n",
      "Relative change: 0.030184%\n",
      "Iteration: 1531\n",
      "Loss: 0.6995391972438953\n",
      "Relative change: 0.030151%\n",
      "Iteration: 1532\n",
      "Loss: 0.6993285127207072\n",
      "Relative change: 0.030118%\n",
      "Iteration: 1533\n",
      "Loss: 0.6991181234682796\n",
      "Relative change: 0.030084%\n",
      "Iteration: 1534\n",
      "Loss: 0.6989080286296315\n",
      "Relative change: 0.030051%\n",
      "Iteration: 1535\n",
      "Loss: 0.6986982273504618\n",
      "Relative change: 0.030018%\n",
      "Iteration: 1536\n",
      "Loss: 0.6984887187791466\n",
      "Relative change: 0.029986%\n",
      "Iteration: 1537\n",
      "Loss: 0.6982795020667398\n",
      "Relative change: 0.029953%\n",
      "Iteration: 1538\n",
      "Loss: 0.6980705763669703\n",
      "Relative change: 0.029920%\n",
      "Iteration: 1539\n",
      "Loss: 0.6978619408362431\n",
      "Relative change: 0.029887%\n",
      "Iteration: 1540\n",
      "Loss: 0.6976535946336369\n",
      "Relative change: 0.029855%\n",
      "Iteration: 1541\n",
      "Loss: 0.6974455369209045\n",
      "Relative change: 0.029822%\n",
      "Iteration: 1542\n",
      "Loss: 0.6972377668624721\n",
      "Relative change: 0.029790%\n",
      "Iteration: 1543\n",
      "Loss: 0.6970302836254388\n",
      "Relative change: 0.029758%\n",
      "Iteration: 1544\n",
      "Loss: 0.6968230863795772\n",
      "Relative change: 0.029726%\n",
      "Iteration: 1545\n",
      "Loss: 0.6966161742973322\n",
      "Relative change: 0.029694%\n",
      "Iteration: 1546\n",
      "Loss: 0.6964095465538218\n",
      "Relative change: 0.029662%\n",
      "Iteration: 1547\n",
      "Loss: 0.696203202326837\n",
      "Relative change: 0.029630%\n",
      "Iteration: 1548\n",
      "Loss: 0.6959971407968419\n",
      "Relative change: 0.029598%\n",
      "Iteration: 1549\n",
      "Loss: 0.6957913611469737\n",
      "Relative change: 0.029566%\n",
      "Iteration: 1550\n",
      "Loss: 0.6955858625630438\n",
      "Relative change: 0.029535%\n",
      "Iteration: 1551\n",
      "Loss: 0.6953806442335376\n",
      "Relative change: 0.029503%\n",
      "Iteration: 1552\n",
      "Loss: 0.6951757053496149\n",
      "Relative change: 0.029471%\n",
      "Iteration: 1553\n",
      "Loss: 0.694971045105111\n",
      "Relative change: 0.029440%\n",
      "Iteration: 1554\n",
      "Loss: 0.6947666626965363\n",
      "Relative change: 0.029409%\n",
      "Iteration: 1555\n",
      "Loss: 0.6945625573230778\n",
      "Relative change: 0.029378%\n",
      "Iteration: 1556\n",
      "Loss: 0.6943587281865994\n",
      "Relative change: 0.029346%\n",
      "Iteration: 1557\n",
      "Loss: 0.6941551744916423\n",
      "Relative change: 0.029315%\n",
      "Iteration: 1558\n",
      "Loss: 0.6939518954454256\n",
      "Relative change: 0.029284%\n",
      "Iteration: 1559\n",
      "Loss: 0.6937488902578473\n",
      "Relative change: 0.029253%\n",
      "Iteration: 1560\n",
      "Loss: 0.6935461581414847\n",
      "Relative change: 0.029223%\n",
      "Iteration: 1561\n",
      "Loss: 0.6933436983115955\n",
      "Relative change: 0.029192%\n",
      "Iteration: 1562\n",
      "Loss: 0.6931415099861177\n",
      "Relative change: 0.029161%\n",
      "Iteration: 1563\n",
      "Loss: 0.6929395923856704\n",
      "Relative change: 0.029131%\n",
      "Iteration: 1564\n",
      "Loss: 0.6927379447335552\n",
      "Relative change: 0.029100%\n",
      "Iteration: 1565\n",
      "Loss: 0.6925365662557559\n",
      "Relative change: 0.029070%\n",
      "Iteration: 1566\n",
      "Loss: 0.692335456180939\n",
      "Relative change: 0.029040%\n",
      "Iteration: 1567\n",
      "Loss: 0.6921346137404549\n",
      "Relative change: 0.029009%\n",
      "Iteration: 1568\n",
      "Loss: 0.6919340381683378\n",
      "Relative change: 0.028979%\n",
      "Iteration: 1569\n",
      "Loss: 0.6917337287013064\n",
      "Relative change: 0.028949%\n",
      "Iteration: 1570\n",
      "Loss: 0.691533684578764\n",
      "Relative change: 0.028919%\n",
      "Iteration: 1571\n",
      "Loss: 0.691333905042799\n",
      "Relative change: 0.028889%\n",
      "Iteration: 1572\n",
      "Loss: 0.691134389338185\n",
      "Relative change: 0.028860%\n",
      "Iteration: 1573\n",
      "Loss: 0.6909351367123817\n",
      "Relative change: 0.028830%\n",
      "Iteration: 1574\n",
      "Loss: 0.6907361464155333\n",
      "Relative change: 0.028800%\n",
      "Iteration: 1575\n",
      "Loss: 0.6905374177004702\n",
      "Relative change: 0.028771%\n",
      "Iteration: 1576\n",
      "Loss: 0.6903389498227083\n",
      "Relative change: 0.028741%\n",
      "Iteration: 1577\n",
      "Loss: 0.6901407420404496\n",
      "Relative change: 0.028712%\n",
      "Iteration: 1578\n",
      "Loss: 0.6899427936145802\n",
      "Relative change: 0.028682%\n",
      "Iteration: 1579\n",
      "Loss: 0.6897451038086718\n",
      "Relative change: 0.028653%\n",
      "Iteration: 1580\n",
      "Loss: 0.6895476718889804\n",
      "Relative change: 0.028624%\n",
      "Iteration: 1581\n",
      "Loss: 0.6893504971244463\n",
      "Relative change: 0.028595%\n",
      "Iteration: 1582\n",
      "Loss: 0.6891535787866935\n",
      "Relative change: 0.028566%\n",
      "Iteration: 1583\n",
      "Loss: 0.6889569161500281\n",
      "Relative change: 0.028537%\n",
      "Iteration: 1584\n",
      "Loss: 0.688760508491439\n",
      "Relative change: 0.028508%\n",
      "Iteration: 1585\n",
      "Loss: 0.6885643550905962\n",
      "Relative change: 0.028479%\n",
      "Iteration: 1586\n",
      "Loss: 0.688368455229849\n",
      "Relative change: 0.028450%\n",
      "Iteration: 1587\n",
      "Loss: 0.6881728081942274\n",
      "Relative change: 0.028422%\n",
      "Iteration: 1588\n",
      "Loss: 0.687977413271438\n",
      "Relative change: 0.028393%\n",
      "Iteration: 1589\n",
      "Loss: 0.6877822697518643\n",
      "Relative change: 0.028365%\n",
      "Iteration: 1590\n",
      "Loss: 0.6875873769285656\n",
      "Relative change: 0.028336%\n",
      "Iteration: 1591\n",
      "Loss: 0.6873927340972742\n",
      "Relative change: 0.028308%\n",
      "Iteration: 1592\n",
      "Loss: 0.6871983405563947\n",
      "Relative change: 0.028280%\n",
      "Iteration: 1593\n",
      "Loss: 0.6870041956070021\n",
      "Relative change: 0.028252%\n",
      "Iteration: 1594\n",
      "Loss: 0.6868102985528393\n",
      "Relative change: 0.028224%\n",
      "Iteration: 1595\n",
      "Loss: 0.6866166487003161\n",
      "Relative change: 0.028196%\n",
      "Iteration: 1596\n",
      "Loss: 0.6864232453585062\n",
      "Relative change: 0.028168%\n",
      "Iteration: 1597\n",
      "Loss: 0.6862300878391459\n",
      "Relative change: 0.028140%\n",
      "Iteration: 1598\n",
      "Loss: 0.68603717545663\n",
      "Relative change: 0.028112%\n",
      "Iteration: 1599\n",
      "Loss: 0.6858445075280124\n",
      "Relative change: 0.028084%\n",
      "Iteration: 1600\n",
      "Loss: 0.6856520833730003\n",
      "Relative change: 0.028057%\n",
      "Iteration: 1601\n",
      "Loss: 0.6854599023139532\n",
      "Relative change: 0.028029%\n",
      "Iteration: 1602\n",
      "Loss: 0.6852679636758794\n",
      "Relative change: 0.028001%\n",
      "Iteration: 1603\n",
      "Loss: 0.685076266786434\n",
      "Relative change: 0.027974%\n",
      "Iteration: 1604\n",
      "Loss: 0.684884810975915\n",
      "Relative change: 0.027947%\n",
      "Iteration: 1605\n",
      "Loss: 0.6846935955772604\n",
      "Relative change: 0.027919%\n",
      "Iteration: 1606\n",
      "Loss: 0.6845026199260447\n",
      "Relative change: 0.027892%\n",
      "Iteration: 1607\n",
      "Loss: 0.6843118833604758\n",
      "Relative change: 0.027865%\n",
      "Iteration: 1608\n",
      "Loss: 0.6841213852213911\n",
      "Relative change: 0.027838%\n",
      "Iteration: 1609\n",
      "Loss: 0.6839311248522549\n",
      "Relative change: 0.027811%\n",
      "Iteration: 1610\n",
      "Loss: 0.6837411015991529\n",
      "Relative change: 0.027784%\n",
      "Iteration: 1611\n",
      "Loss: 0.6835513148107898\n",
      "Relative change: 0.027757%\n",
      "Iteration: 1612\n",
      "Loss: 0.6833617638384845\n",
      "Relative change: 0.027730%\n",
      "Iteration: 1613\n",
      "Loss: 0.6831724480361667\n",
      "Relative change: 0.027704%\n",
      "Iteration: 1614\n",
      "Loss: 0.6829833667603715\n",
      "Relative change: 0.027677%\n",
      "Iteration: 1615\n",
      "Loss: 0.6827945193702372\n",
      "Relative change: 0.027650%\n",
      "Iteration: 1616\n",
      "Loss: 0.6826059052274985\n",
      "Relative change: 0.027624%\n",
      "Iteration: 1617\n",
      "Loss: 0.6824175236964832\n",
      "Relative change: 0.027597%\n",
      "Iteration: 1618\n",
      "Loss: 0.682229374144108\n",
      "Relative change: 0.027571%\n",
      "Iteration: 1619\n",
      "Loss: 0.682041455939872\n",
      "Relative change: 0.027545%\n",
      "Iteration: 1620\n",
      "Loss: 0.6818537684558545\n",
      "Relative change: 0.027518%\n",
      "Iteration: 1621\n",
      "Loss: 0.6816663110667075\n",
      "Relative change: 0.027492%\n",
      "Iteration: 1622\n",
      "Loss: 0.6814790831496516\n",
      "Relative change: 0.027466%\n",
      "Iteration: 1623\n",
      "Loss: 0.6812920840844721\n",
      "Relative change: 0.027440%\n",
      "Iteration: 1624\n",
      "Loss: 0.6811053132535112\n",
      "Relative change: 0.027414%\n",
      "Iteration: 1625\n",
      "Loss: 0.6809187700416653\n",
      "Relative change: 0.027388%\n",
      "Iteration: 1626\n",
      "Loss: 0.6807324538363779\n",
      "Relative change: 0.027362%\n",
      "Iteration: 1627\n",
      "Loss: 0.6805463640276344\n",
      "Relative change: 0.027337%\n",
      "Iteration: 1628\n",
      "Loss: 0.6803605000079563\n",
      "Relative change: 0.027311%\n",
      "Iteration: 1629\n",
      "Loss: 0.6801748611723966\n",
      "Relative change: 0.027285%\n",
      "Iteration: 1630\n",
      "Loss: 0.6799894469185325\n",
      "Relative change: 0.027260%\n",
      "Iteration: 1631\n",
      "Loss: 0.6798042566464607\n",
      "Relative change: 0.027234%\n",
      "Iteration: 1632\n",
      "Loss: 0.6796192897587903\n",
      "Relative change: 0.027209%\n",
      "Iteration: 1633\n",
      "Loss: 0.6794345456606379\n",
      "Relative change: 0.027183%\n",
      "Iteration: 1634\n",
      "Loss: 0.6792500237596208\n",
      "Relative change: 0.027158%\n",
      "Iteration: 1635\n",
      "Loss: 0.6790657234658505\n",
      "Relative change: 0.027133%\n",
      "Iteration: 1636\n",
      "Loss: 0.6788816441919273\n",
      "Relative change: 0.027108%\n",
      "Iteration: 1637\n",
      "Loss: 0.6786977853529332\n",
      "Relative change: 0.027083%\n",
      "Iteration: 1638\n",
      "Loss: 0.6785141463664257\n",
      "Relative change: 0.027058%\n",
      "Iteration: 1639\n",
      "Loss: 0.6783307266524311\n",
      "Relative change: 0.027033%\n",
      "Iteration: 1640\n",
      "Loss: 0.678147525633438\n",
      "Relative change: 0.027008%\n",
      "Iteration: 1641\n",
      "Loss: 0.6779645427343908\n",
      "Relative change: 0.026983%\n",
      "Iteration: 1642\n",
      "Loss: 0.6777817773826824\n",
      "Relative change: 0.026958%\n",
      "Iteration: 1643\n",
      "Loss: 0.6775992290081484\n",
      "Relative change: 0.026933%\n",
      "Iteration: 1644\n",
      "Loss: 0.6774168970430589\n",
      "Relative change: 0.026909%\n",
      "Iteration: 1645\n",
      "Loss: 0.6772347809221126\n",
      "Relative change: 0.026884%\n",
      "Iteration: 1646\n",
      "Loss: 0.6770528800824295\n",
      "Relative change: 0.026859%\n",
      "Iteration: 1647\n",
      "Loss: 0.6768711939635432\n",
      "Relative change: 0.026835%\n",
      "Iteration: 1648\n",
      "Loss: 0.6766897220073947\n",
      "Relative change: 0.026810%\n",
      "Iteration: 1649\n",
      "Loss: 0.6765084636583247\n",
      "Relative change: 0.026786%\n",
      "Iteration: 1650\n",
      "Loss: 0.6763274183630665\n",
      "Relative change: 0.026762%\n",
      "Iteration: 1651\n",
      "Loss: 0.6761465855707387\n",
      "Relative change: 0.026737%\n",
      "Iteration: 1652\n",
      "Loss: 0.6759659647328369\n",
      "Relative change: 0.026713%\n",
      "Iteration: 1653\n",
      "Loss: 0.6757855553032281\n",
      "Relative change: 0.026689%\n",
      "Iteration: 1654\n",
      "Loss: 0.6756053567381418\n",
      "Relative change: 0.026665%\n",
      "Iteration: 1655\n",
      "Loss: 0.6754253684961631\n",
      "Relative change: 0.026641%\n",
      "Iteration: 1656\n",
      "Loss: 0.6752455900382244\n",
      "Relative change: 0.026617%\n",
      "Iteration: 1657\n",
      "Loss: 0.6750660208275985\n",
      "Relative change: 0.026593%\n",
      "Iteration: 1658\n",
      "Loss: 0.674886660329891\n",
      "Relative change: 0.026569%\n",
      "Iteration: 1659\n",
      "Loss: 0.6747075080130317\n",
      "Relative change: 0.026546%\n",
      "Iteration: 1660\n",
      "Loss: 0.6745285633472679\n",
      "Relative change: 0.026522%\n",
      "Iteration: 1661\n",
      "Loss: 0.6743498258051553\n",
      "Relative change: 0.026498%\n",
      "Iteration: 1662\n",
      "Loss: 0.6741712948615518\n",
      "Relative change: 0.026475%\n",
      "Iteration: 1663\n",
      "Loss: 0.673992969993608\n",
      "Relative change: 0.026451%\n",
      "Iteration: 1664\n",
      "Loss: 0.6738148506807601\n",
      "Relative change: 0.026427%\n",
      "Iteration: 1665\n",
      "Loss: 0.6736369364047218\n",
      "Relative change: 0.026404%\n",
      "Iteration: 1666\n",
      "Loss: 0.6734592266494761\n",
      "Relative change: 0.026381%\n",
      "Iteration: 1667\n",
      "Loss: 0.6732817209012676\n",
      "Relative change: 0.026357%\n",
      "Iteration: 1668\n",
      "Loss: 0.6731044186485935\n",
      "Relative change: 0.026334%\n",
      "Iteration: 1669\n",
      "Loss: 0.6729273193821969\n",
      "Relative change: 0.026311%\n",
      "Iteration: 1670\n",
      "Loss: 0.6727504225950575\n",
      "Relative change: 0.026288%\n",
      "Iteration: 1671\n",
      "Loss: 0.6725737277823832\n",
      "Relative change: 0.026265%\n",
      "Iteration: 1672\n",
      "Loss: 0.6723972344416034\n",
      "Relative change: 0.026241%\n",
      "Iteration: 1673\n",
      "Loss: 0.6722209420723588\n",
      "Relative change: 0.026218%\n",
      "Iteration: 1674\n",
      "Loss: 0.6720448501764947\n",
      "Relative change: 0.026196%\n",
      "Iteration: 1675\n",
      "Loss: 0.6718689582580514\n",
      "Relative change: 0.026173%\n",
      "Iteration: 1676\n",
      "Loss: 0.6716932658232568\n",
      "Relative change: 0.026150%\n",
      "Iteration: 1677\n",
      "Loss: 0.6715177723805175\n",
      "Relative change: 0.026127%\n",
      "Iteration: 1678\n",
      "Loss: 0.6713424774404104\n",
      "Relative change: 0.026104%\n",
      "Iteration: 1679\n",
      "Loss: 0.6711673805156747\n",
      "Relative change: 0.026082%\n",
      "Iteration: 1680\n",
      "Loss: 0.6709924811212028\n",
      "Relative change: 0.026059%\n",
      "Iteration: 1681\n",
      "Loss: 0.6708177787740324\n",
      "Relative change: 0.026036%\n",
      "Iteration: 1682\n",
      "Loss: 0.6706432729933374\n",
      "Relative change: 0.026014%\n",
      "Iteration: 1683\n",
      "Loss: 0.67046896330042\n",
      "Relative change: 0.025991%\n",
      "Iteration: 1684\n",
      "Loss: 0.6702948492187016\n",
      "Relative change: 0.025969%\n",
      "Iteration: 1685\n",
      "Loss: 0.6701209302737146\n",
      "Relative change: 0.025947%\n",
      "Iteration: 1686\n",
      "Loss: 0.6699472059930935\n",
      "Relative change: 0.025924%\n",
      "Iteration: 1687\n",
      "Loss: 0.6697736759065668\n",
      "Relative change: 0.025902%\n",
      "Iteration: 1688\n",
      "Loss: 0.6696003395459473\n",
      "Relative change: 0.025880%\n",
      "Iteration: 1689\n",
      "Loss: 0.669427196445125\n",
      "Relative change: 0.025858%\n",
      "Iteration: 1690\n",
      "Loss: 0.669254246140057\n",
      "Relative change: 0.025836%\n",
      "Iteration: 1691\n",
      "Loss: 0.6690814881687598\n",
      "Relative change: 0.025814%\n",
      "Iteration: 1692\n",
      "Loss: 0.6689089220712999\n",
      "Relative change: 0.025791%\n",
      "Iteration: 1693\n",
      "Loss: 0.6687365473897857\n",
      "Relative change: 0.025770%\n",
      "Iteration: 1694\n",
      "Loss: 0.6685643636683585\n",
      "Relative change: 0.025748%\n",
      "Iteration: 1695\n",
      "Loss: 0.6683923704531832\n",
      "Relative change: 0.025726%\n",
      "Iteration: 1696\n",
      "Loss: 0.6682205672924408\n",
      "Relative change: 0.025704%\n",
      "Iteration: 1697\n",
      "Loss: 0.6680489537363187\n",
      "Relative change: 0.025682%\n",
      "Iteration: 1698\n",
      "Loss: 0.6678775293370023\n",
      "Relative change: 0.025660%\n",
      "Iteration: 1699\n",
      "Loss: 0.6677062936486656\n",
      "Relative change: 0.025639%\n",
      "Iteration: 1700\n",
      "Loss: 0.6675352462274637\n",
      "Relative change: 0.025617%\n",
      "Iteration: 1701\n",
      "Loss: 0.667364386631523\n",
      "Relative change: 0.025596%\n",
      "Iteration: 1702\n",
      "Loss: 0.6671937144209324\n",
      "Relative change: 0.025574%\n",
      "Iteration: 1703\n",
      "Loss: 0.6670232291577347\n",
      "Relative change: 0.025553%\n",
      "Iteration: 1704\n",
      "Loss: 0.6668529304059183\n",
      "Relative change: 0.025531%\n",
      "Iteration: 1705\n",
      "Loss: 0.6666828177314077\n",
      "Relative change: 0.025510%\n",
      "Iteration: 1706\n",
      "Loss: 0.6665128907020552\n",
      "Relative change: 0.025488%\n",
      "Iteration: 1707\n",
      "Loss: 0.6663431488876307\n",
      "Relative change: 0.025467%\n",
      "Iteration: 1708\n",
      "Loss: 0.6661735918598154\n",
      "Relative change: 0.025446%\n",
      "Iteration: 1709\n",
      "Loss: 0.6660042191921908\n",
      "Relative change: 0.025425%\n",
      "Iteration: 1710\n",
      "Loss: 0.665835030460231\n",
      "Relative change: 0.025404%\n",
      "Iteration: 1711\n",
      "Loss: 0.665666025241293\n",
      "Relative change: 0.025382%\n",
      "Iteration: 1712\n",
      "Loss: 0.6654972031146092\n",
      "Relative change: 0.025361%\n",
      "Iteration: 1713\n",
      "Loss: 0.6653285636612774\n",
      "Relative change: 0.025340%\n",
      "Iteration: 1714\n",
      "Loss: 0.6651601064642516\n",
      "Relative change: 0.025319%\n",
      "Iteration: 1715\n",
      "Loss: 0.6649918311083355\n",
      "Relative change: 0.025298%\n",
      "Iteration: 1716\n",
      "Loss: 0.6648237371801706\n",
      "Relative change: 0.025278%\n",
      "Iteration: 1717\n",
      "Loss: 0.6646558242682307\n",
      "Relative change: 0.025257%\n",
      "Iteration: 1718\n",
      "Loss: 0.6644880919628097\n",
      "Relative change: 0.025236%\n",
      "Iteration: 1719\n",
      "Loss: 0.664320539856015\n",
      "Relative change: 0.025215%\n",
      "Iteration: 1720\n",
      "Loss: 0.664153167541759\n",
      "Relative change: 0.025195%\n",
      "Iteration: 1721\n",
      "Loss: 0.6639859746157489\n",
      "Relative change: 0.025174%\n",
      "Iteration: 1722\n",
      "Loss: 0.6638189606754779\n",
      "Relative change: 0.025153%\n",
      "Iteration: 1723\n",
      "Loss: 0.6636521253202183\n",
      "Relative change: 0.025133%\n",
      "Iteration: 1724\n",
      "Loss: 0.663485468151011\n",
      "Relative change: 0.025112%\n",
      "Iteration: 1725\n",
      "Loss: 0.6633189887706571\n",
      "Relative change: 0.025092%\n",
      "Iteration: 1726\n",
      "Loss: 0.6631526867837101\n",
      "Relative change: 0.025071%\n",
      "Iteration: 1727\n",
      "Loss: 0.6629865617964654\n",
      "Relative change: 0.025051%\n",
      "Iteration: 1728\n",
      "Loss: 0.6628206134169539\n",
      "Relative change: 0.025030%\n",
      "Iteration: 1729\n",
      "Loss: 0.6626548412549312\n",
      "Relative change: 0.025010%\n",
      "Iteration: 1730\n",
      "Loss: 0.6624892449218702\n",
      "Relative change: 0.024990%\n",
      "Iteration: 1731\n",
      "Loss: 0.6623238240309522\n",
      "Relative change: 0.024970%\n",
      "Iteration: 1732\n",
      "Loss: 0.662158578197058\n",
      "Relative change: 0.024949%\n",
      "Iteration: 1733\n",
      "Loss: 0.6619935070367591\n",
      "Relative change: 0.024929%\n",
      "Iteration: 1734\n",
      "Loss: 0.6618286101683098\n",
      "Relative change: 0.024909%\n",
      "Iteration: 1735\n",
      "Loss: 0.6616638872116382\n",
      "Relative change: 0.024889%\n",
      "Iteration: 1736\n",
      "Loss: 0.6614993377883376\n",
      "Relative change: 0.024869%\n",
      "Iteration: 1737\n",
      "Loss: 0.6613349615216576\n",
      "Relative change: 0.024849%\n",
      "Iteration: 1738\n",
      "Loss: 0.6611707580364966\n",
      "Relative change: 0.024829%\n",
      "Iteration: 1739\n",
      "Loss: 0.661006726959392\n",
      "Relative change: 0.024809%\n",
      "Iteration: 1740\n",
      "Loss: 0.6608428679185127\n",
      "Relative change: 0.024789%\n",
      "Iteration: 1741\n",
      "Loss: 0.6606791805436503\n",
      "Relative change: 0.024769%\n",
      "Iteration: 1742\n",
      "Loss: 0.6605156644662102\n",
      "Relative change: 0.024750%\n",
      "Iteration: 1743\n",
      "Loss: 0.6603523193192042\n",
      "Relative change: 0.024730%\n",
      "Iteration: 1744\n",
      "Loss: 0.660189144737241\n",
      "Relative change: 0.024710%\n",
      "Iteration: 1745\n",
      "Loss: 0.6600261403565182\n",
      "Relative change: 0.024691%\n",
      "Iteration: 1746\n",
      "Loss: 0.6598633058148144\n",
      "Relative change: 0.024671%\n",
      "Iteration: 1747\n",
      "Loss: 0.6597006407514802\n",
      "Relative change: 0.024651%\n",
      "Iteration: 1748\n",
      "Loss: 0.6595381448074302\n",
      "Relative change: 0.024632%\n",
      "Iteration: 1749\n",
      "Loss: 0.6593758176251345\n",
      "Relative change: 0.024612%\n",
      "Iteration: 1750\n",
      "Loss: 0.6592136588486113\n",
      "Relative change: 0.024593%\n",
      "Iteration: 1751\n",
      "Loss: 0.6590516681234171\n",
      "Relative change: 0.024573%\n",
      "Iteration: 1752\n",
      "Loss: 0.6588898450966396\n",
      "Relative change: 0.024554%\n",
      "Iteration: 1753\n",
      "Loss: 0.6587281894168895\n",
      "Relative change: 0.024535%\n",
      "Iteration: 1754\n",
      "Loss: 0.658566700734292\n",
      "Relative change: 0.024515%\n",
      "Iteration: 1755\n",
      "Loss: 0.6584053787004787\n",
      "Relative change: 0.024496%\n",
      "Iteration: 1756\n",
      "Loss: 0.6582442229685795\n",
      "Relative change: 0.024477%\n",
      "Iteration: 1757\n",
      "Loss: 0.6580832331932148\n",
      "Relative change: 0.024457%\n",
      "Iteration: 1758\n",
      "Loss: 0.6579224090304868\n",
      "Relative change: 0.024438%\n",
      "Iteration: 1759\n",
      "Loss: 0.6577617501379723\n",
      "Relative change: 0.024419%\n",
      "Iteration: 1760\n",
      "Loss: 0.6576012561747144\n",
      "Relative change: 0.024400%\n",
      "Iteration: 1761\n",
      "Loss: 0.6574409268012137\n",
      "Relative change: 0.024381%\n",
      "Iteration: 1762\n",
      "Loss: 0.6572807616794223\n",
      "Relative change: 0.024362%\n",
      "Iteration: 1763\n",
      "Loss: 0.6571207604727337\n",
      "Relative change: 0.024343%\n",
      "Iteration: 1764\n",
      "Loss: 0.6569609228459762\n",
      "Relative change: 0.024324%\n",
      "Iteration: 1765\n",
      "Loss: 0.6568012484654049\n",
      "Relative change: 0.024305%\n",
      "Iteration: 1766\n",
      "Loss: 0.6566417369986939\n",
      "Relative change: 0.024286%\n",
      "Iteration: 1767\n",
      "Loss: 0.6564823881149282\n",
      "Relative change: 0.024267%\n",
      "Iteration: 1768\n",
      "Loss: 0.6563232014845956\n",
      "Relative change: 0.024248%\n",
      "Iteration: 1769\n",
      "Loss: 0.6561641767795806\n",
      "Relative change: 0.024230%\n",
      "Iteration: 1770\n",
      "Loss: 0.6560053136731547\n",
      "Relative change: 0.024211%\n",
      "Iteration: 1771\n",
      "Loss: 0.6558466118399701\n",
      "Relative change: 0.024192%\n",
      "Iteration: 1772\n",
      "Loss: 0.6556880709560512\n",
      "Relative change: 0.024173%\n",
      "Iteration: 1773\n",
      "Loss: 0.6555296906987877\n",
      "Relative change: 0.024155%\n",
      "Iteration: 1774\n",
      "Loss: 0.6553714707469266\n",
      "Relative change: 0.024136%\n",
      "Iteration: 1775\n",
      "Loss: 0.6552134107805649\n",
      "Relative change: 0.024118%\n",
      "Iteration: 1776\n",
      "Loss: 0.655055510481142\n",
      "Relative change: 0.024099%\n",
      "Iteration: 1777\n",
      "Loss: 0.6548977695314316\n",
      "Relative change: 0.024081%\n",
      "Iteration: 1778\n",
      "Loss: 0.6547401876155359\n",
      "Relative change: 0.024062%\n",
      "Iteration: 1779\n",
      "Loss: 0.6545827644188759\n",
      "Relative change: 0.024044%\n",
      "Iteration: 1780\n",
      "Loss: 0.654425499628186\n",
      "Relative change: 0.024025%\n",
      "Iteration: 1781\n",
      "Loss: 0.6542683929315057\n",
      "Relative change: 0.024007%\n",
      "Iteration: 1782\n",
      "Loss: 0.6541114440181728\n",
      "Relative change: 0.023988%\n",
      "Iteration: 1783\n",
      "Loss: 0.6539546525788149\n",
      "Relative change: 0.023970%\n",
      "Iteration: 1784\n",
      "Loss: 0.6537980183053436\n",
      "Relative change: 0.023952%\n",
      "Iteration: 1785\n",
      "Loss: 0.6536415408909464\n",
      "Relative change: 0.023934%\n",
      "Iteration: 1786\n",
      "Loss: 0.6534852200300799\n",
      "Relative change: 0.023915%\n",
      "Iteration: 1787\n",
      "Loss: 0.6533290554184623\n",
      "Relative change: 0.023897%\n",
      "Iteration: 1788\n",
      "Loss: 0.6531730467530664\n",
      "Relative change: 0.023879%\n",
      "Iteration: 1789\n",
      "Loss: 0.6530171937321131\n",
      "Relative change: 0.023861%\n",
      "Iteration: 1790\n",
      "Loss: 0.6528614960550626\n",
      "Relative change: 0.023843%\n",
      "Iteration: 1791\n",
      "Loss: 0.6527059534226091\n",
      "Relative change: 0.023825%\n",
      "Iteration: 1792\n",
      "Loss: 0.6525505655366733\n",
      "Relative change: 0.023807%\n",
      "Iteration: 1793\n",
      "Loss: 0.6523953321003947\n",
      "Relative change: 0.023789%\n",
      "Iteration: 1794\n",
      "Loss: 0.6522402528181257\n",
      "Relative change: 0.023771%\n",
      "Iteration: 1795\n",
      "Loss: 0.6520853273954237\n",
      "Relative change: 0.023753%\n",
      "Iteration: 1796\n",
      "Loss: 0.6519305555390447\n",
      "Relative change: 0.023735%\n",
      "Iteration: 1797\n",
      "Loss: 0.6517759369569364\n",
      "Relative change: 0.023717%\n",
      "Iteration: 1798\n",
      "Loss: 0.6516214713582311\n",
      "Relative change: 0.023699%\n",
      "Iteration: 1799\n",
      "Loss: 0.6514671584532391\n",
      "Relative change: 0.023681%\n",
      "Iteration: 1800\n",
      "Loss: 0.651312997953442\n",
      "Relative change: 0.023664%\n",
      "Iteration: 1801\n",
      "Loss: 0.6511589895714855\n",
      "Relative change: 0.023646%\n",
      "Iteration: 1802\n",
      "Loss: 0.651005133021173\n",
      "Relative change: 0.023628%\n",
      "Iteration: 1803\n",
      "Loss: 0.6508514280174588\n",
      "Relative change: 0.023610%\n",
      "Iteration: 1804\n",
      "Loss: 0.650697874276441\n",
      "Relative change: 0.023593%\n",
      "Iteration: 1805\n",
      "Loss: 0.6505444715153559\n",
      "Relative change: 0.023575%\n",
      "Iteration: 1806\n",
      "Loss: 0.6503912194525702\n",
      "Relative change: 0.023558%\n",
      "Iteration: 1807\n",
      "Loss: 0.6502381178075748\n",
      "Relative change: 0.023540%\n",
      "Iteration: 1808\n",
      "Loss: 0.6500851663009785\n",
      "Relative change: 0.023522%\n",
      "Iteration: 1809\n",
      "Loss: 0.649932364654501\n",
      "Relative change: 0.023505%\n",
      "Iteration: 1810\n",
      "Loss: 0.6497797125909667\n",
      "Relative change: 0.023487%\n",
      "Iteration: 1811\n",
      "Loss: 0.6496272098342977\n",
      "Relative change: 0.023470%\n",
      "Iteration: 1812\n",
      "Loss: 0.6494748561095078\n",
      "Relative change: 0.023452%\n",
      "Iteration: 1813\n",
      "Loss: 0.6493226511426962\n",
      "Relative change: 0.023435%\n",
      "Iteration: 1814\n",
      "Loss: 0.6491705946610401\n",
      "Relative change: 0.023418%\n",
      "Iteration: 1815\n",
      "Loss: 0.6490186863927891\n",
      "Relative change: 0.023400%\n",
      "Iteration: 1816\n",
      "Loss: 0.6488669260672587\n",
      "Relative change: 0.023383%\n",
      "Iteration: 1817\n",
      "Loss: 0.6487153134148236\n",
      "Relative change: 0.023366%\n",
      "Iteration: 1818\n",
      "Loss: 0.6485638481669115\n",
      "Relative change: 0.023348%\n",
      "Iteration: 1819\n",
      "Loss: 0.648412530055997\n",
      "Relative change: 0.023331%\n",
      "Iteration: 1820\n",
      "Loss: 0.6482613588155948\n",
      "Relative change: 0.023314%\n",
      "Iteration: 1821\n",
      "Loss: 0.6481103341802538\n",
      "Relative change: 0.023297%\n",
      "Iteration: 1822\n",
      "Loss: 0.6479594558855507\n",
      "Relative change: 0.023280%\n",
      "Iteration: 1823\n",
      "Loss: 0.6478087236680837\n",
      "Relative change: 0.023263%\n",
      "Iteration: 1824\n",
      "Loss: 0.6476581372654665\n",
      "Relative change: 0.023246%\n",
      "Iteration: 1825\n",
      "Loss: 0.6475076964163214\n",
      "Relative change: 0.023228%\n",
      "Iteration: 1826\n",
      "Loss: 0.6473574008602743\n",
      "Relative change: 0.023211%\n",
      "Iteration: 1827\n",
      "Loss: 0.6472072503379469\n",
      "Relative change: 0.023194%\n",
      "Iteration: 1828\n",
      "Loss: 0.6470572445909523\n",
      "Relative change: 0.023177%\n",
      "Iteration: 1829\n",
      "Loss: 0.6469073833618878\n",
      "Relative change: 0.023160%\n",
      "Iteration: 1830\n",
      "Loss: 0.6467576663943287\n",
      "Relative change: 0.023143%\n",
      "Iteration: 1831\n",
      "Loss: 0.6466080934328231\n",
      "Relative change: 0.023127%\n",
      "Iteration: 1832\n",
      "Loss: 0.6464586642228843\n",
      "Relative change: 0.023110%\n",
      "Iteration: 1833\n",
      "Loss: 0.6463093785109868\n",
      "Relative change: 0.023093%\n",
      "Iteration: 1834\n",
      "Loss: 0.6461602360445582\n",
      "Relative change: 0.023076%\n",
      "Iteration: 1835\n",
      "Loss: 0.6460112365719745\n",
      "Relative change: 0.023059%\n",
      "Iteration: 1836\n",
      "Loss: 0.6458623798425537\n",
      "Relative change: 0.023042%\n",
      "Iteration: 1837\n",
      "Loss: 0.6457136656065497\n",
      "Relative change: 0.023026%\n",
      "Iteration: 1838\n",
      "Loss: 0.6455650936151465\n",
      "Relative change: 0.023009%\n",
      "Iteration: 1839\n",
      "Loss: 0.6454166636204518\n",
      "Relative change: 0.022992%\n",
      "Iteration: 1840\n",
      "Loss: 0.6452683753754919\n",
      "Relative change: 0.022976%\n",
      "Iteration: 1841\n",
      "Loss: 0.6451202286342049\n",
      "Relative change: 0.022959%\n",
      "Iteration: 1842\n",
      "Loss: 0.6449722231514357\n",
      "Relative change: 0.022942%\n",
      "Iteration: 1843\n",
      "Loss: 0.6448243586829285\n",
      "Relative change: 0.022926%\n",
      "Iteration: 1844\n",
      "Loss: 0.6446766349853232\n",
      "Relative change: 0.022909%\n",
      "Iteration: 1845\n",
      "Loss: 0.6445290518161478\n",
      "Relative change: 0.022893%\n",
      "Iteration: 1846\n",
      "Loss: 0.6443816089338128\n",
      "Relative change: 0.022876%\n",
      "Iteration: 1847\n",
      "Loss: 0.6442343060976062\n",
      "Relative change: 0.022860%\n",
      "Iteration: 1848\n",
      "Loss: 0.644087143067687\n",
      "Relative change: 0.022843%\n",
      "Iteration: 1849\n",
      "Loss: 0.6439401196050789\n",
      "Relative change: 0.022827%\n",
      "Iteration: 1850\n",
      "Loss: 0.6437932354716661\n",
      "Relative change: 0.022810%\n",
      "Iteration: 1851\n",
      "Loss: 0.6436464904301858\n",
      "Relative change: 0.022794%\n",
      "Iteration: 1852\n",
      "Loss: 0.6434998842442239\n",
      "Relative change: 0.022777%\n",
      "Iteration: 1853\n",
      "Loss: 0.6433534166782081\n",
      "Relative change: 0.022761%\n",
      "Iteration: 1854\n",
      "Loss: 0.6432070874974024\n",
      "Relative change: 0.022745%\n",
      "Iteration: 1855\n",
      "Loss: 0.6430608964679024\n",
      "Relative change: 0.022728%\n",
      "Iteration: 1856\n",
      "Loss: 0.6429148433566285\n",
      "Relative change: 0.022712%\n",
      "Iteration: 1857\n",
      "Loss: 0.6427689279313202\n",
      "Relative change: 0.022696%\n",
      "Iteration: 1858\n",
      "Loss: 0.6426231499605317\n",
      "Relative change: 0.022680%\n",
      "Iteration: 1859\n",
      "Loss: 0.6424775092136243\n",
      "Relative change: 0.022663%\n",
      "Iteration: 1860\n",
      "Loss: 0.6423320054607623\n",
      "Relative change: 0.022647%\n",
      "Iteration: 1861\n",
      "Loss: 0.6421866384729071\n",
      "Relative change: 0.022631%\n",
      "Iteration: 1862\n",
      "Loss: 0.6420414080218109\n",
      "Relative change: 0.022615%\n",
      "Iteration: 1863\n",
      "Loss: 0.641896313880012\n",
      "Relative change: 0.022599%\n",
      "Iteration: 1864\n",
      "Loss: 0.6417513558208288\n",
      "Relative change: 0.022583%\n",
      "Iteration: 1865\n",
      "Loss: 0.6416065336183542\n",
      "Relative change: 0.022567%\n",
      "Iteration: 1866\n",
      "Loss: 0.6414618470474495\n",
      "Relative change: 0.022551%\n",
      "Iteration: 1867\n",
      "Loss: 0.6413172958837406\n",
      "Relative change: 0.022535%\n",
      "Iteration: 1868\n",
      "Loss: 0.6411728799036106\n",
      "Relative change: 0.022519%\n",
      "Iteration: 1869\n",
      "Loss: 0.6410285988841954\n",
      "Relative change: 0.022503%\n",
      "Iteration: 1870\n",
      "Loss: 0.6408844526033777\n",
      "Relative change: 0.022487%\n",
      "Iteration: 1871\n",
      "Loss: 0.6407404408397817\n",
      "Relative change: 0.022471%\n",
      "Iteration: 1872\n",
      "Loss: 0.6405965633727678\n",
      "Relative change: 0.022455%\n",
      "Iteration: 1873\n",
      "Loss: 0.6404528199824272\n",
      "Relative change: 0.022439%\n",
      "Iteration: 1874\n",
      "Loss: 0.6403092104495757\n",
      "Relative change: 0.022423%\n",
      "Iteration: 1875\n",
      "Loss: 0.6401657345557495\n",
      "Relative change: 0.022407%\n",
      "Iteration: 1876\n",
      "Loss: 0.6400223920831993\n",
      "Relative change: 0.022391%\n",
      "Iteration: 1877\n",
      "Loss: 0.6398791828148843\n",
      "Relative change: 0.022376%\n",
      "Iteration: 1878\n",
      "Loss: 0.6397361065344678\n",
      "Relative change: 0.022360%\n",
      "Iteration: 1879\n",
      "Loss: 0.6395931630263112\n",
      "Relative change: 0.022344%\n",
      "Iteration: 1880\n",
      "Loss: 0.6394503520754696\n",
      "Relative change: 0.022328%\n",
      "Iteration: 1881\n",
      "Loss: 0.6393076734676846\n",
      "Relative change: 0.022313%\n",
      "Iteration: 1882\n",
      "Loss: 0.6391651269893815\n",
      "Relative change: 0.022297%\n",
      "Iteration: 1883\n",
      "Loss: 0.6390227124276623\n",
      "Relative change: 0.022281%\n",
      "Iteration: 1884\n",
      "Loss: 0.6388804295703009\n",
      "Relative change: 0.022266%\n",
      "Iteration: 1885\n",
      "Loss: 0.6387382782057378\n",
      "Relative change: 0.022250%\n",
      "Iteration: 1886\n",
      "Loss: 0.6385962581230753\n",
      "Relative change: 0.022234%\n",
      "Iteration: 1887\n",
      "Loss: 0.6384543691120717\n",
      "Relative change: 0.022219%\n",
      "Iteration: 1888\n",
      "Loss: 0.638312610963137\n",
      "Relative change: 0.022203%\n",
      "Iteration: 1889\n",
      "Loss: 0.6381709834673264\n",
      "Relative change: 0.022188%\n",
      "Iteration: 1890\n",
      "Loss: 0.6380294864163364\n",
      "Relative change: 0.022172%\n",
      "Iteration: 1891\n",
      "Loss: 0.6378881196024992\n",
      "Relative change: 0.022157%\n",
      "Iteration: 1892\n",
      "Loss: 0.6377468828187778\n",
      "Relative change: 0.022141%\n",
      "Iteration: 1893\n",
      "Loss: 0.6376057758587601\n",
      "Relative change: 0.022126%\n",
      "Iteration: 1894\n",
      "Loss: 0.6374647985166552\n",
      "Relative change: 0.022110%\n",
      "Iteration: 1895\n",
      "Loss: 0.6373239505872871\n",
      "Relative change: 0.022095%\n",
      "Iteration: 1896\n",
      "Loss: 0.6371832318660908\n",
      "Relative change: 0.022080%\n",
      "Iteration: 1897\n",
      "Loss: 0.6370426421491064\n",
      "Relative change: 0.022064%\n",
      "Iteration: 1898\n",
      "Loss: 0.6369021812329744\n",
      "Relative change: 0.022049%\n",
      "Iteration: 1899\n",
      "Loss: 0.6367618489149313\n",
      "Relative change: 0.022034%\n",
      "Iteration: 1900\n",
      "Loss: 0.6366216449928035\n",
      "Relative change: 0.022018%\n",
      "Iteration: 1901\n",
      "Loss: 0.6364815692650033\n",
      "Relative change: 0.022003%\n",
      "Iteration: 1902\n",
      "Loss: 0.6363416215305243\n",
      "Relative change: 0.021988%\n",
      "Iteration: 1903\n",
      "Loss: 0.6362018015889356\n",
      "Relative change: 0.021972%\n",
      "Iteration: 1904\n",
      "Loss: 0.6360621092403774\n",
      "Relative change: 0.021957%\n",
      "Iteration: 1905\n",
      "Loss: 0.6359225442855563\n",
      "Relative change: 0.021942%\n",
      "Iteration: 1906\n",
      "Loss: 0.6357831065257402\n",
      "Relative change: 0.021927%\n",
      "Iteration: 1907\n",
      "Loss: 0.6356437957627541\n",
      "Relative change: 0.021912%\n",
      "Iteration: 1908\n",
      "Loss: 0.6355046117989742\n",
      "Relative change: 0.021897%\n",
      "Iteration: 1909\n",
      "Loss: 0.6353655544373251\n",
      "Relative change: 0.021881%\n",
      "Iteration: 1910\n",
      "Loss: 0.6352266234812729\n",
      "Relative change: 0.021866%\n",
      "Iteration: 1911\n",
      "Loss: 0.6350878187348221\n",
      "Relative change: 0.021851%\n",
      "Iteration: 1912\n",
      "Loss: 0.6349491400025103\n",
      "Relative change: 0.021836%\n",
      "Iteration: 1913\n",
      "Loss: 0.6348105870894037\n",
      "Relative change: 0.021821%\n",
      "Iteration: 1914\n",
      "Loss: 0.6346721598010928\n",
      "Relative change: 0.021806%\n",
      "Iteration: 1915\n",
      "Loss: 0.6345338579436871\n",
      "Relative change: 0.021791%\n",
      "Iteration: 1916\n",
      "Loss: 0.6343956813238109\n",
      "Relative change: 0.021776%\n",
      "Iteration: 1917\n",
      "Loss: 0.6342576297485996\n",
      "Relative change: 0.021761%\n",
      "Iteration: 1918\n",
      "Loss: 0.6341197030256938\n",
      "Relative change: 0.021746%\n",
      "Iteration: 1919\n",
      "Loss: 0.6339819009632358\n",
      "Relative change: 0.021731%\n",
      "Iteration: 1920\n",
      "Loss: 0.6338442233698649\n",
      "Relative change: 0.021716%\n",
      "Iteration: 1921\n",
      "Loss: 0.6337066700547127\n",
      "Relative change: 0.021701%\n",
      "Iteration: 1922\n",
      "Loss: 0.6335692408273994\n",
      "Relative change: 0.021687%\n",
      "Iteration: 1923\n",
      "Loss: 0.6334319354980287\n",
      "Relative change: 0.021672%\n",
      "Iteration: 1924\n",
      "Loss: 0.6332947538771841\n",
      "Relative change: 0.021657%\n",
      "Iteration: 1925\n",
      "Loss: 0.6331576957759237\n",
      "Relative change: 0.021642%\n",
      "Iteration: 1926\n",
      "Loss: 0.6330207610057773\n",
      "Relative change: 0.021627%\n",
      "Iteration: 1927\n",
      "Loss: 0.6328839493787413\n",
      "Relative change: 0.021613%\n",
      "Iteration: 1928\n",
      "Loss: 0.6327472607072737\n",
      "Relative change: 0.021598%\n",
      "Iteration: 1929\n",
      "Loss: 0.6326106948042917\n",
      "Relative change: 0.021583%\n",
      "Iteration: 1930\n",
      "Loss: 0.6324742514831662\n",
      "Relative change: 0.021568%\n",
      "Iteration: 1931\n",
      "Loss: 0.6323379305577184\n",
      "Relative change: 0.021554%\n",
      "Iteration: 1932\n",
      "Loss: 0.6322017318422151\n",
      "Relative change: 0.021539%\n",
      "Iteration: 1933\n",
      "Loss: 0.6320656551513648\n",
      "Relative change: 0.021524%\n",
      "Iteration: 1934\n",
      "Loss: 0.6319297003003149\n",
      "Relative change: 0.021510%\n",
      "Iteration: 1935\n",
      "Loss: 0.6317938671046449\n",
      "Relative change: 0.021495%\n",
      "Iteration: 1936\n",
      "Loss: 0.6316581553803654\n",
      "Relative change: 0.021480%\n",
      "Iteration: 1937\n",
      "Loss: 0.6315225649439123\n",
      "Relative change: 0.021466%\n",
      "Iteration: 1938\n",
      "Loss: 0.6313870956121439\n",
      "Relative change: 0.021451%\n",
      "Iteration: 1939\n",
      "Loss: 0.631251747202336\n",
      "Relative change: 0.021437%\n",
      "Iteration: 1940\n",
      "Loss: 0.6311165195321791\n",
      "Relative change: 0.021422%\n",
      "Iteration: 1941\n",
      "Loss: 0.6309814124197736\n",
      "Relative change: 0.021408%\n",
      "Iteration: 1942\n",
      "Loss: 0.6308464256836273\n",
      "Relative change: 0.021393%\n",
      "Iteration: 1943\n",
      "Loss: 0.6307115591426498\n",
      "Relative change: 0.021379%\n",
      "Iteration: 1944\n",
      "Loss: 0.6305768126161506\n",
      "Relative change: 0.021364%\n",
      "Iteration: 1945\n",
      "Loss: 0.6304421859238346\n",
      "Relative change: 0.021350%\n",
      "Iteration: 1946\n",
      "Loss: 0.6303076788857982\n",
      "Relative change: 0.021335%\n",
      "Iteration: 1947\n",
      "Loss: 0.6301732913225258\n",
      "Relative change: 0.021321%\n",
      "Iteration: 1948\n",
      "Loss: 0.6300390230548867\n",
      "Relative change: 0.021307%\n",
      "Iteration: 1949\n",
      "Loss: 0.6299048739041307\n",
      "Relative change: 0.021292%\n",
      "Iteration: 1950\n",
      "Loss: 0.6297708436918855\n",
      "Relative change: 0.021278%\n",
      "Iteration: 1951\n",
      "Loss: 0.6296369322401524\n",
      "Relative change: 0.021264%\n",
      "Iteration: 1952\n",
      "Loss: 0.6295031393713034\n",
      "Relative change: 0.021249%\n",
      "Iteration: 1953\n",
      "Loss: 0.6293694649080771\n",
      "Relative change: 0.021235%\n",
      "Iteration: 1954\n",
      "Loss: 0.6292359086735764\n",
      "Relative change: 0.021221%\n",
      "Iteration: 1955\n",
      "Loss: 0.6291024704912631\n",
      "Relative change: 0.021206%\n",
      "Iteration: 1956\n",
      "Loss: 0.6289691501849578\n",
      "Relative change: 0.021192%\n",
      "Iteration: 1957\n",
      "Loss: 0.6288359475788332\n",
      "Relative change: 0.021178%\n",
      "Iteration: 1958\n",
      "Loss: 0.6287028624974125\n",
      "Relative change: 0.021164%\n",
      "Iteration: 1959\n",
      "Loss: 0.6285698947655667\n",
      "Relative change: 0.021150%\n",
      "Iteration: 1960\n",
      "Loss: 0.6284370442085097\n",
      "Relative change: 0.021135%\n",
      "Iteration: 1961\n",
      "Loss: 0.6283043106517969\n",
      "Relative change: 0.021121%\n",
      "Iteration: 1962\n",
      "Loss: 0.6281716939213209\n",
      "Relative change: 0.021107%\n",
      "Iteration: 1963\n",
      "Loss: 0.6280391938433084\n",
      "Relative change: 0.021093%\n",
      "Iteration: 1964\n",
      "Loss: 0.6279068102443179\n",
      "Relative change: 0.021079%\n",
      "Iteration: 1965\n",
      "Loss: 0.6277745429512364\n",
      "Relative change: 0.021065%\n",
      "Iteration: 1966\n",
      "Loss: 0.6276423917912753\n",
      "Relative change: 0.021051%\n",
      "Iteration: 1967\n",
      "Loss: 0.6275103565919692\n",
      "Relative change: 0.021037%\n",
      "Iteration: 1968\n",
      "Loss: 0.6273784371811719\n",
      "Relative change: 0.021023%\n",
      "Iteration: 1969\n",
      "Loss: 0.6272466333870533\n",
      "Relative change: 0.021009%\n",
      "Iteration: 1970\n",
      "Loss: 0.6271149450380973\n",
      "Relative change: 0.020995%\n",
      "Iteration: 1971\n",
      "Loss: 0.6269833719630982\n",
      "Relative change: 0.020981%\n",
      "Iteration: 1972\n",
      "Loss: 0.6268519139911582\n",
      "Relative change: 0.020967%\n",
      "Iteration: 1973\n",
      "Loss: 0.6267205709516854\n",
      "Relative change: 0.020953%\n",
      "Iteration: 1974\n",
      "Loss: 0.6265893426743894\n",
      "Relative change: 0.020939%\n",
      "Iteration: 1975\n",
      "Loss: 0.6264582289892799\n",
      "Relative change: 0.020925%\n",
      "Iteration: 1976\n",
      "Loss: 0.6263272297266638\n",
      "Relative change: 0.020911%\n",
      "Iteration: 1977\n",
      "Loss: 0.6261963447171417\n",
      "Relative change: 0.020897%\n",
      "Iteration: 1978\n",
      "Loss: 0.6260655737916071\n",
      "Relative change: 0.020883%\n",
      "Iteration: 1979\n",
      "Loss: 0.6259349167812415\n",
      "Relative change: 0.020870%\n",
      "Iteration: 1980\n",
      "Loss: 0.6258043735175136\n",
      "Relative change: 0.020856%\n",
      "Iteration: 1981\n",
      "Loss: 0.6256739438321766\n",
      "Relative change: 0.020842%\n",
      "Iteration: 1982\n",
      "Loss: 0.6255436275572646\n",
      "Relative change: 0.020828%\n",
      "Iteration: 1983\n",
      "Loss: 0.6254134245250914\n",
      "Relative change: 0.020814%\n",
      "Iteration: 1984\n",
      "Loss: 0.6252833345682471\n",
      "Relative change: 0.020801%\n",
      "Iteration: 1985\n",
      "Loss: 0.6251533575195971\n",
      "Relative change: 0.020787%\n",
      "Iteration: 1986\n",
      "Loss: 0.6250234932122781\n",
      "Relative change: 0.020773%\n",
      "Iteration: 1987\n",
      "Loss: 0.6248937414796965\n",
      "Relative change: 0.020759%\n",
      "Iteration: 1988\n",
      "Loss: 0.6247641021555268\n",
      "Relative change: 0.020746%\n",
      "Iteration: 1989\n",
      "Loss: 0.6246345750737078\n",
      "Relative change: 0.020732%\n",
      "Iteration: 1990\n",
      "Loss: 0.6245051600684421\n",
      "Relative change: 0.020719%\n",
      "Iteration: 1991\n",
      "Loss: 0.6243758569741932\n",
      "Relative change: 0.020705%\n",
      "Iteration: 1992\n",
      "Loss: 0.6242466656256822\n",
      "Relative change: 0.020691%\n",
      "Iteration: 1993\n",
      "Loss: 0.6241175858578877\n",
      "Relative change: 0.020678%\n",
      "Iteration: 1994\n",
      "Loss: 0.6239886175060423\n",
      "Relative change: 0.020664%\n",
      "Iteration: 1995\n",
      "Loss: 0.6238597604056313\n",
      "Relative change: 0.020651%\n",
      "Iteration: 1996\n",
      "Loss: 0.6237310143923899\n",
      "Relative change: 0.020637%\n",
      "Iteration: 1997\n",
      "Loss: 0.6236023793023022\n",
      "Relative change: 0.020623%\n",
      "Iteration: 1998\n",
      "Loss: 0.6234738549715982\n",
      "Relative change: 0.020610%\n",
      "Iteration: 1999\n",
      "Loss: 0.623345441236753\n",
      "Relative change: 0.020596%\n",
      "Iteration: 2000\n",
      "Loss: 0.6232171379344833\n",
      "Relative change: 0.020583%\n",
      "Iteration: 2001\n",
      "Loss: 0.6230889449017474\n",
      "Relative change: 0.020570%\n",
      "Iteration: 2002\n",
      "Loss: 0.6229608619757421\n",
      "Relative change: 0.020556%\n",
      "Iteration: 2003\n",
      "Loss: 0.6228328889939013\n",
      "Relative change: 0.020543%\n",
      "Iteration: 2004\n",
      "Loss: 0.6227050257938942\n",
      "Relative change: 0.020529%\n",
      "Iteration: 2005\n",
      "Loss: 0.6225772722136239\n",
      "Relative change: 0.020516%\n",
      "Iteration: 2006\n",
      "Loss: 0.6224496280912244\n",
      "Relative change: 0.020503%\n",
      "Iteration: 2007\n",
      "Loss: 0.622322093265061\n",
      "Relative change: 0.020489%\n",
      "Iteration: 2008\n",
      "Loss: 0.6221946675737264\n",
      "Relative change: 0.020476%\n",
      "Iteration: 2009\n",
      "Loss: 0.6220673508560416\n",
      "Relative change: 0.020463%\n",
      "Iteration: 2010\n",
      "Loss: 0.6219401429510515\n",
      "Relative change: 0.020449%\n",
      "Iteration: 2011\n",
      "Loss: 0.6218130436980257\n",
      "Relative change: 0.020436%\n",
      "Iteration: 2012\n",
      "Loss: 0.6216860529364554\n",
      "Relative change: 0.020423%\n",
      "Iteration: 2013\n",
      "Loss: 0.6215591705060529\n",
      "Relative change: 0.020409%\n",
      "Iteration: 2014\n",
      "Loss: 0.6214323962467496\n",
      "Relative change: 0.020396%\n",
      "Iteration: 2015\n",
      "Loss: 0.6213057299986953\n",
      "Relative change: 0.020383%\n",
      "Iteration: 2016\n",
      "Loss: 0.621179171602255\n",
      "Relative change: 0.020370%\n",
      "Iteration: 2017\n",
      "Loss: 0.6210527208980101\n",
      "Relative change: 0.020357%\n",
      "Iteration: 2018\n",
      "Loss: 0.6209263777267551\n",
      "Relative change: 0.020343%\n",
      "Iteration: 2019\n",
      "Loss: 0.6208001419294964\n",
      "Relative change: 0.020330%\n",
      "Iteration: 2020\n",
      "Loss: 0.6206740133474525\n",
      "Relative change: 0.020317%\n",
      "Iteration: 2021\n",
      "Loss: 0.6205479918220511\n",
      "Relative change: 0.020304%\n",
      "Iteration: 2022\n",
      "Loss: 0.6204220771949287\n",
      "Relative change: 0.020291%\n",
      "Iteration: 2023\n",
      "Loss: 0.6202962693079295\n",
      "Relative change: 0.020278%\n",
      "Iteration: 2024\n",
      "Loss: 0.620170568003103\n",
      "Relative change: 0.020265%\n",
      "Iteration: 2025\n",
      "Loss: 0.6200449731227049\n",
      "Relative change: 0.020252%\n",
      "Iteration: 2026\n",
      "Loss: 0.6199194845091944\n",
      "Relative change: 0.020239%\n",
      "Iteration: 2027\n",
      "Loss: 0.6197941020052339\n",
      "Relative change: 0.020226%\n",
      "Iteration: 2028\n",
      "Loss: 0.6196688254536873\n",
      "Relative change: 0.020213%\n",
      "Iteration: 2029\n",
      "Loss: 0.6195436546976196\n",
      "Relative change: 0.020200%\n",
      "Iteration: 2030\n",
      "Loss: 0.6194185895802956\n",
      "Relative change: 0.020187%\n",
      "Iteration: 2031\n",
      "Loss: 0.619293629945179\n",
      "Relative change: 0.020174%\n",
      "Iteration: 2032\n",
      "Loss: 0.6191687756359313\n",
      "Relative change: 0.020161%\n",
      "Iteration: 2033\n",
      "Loss: 0.6190440264964112\n",
      "Relative change: 0.020148%\n",
      "Iteration: 2034\n",
      "Loss: 0.6189193823706737\n",
      "Relative change: 0.020135%\n",
      "Iteration: 2035\n",
      "Loss: 0.6187948431029686\n",
      "Relative change: 0.020122%\n",
      "Iteration: 2036\n",
      "Loss: 0.6186704085377409\n",
      "Relative change: 0.020109%\n",
      "Iteration: 2037\n",
      "Loss: 0.6185460785196283\n",
      "Relative change: 0.020096%\n",
      "Iteration: 2038\n",
      "Loss: 0.6184218528934622\n",
      "Relative change: 0.020083%\n",
      "Iteration: 2039\n",
      "Loss: 0.618297731504266\n",
      "Relative change: 0.020071%\n",
      "Iteration: 2040\n",
      "Loss: 0.6181737141972544\n",
      "Relative change: 0.020058%\n",
      "Iteration: 2041\n",
      "Loss: 0.6180498008178328\n",
      "Relative change: 0.020045%\n",
      "Iteration: 2042\n",
      "Loss: 0.6179259912115966\n",
      "Relative change: 0.020032%\n",
      "Iteration: 2043\n",
      "Loss: 0.6178022852243313\n",
      "Relative change: 0.020020%\n",
      "Iteration: 2044\n",
      "Loss: 0.6176786827020103\n",
      "Relative change: 0.020007%\n",
      "Iteration: 2045\n",
      "Loss: 0.6175551834907957\n",
      "Relative change: 0.019994%\n"
     ]
    }
   ],
   "source": [
    "#  train model with relative n_iters\n",
    "W1, b1, W2, b2, losses,iteration = fit(X_train_scaled, y_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 10.03%\n",
      "Iteration:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-1d4613750e1e>:3: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "predictions = predict(X_test, W1, b1, W2, b2)\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Iteration:{iteration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.780e+00, 2.140e+00, 1.120e+01, ..., 1.050e+00, 3.400e+00,\n",
       "        1.050e+03],\n",
       "       [2.360e+00, 2.670e+00, 1.860e+01, ..., 1.030e+00, 3.170e+00,\n",
       "        1.185e+03],\n",
       "       [1.950e+00, 2.500e+00, 1.680e+01, ..., 8.600e-01, 3.450e+00,\n",
       "        1.480e+03],\n",
       "       ...,\n",
       "       [4.280e+00, 2.260e+00, 2.000e+01, ..., 5.900e-01, 1.560e+00,\n",
       "        8.350e+02],\n",
       "       [2.590e+00, 2.370e+00, 2.000e+01, ..., 6.000e-01, 1.620e+00,\n",
       "        8.400e+02],\n",
       "       [4.100e+00, 2.740e+00, 2.450e+01, ..., 6.100e-01, 1.600e+00,\n",
       "        5.600e+02]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:, [2,3,4,5,6,7,8,9,10,11,12,13]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "#   Read Data\n",
    "#########################################################################\n",
    "\n",
    "\n",
    "with open('wine.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    next(csv_reader) # to skip the header file\n",
    "    X = []\n",
    "    y = []\n",
    "    for row in csv_reader:\n",
    "        X.append([float(row[0]), 1])\n",
    "        y.append(float(row[1]))\n",
    "\n",
    "X = np.asarray()\n",
    "y = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asarray(df.iloc[:, [2,3,4,5,6,7,8,9,10,11,12,13]].values)\n",
    "y = np.asarray(df.iloc[:, 1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'J' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-560093f18909>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtheta_direct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Solution (closed-form): J={:.1f}, Theta=({:.2f}, {:.2f})'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mJ\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta_direct\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta_direct\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta_direct\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'J' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#########################################################################\n",
    "#   Closed-form method\n",
    "#########################################################################\n",
    "\n",
    "theta_direct = np.matmul(np.matmul(np.linalg.inv(np.matmul(np.transpose(X), X)), np.transpose(X)), y)\n",
    "\n",
    "print('Solution (closed-form): J={:.1f}, Theta=({:.2f}, {:.2f})'.format(J(X,y, theta_direct), theta_direct[0], theta_direct[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'J' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-f854ccafc8e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mtheta_gd\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheta_gd\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mt_1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Solution (Gradient descent): J={:.1f}, Theta=({:.2f}, {:.2f})'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mJ\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta_gd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta_gd\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta_gd\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'J' is not defined"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "#   Gradient Descent method\n",
    "#########################################################################\n",
    "\n",
    "learning_rate = 0.0000001\n",
    "tot_iterations = 100\n",
    "\n",
    "theta_gd = [uniform(0., 0.5), uniform(750., 1000.)]\n",
    "\n",
    "for i in range(tot_iterations):\n",
    "    t_0 = 0\n",
    "    t_1 = 0\n",
    "    for j in range(len(y)):\n",
    "        t_0 +=  (theta_gd[0] * X[j][0] + theta_gd[1] - y[j]) * X[j][0]\n",
    "        t_1 +=  theta_gd[0] * X[j][0] + theta_gd[1] - y[j]\n",
    "\n",
    "    t_0 /= len(y)\n",
    "    t_1 /= len(y)\n",
    "\n",
    "    theta_gd[0] = theta_gd[0] - learning_rate * t_0\n",
    "    theta_gd[1] = theta_gd[1] - learning_rate * t_1\n",
    "\n",
    "print('Solution (Gradient descent): J={:.1f}, Theta=({:.2f}, {:.2f})'.format(J(X,y, theta_gd), theta_gd[0], theta_gd[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "#   Tensor flow\n",
    "#########################################################################\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "# Graph Definition\n",
    "\n",
    "x_data = tf.placeholder(shape=[None, len(X[0])], dtype=tf.float64)\n",
    "y_target = tf.placeholder(shape=[None], dtype=tf.float64)\n",
    "\n",
    "theta_gd_initial = np.random.uniform(-0.5, 0.5, size=(len(X[0]),))\n",
    "#weights = tf.Variable(tf.random.uniform(shape=[1, len(X[0])], minval=750., maxval=1000))\n",
    "weights = tf.Variable(tf.convert_to_tensor(theta_gd_initial))\n",
    "#criar um vetor para os 12 pesos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Model\n",
    "with tf.variable_scope('model_definition') as scope:\n",
    "    model_output = 1.0/(1.0 + tf.exp(- tf.matmul(x_data, tf.expand_dims(weights,1))))\n",
    "    scope.reuse_variables()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_l2(predict, gt):\n",
    "    predict = tf.squeeze(predict)\n",
    "    #predict = tf.Print(predict,[\"predict: \", tf.shape(predict)])\n",
    "    resid = predict - gt\n",
    "    ret = tf.sqrt(tf.reduce_sum(tf.pow(resid, tf.constant(2.))))\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Input 'y' of 'Pow' Op has type float32 that does not match type float64 of argument 'x'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-1dcc9bb58b6b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_l2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmy_opt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtrain_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmy_opt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-64-a0f60545909e>\u001b[0m in \u001b[0;36mloss_l2\u001b[1;34m(predict, gt)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m#predict = tf.Print(predict,[\"predict: \", tf.shape(predict)])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mresid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mgt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\rene_\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\rene_\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_ExtractInputsAndAttrs\u001b[1;34m(op_type_name, op_def, allowed_list_attr_map, keywords, default_type_attr_map, attrs, inputs, input_types)\u001b[0m\n\u001b[0;32m    588\u001b[0m                 \u001b[0minferred_from\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Default in OpDef\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m           raise TypeError(\n\u001b[0m\u001b[0;32m    591\u001b[0m               \u001b[1;34mf\"{prefix} type \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m               \u001b[1;34mf\"{dtypes.as_dtype(attrs[input_arg.type_attr]).name} of \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Input 'y' of 'Pow' Op has type float32 that does not match type float64 of argument 'x'."
     ]
    }
   ],
   "source": [
    "loss = loss_l2(model_output, y_target)\n",
    "my_opt = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train_step = my_opt.minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution (Tensor flow): J=649.3, Theta=(0.34, 893.37)\n"
     ]
    }
   ],
   "source": [
    "# Graph execution\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "\n",
    "for i in range(tot_iterations):\n",
    "    sess.run(train_step, feed_dict={x_data: X, y_target: y})\n",
    "\n",
    "theta_tf = sess.run(weights)\n",
    "cur_loss = J(X,y, theta_tf)\n",
    "\n",
    "print('Solution (Tensor flow): J={:.1f}, Theta=({:.2f}, {:.2f})'.format(cur_loss, theta_tf[0][0], theta_tf[1][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#########################################################################\n",
    "#   TResults Visualization\n",
    "#########################################################################\n",
    "plt.ion()\n",
    "plt.figure(2)\n",
    "plt.plot(X[:,0], y, 'o')\n",
    "plt.plot([np.min(X[:,0]), np.max(X[:,0])],[theta_direct[0]*np.min(X[:,0])+theta_direct[1], theta_direct[0]*np.max(X[:,0])+theta_direct[1] ] ,'-r', label='Closed-form')\n",
    "plt.plot([np.min(X[:,0]), np.max(X[:,0])],[theta_gd[0]*np.min(X[:,0])+theta_gd[1], theta_gd[0]*np.max(X[:,0])+theta_gd[1] ] ,'-g', label='Gradient Descent')\n",
    "plt.plot([np.min(X[:,0]), np.max(X[:,0])],[theta_tf[0]*np.min(X[:,0])+theta_tf[1], theta_tf[0]*np.max(X[:,0])+theta_tf[1] ] ,'-b', label='Tensor flow')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "plt.pause(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input('Close app?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
