{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid black\">\n",
    "<b><center><font size=\"4\">Data Science</font></center></b>\n",
    "\n",
    "<b><center><font size=\"3\">Manipulação e Análise de Dados com Pandas Dataframes</font></center></b>\n",
    "\n",
    "<b><center><font size=\"2\">6 - Exploratory Data Analysis</font></center></b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Developed by**: [Ricardo Campos](https://www.di.ubi.pt/~rcampos)<br>\n",
    "**email:**  ricardo.campos@ubi.pt<br>\n",
    "**Affiliation:** *Assistant Professor* @ [University of Beira Interior](http://www.ubi.pt);\n",
    "*Researcher* @ [LIAAD](https://www.inesctec.pt/en/centres/liaad)-[INESC TEC](https://www.inesctec.pt/en)\n",
    "\n",
    "<hr>\n",
    "\n",
    "<p><a href=\"6ExploratoryDataAnalysis.ipynb\" title=\"Download Notebook\" download><img src=\"https://www.di.ubi.pt/~rcampos/assets/img_tutorials/download.jpg\" align = \"left\" width=\"50\" height=\"50\" alt=\"Download Notebook\"></a></p>\n",
    "<p>&nbsp;</p>\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data-Loading\" data-toc-modified-id=\"Data-Loading-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Data Loading</a></span></li><li><span><a href=\"#Understand-the-big-picture\" data-toc-modified-id=\"Understand-the-big-picture-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Understand the big picture</a></span><ul class=\"toc-item\"><li><span><a href=\"#View-the-Dataframe\" data-toc-modified-id=\"View-the-Dataframe-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>View the Dataframe</a></span></li><li><span><a href=\"#Basic-Information\" data-toc-modified-id=\"Basic-Information-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Basic Information</a></span><ul class=\"toc-item\"><li><span><a href=\"#Shape\" data-toc-modified-id=\"Shape-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Shape</a></span></li><li><span><a href=\"#info\" data-toc-modified-id=\"info-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>info</a></span></li><li><span><a href=\"#dtypes\" data-toc-modified-id=\"dtypes-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>dtypes</a></span></li><li><span><a href=\"#index\" data-toc-modified-id=\"index-2.2.4\"><span class=\"toc-item-num\">2.2.4&nbsp;&nbsp;</span>index</a></span></li><li><span><a href=\"#columns\" data-toc-modified-id=\"columns-2.2.5\"><span class=\"toc-item-num\">2.2.5&nbsp;&nbsp;</span>columns</a></span></li></ul></li></ul></li><li><span><a href=\"#Data-Cleaning\" data-toc-modified-id=\"Data-Cleaning-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Data Cleaning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dropping-columns\" data-toc-modified-id=\"Dropping-columns-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Dropping columns</a></span></li><li><span><a href=\"#Missing-Data\" data-toc-modified-id=\"Missing-Data-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Missing Data</a></span></li><li><span><a href=\"#Duplicate-Values\" data-toc-modified-id=\"Duplicate-Values-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Duplicate Values</a></span></li></ul></li><li><span><a href=\"#Descriptive-Stats\" data-toc-modified-id=\"Descriptive-Stats-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Descriptive Stats</a></span><ul class=\"toc-item\"><li><span><a href=\"#Describe\" data-toc-modified-id=\"Describe-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Describe</a></span></li><li><span><a href=\"#Basic-Stats\" data-toc-modified-id=\"Basic-Stats-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Basic Stats</a></span><ul class=\"toc-item\"><li><span><a href=\"#Sum\" data-toc-modified-id=\"Sum-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Sum</a></span></li><li><span><a href=\"#Min\" data-toc-modified-id=\"Min-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>Min</a></span></li><li><span><a href=\"#Max\" data-toc-modified-id=\"Max-4.2.3\"><span class=\"toc-item-num\">4.2.3&nbsp;&nbsp;</span>Max</a></span></li></ul></li><li><span><a href=\"#Central-Tendency-Measures\" data-toc-modified-id=\"Central-Tendency-Measures-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Central Tendency Measures</a></span><ul class=\"toc-item\"><li><span><a href=\"#Mean\" data-toc-modified-id=\"Mean-4.3.1\"><span class=\"toc-item-num\">4.3.1&nbsp;&nbsp;</span>Mean</a></span></li><li><span><a href=\"#Median\" data-toc-modified-id=\"Median-4.3.2\"><span class=\"toc-item-num\">4.3.2&nbsp;&nbsp;</span>Median</a></span></li><li><span><a href=\"#Mode\" data-toc-modified-id=\"Mode-4.3.3\"><span class=\"toc-item-num\">4.3.3&nbsp;&nbsp;</span>Mode</a></span></li></ul></li><li><span><a href=\"#Measures-of-Variability\" data-toc-modified-id=\"Measures-of-Variability-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Measures of Variability</a></span><ul class=\"toc-item\"><li><span><a href=\"#Variance-(Amplitude)\" data-toc-modified-id=\"Variance-(Amplitude)-4.4.1\"><span class=\"toc-item-num\">4.4.1&nbsp;&nbsp;</span>Variance (Amplitude)</a></span></li><li><span><a href=\"#Standard-Deviation\" data-toc-modified-id=\"Standard-Deviation-4.4.2\"><span class=\"toc-item-num\">4.4.2&nbsp;&nbsp;</span>Standard Deviation</a></span></li></ul></li></ul></li><li><span><a href=\"#Data-Analysis\" data-toc-modified-id=\"Data-Analysis-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Data Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Univariate-analysis\" data-toc-modified-id=\"Univariate-analysis-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Univariate analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Categorical-unordered-(nominal)\" data-toc-modified-id=\"Categorical-unordered-(nominal)-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Categorical unordered (nominal)</a></span><ul class=\"toc-item\"><li><span><a href=\"#unique\" data-toc-modified-id=\"unique-5.1.1.1\"><span class=\"toc-item-num\">5.1.1.1&nbsp;&nbsp;</span>unique</a></span></li><li><span><a href=\"#value_counts\" data-toc-modified-id=\"value_counts-5.1.1.2\"><span class=\"toc-item-num\">5.1.1.2&nbsp;&nbsp;</span>value_counts</a></span><ul class=\"toc-item\"><li><span><a href=\"#bar-plot\" data-toc-modified-id=\"bar-plot-5.1.1.2.1\"><span class=\"toc-item-num\">5.1.1.2.1&nbsp;&nbsp;</span>bar plot</a></span></li><li><span><a href=\"#pie-plot\" data-toc-modified-id=\"pie-plot-5.1.1.2.2\"><span class=\"toc-item-num\">5.1.1.2.2&nbsp;&nbsp;</span>pie plot</a></span></li></ul></li></ul></li><li><span><a href=\"#Categorical-ordered-(ordinal)\" data-toc-modified-id=\"Categorical-ordered-(ordinal)-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>Categorical ordered (ordinal)</a></span></li><li><span><a href=\"#Numerical\" data-toc-modified-id=\"Numerical-5.1.3\"><span class=\"toc-item-num\">5.1.3&nbsp;&nbsp;</span>Numerical</a></span><ul class=\"toc-item\"><li><span><a href=\"#describe\" data-toc-modified-id=\"describe-5.1.3.1\"><span class=\"toc-item-num\">5.1.3.1&nbsp;&nbsp;</span>describe</a></span></li><li><span><a href=\"#histogram\" data-toc-modified-id=\"histogram-5.1.3.2\"><span class=\"toc-item-num\">5.1.3.2&nbsp;&nbsp;</span>histogram</a></span></li><li><span><a href=\"#boxplot\" data-toc-modified-id=\"boxplot-5.1.3.3\"><span class=\"toc-item-num\">5.1.3.3&nbsp;&nbsp;</span>boxplot</a></span></li></ul></li></ul></li><li><span><a href=\"#Bivariate-analysis\" data-toc-modified-id=\"Bivariate-analysis-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Bivariate analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#numeric-numeric\" data-toc-modified-id=\"numeric-numeric-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>numeric-numeric</a></span><ul class=\"toc-item\"><li><span><a href=\"#Correlation-Matrix\" data-toc-modified-id=\"Correlation-Matrix-5.2.1.1\"><span class=\"toc-item-num\">5.2.1.1&nbsp;&nbsp;</span>Correlation Matrix</a></span></li><li><span><a href=\"#Heatmap\" data-toc-modified-id=\"Heatmap-5.2.1.2\"><span class=\"toc-item-num\">5.2.1.2&nbsp;&nbsp;</span>Heatmap</a></span></li><li><span><a href=\"#Scatter-Plot\" data-toc-modified-id=\"Scatter-Plot-5.2.1.3\"><span class=\"toc-item-num\">5.2.1.3&nbsp;&nbsp;</span>Scatter Plot</a></span></li></ul></li><li><span><a href=\"#numeric-categorical\" data-toc-modified-id=\"numeric-categorical-5.2.2\"><span class=\"toc-item-num\">5.2.2&nbsp;&nbsp;</span>numeric-categorical</a></span><ul class=\"toc-item\"><li><span><a href=\"#boxplot\" data-toc-modified-id=\"boxplot-5.2.2.1\"><span class=\"toc-item-num\">5.2.2.1&nbsp;&nbsp;</span>boxplot</a></span></li><li><span><a href=\"#Histogram\" data-toc-modified-id=\"Histogram-5.2.2.2\"><span class=\"toc-item-num\">5.2.2.2&nbsp;&nbsp;</span>Histogram</a></span></li><li><span><a href=\"#bar-plot\" data-toc-modified-id=\"bar-plot-5.2.2.3\"><span class=\"toc-item-num\">5.2.2.3&nbsp;&nbsp;</span>bar plot</a></span></li></ul></li><li><span><a href=\"#Categorical-Categorical\" data-toc-modified-id=\"Categorical-Categorical-5.2.3\"><span class=\"toc-item-num\">5.2.3&nbsp;&nbsp;</span>Categorical-Categorical</a></span><ul class=\"toc-item\"><li><span><a href=\"#crosstab\" data-toc-modified-id=\"crosstab-5.2.3.1\"><span class=\"toc-item-num\">5.2.3.1&nbsp;&nbsp;</span>crosstab</a></span></li><li><span><a href=\"#heatmap\" data-toc-modified-id=\"heatmap-5.2.3.2\"><span class=\"toc-item-num\">5.2.3.2&nbsp;&nbsp;</span>heatmap</a></span></li><li><span><a href=\"#bar-plot\" data-toc-modified-id=\"bar-plot-5.2.3.3\"><span class=\"toc-item-num\">5.2.3.3&nbsp;&nbsp;</span>bar plot</a></span></li></ul></li></ul></li><li><span><a href=\"#Multivariate-Analysis\" data-toc-modified-id=\"Multivariate-Analysis-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Multivariate Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Scatter-Plot\" data-toc-modified-id=\"Scatter-Plot-5.3.1\"><span class=\"toc-item-num\">5.3.1&nbsp;&nbsp;</span>Scatter Plot</a></span></li><li><span><a href=\"#Heatmap\" data-toc-modified-id=\"Heatmap-5.3.2\"><span class=\"toc-item-num\">5.3.2&nbsp;&nbsp;</span>Heatmap</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success alertsuccess\" style=\"margin-top: 20px\">\n",
    "\n",
    "## Objetivos de aprendizagem  <a class=\"tocSkip\">\n",
    "    \n",
    "No final deste notebook o aluno deverá saber proceder a uma análise exploratória dos dados.\n",
    "\n",
    "\n",
    "## Learning Objectives  <a class=\"tocSkip\">\n",
    "       \n",
    "When concluding this notebook, the student should know how to conduct an exploratory data analysis. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success alertsuccess\" style=\"margin-top: 20px\">\n",
    "\n",
    "## Sumário  <a class=\"tocSkip\">\n",
    "### Análise Exploratória de Dados <a class=\"tocSkip\">\n",
    "\n",
    "Introdução dos alunos à análise exploratória de dados\n",
    "- importação de dados\n",
    "- análise global\n",
    "- limpeza de dados\n",
    "- estatísticas descritivas (estatísticas básicas, medidas de tendência central, medidas de variabilidade)\n",
    "- análise de dados (análise univariada, bivariada, multivariada)\n",
    "    \n",
    "## Class Summary  <a class=\"tocSkip\">\n",
    "### Exploratory Data Analysis <a class=\"tocSkip\">\n",
    "Introducing students to exploratory data analysis\n",
    "- data loading\n",
    "- data cleaning\n",
    "- descriptive stats (basic stats, measures of central tendency, measures of variability)\n",
    "- data analysis (univariate analysis, bivariate analysis, multivariate analysis)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory data analysis (EDA) is an approach to analyzing and understanding data sets. It is a crucial step in the data science process, as it allows you to identify patterns, trends, and relationships within the data. EDA typically involves the use of visualizations, such as histograms and scatter plots, to help you understand the data and generate hypotheses about it. By using EDA, you can gain valuable insights into the data that can help inform your subsequent analysis and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is partly based on this notebook: https://deepnote.com/@code-along-tutorials/A-Beginners-Guide-to-Exploratory-Data-Analysis-with-Python-f536530d-7195-4f68-ab5b-5dca4a4c3579"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data analysis pipeline begins with the import of a dataset. Importing a dataset is simple with Pandas through functions dedicated to reading the data. If our dataset is a .csv file, we can just use `df = pd.read_csv(\"path/to/my/file.csv\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we will make use of the `titanic` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/siglimumuni/Datasets/master/customer-data.csv\")\n",
    "#df = pd.read_csv(\"data/customer-data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we’ve imported a usable dataset, let’s move on to applying the EDA pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand the big picture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first phase, our goal is to understand what we are looking at, but without going into detail. We try to understand the problem we want to solve, thinking about the entire dataset and the meaning of the variables. We will leverage several Pandas features and properties to understand the big picture. Let’s see some of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two of the most commonly used functions in Pandas are .head() and .tail(). These two allow us to view an arbitrary number of rows (by default 5) from the beginning or end of the dataset. Very useful for accessing a small part of the dataframe quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we apply `.shape` on the dataset, Pandas returns us a pair of numbers that represent the dimensionality of our dataset. This property is very useful for understanding the number of columns (shape[1]) and the length (i.e., lines: shape[0]) of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`info()` gives us a shorter summary of our dataset. It returns us information about the data type, non-null values and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also `.dtypes` which give us the data type info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows the bytes occupied by each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code enables one to copy the records of a given type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_obj = df.select_dtypes(include=['object']).copy()\n",
    "df_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`index` gives as the labels of the lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`columns` gives as the labels of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While our dataset does not appear to have any serious issues, we will nonetheless have to do some basic cleaning and transformation to get it ready for the main EDA task. Some of the questions we will ask ourselves are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- are there any useless or redundant variables?\n",
    "- are there any duplicate columns?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief exploration of this is given below. More details on preparing data, handling missing data, duplicates, etc, can be seen in the Data Wrangling notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the `id` and `postal_code` columns will not be relevant for our analysis, so we can get rid of these using the `drop()` method. We will set the `axis` argument to 1 since we’re dealing with columns, and set the `inplace` argument to True to make the change permanent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete the id and postal_code columns\n",
    "df.drop([\"id\",\"postal_code\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lists information about the records with `non-N/A` values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by checking the dataset for missing or null values. For this, we can use the `isna()` or `isnull()` method which returns a dataframe of boolean values indicating if a field is null or not. To group all missing values by column, we can include the `sum()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display number missing values per column\n",
    "# df.isnull().sum()\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to find out the percentage of missing values in each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()/df.shape[0] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code shows the records that have at least one `NaN` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.isnull().any(axis=1)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have an idea of what data is missing and where. To treat the missing values we can opt for a method from the following :\n",
    "- Drop the variable: drop the column;\n",
    "- Drop the observation(s): delete rows that contain missing data;\n",
    "- Missing Value Imputation: replace them with a value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, deleting that many rows may affect our analysis, so we will go ahead and replace the values instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several different methods exist for imputing missing values and what works best usually depends on the characteristics of the dataset in question as well as the objective of the analysis. One of the simplest methods is by replacing the null values in each column with the column mean or mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin with the `credit_score` column. Since credit scores are heavily influenced by one’s income situation, it would be a better idea to impute the missing values in this column based on the mean credit score for the income group an individual belongs in. We can first run a `groupby()` method to see how the mean values for each income group differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the mean credit score for each income group\n",
    "df.groupby(by=\"income\")[\"credit_score\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean credit scores for each group do differ widely as we suspected. We can go ahead and impute the missing values for the `credit_score` column using the mean credit score for each income group. The simplest way to do this would be by creating a function so we don’t have to repeat codes for each income group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function to impute missing values based on mean credit score for each income group\n",
    "def impute_creditscore(income_classes):\n",
    "    \"\"\"This function takes a list of income groups and imputes the missing values of each based on the mean credit score for          each group\"\"\"\n",
    "    #iterate through each income group\n",
    "    for income_class in income_classes:      \n",
    "        \n",
    "        #create a subset of dataframe to use as filter\n",
    "        mask = df[\"income\"] == income_class\n",
    "        \n",
    "        #calculate the mean for the income group\n",
    "        mean = df[mask][\"credit_score\"].mean()\n",
    "        \n",
    "        #fill the missing values with mean of credit score for group\n",
    "        df.loc[mask,\"credit_score\"] = df[mask]['credit_score'].fillna(mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply our custom function to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the function to the dataframe\n",
    "income_groups = [\"poverty\",\"upper class\",\"middle class\",\"working class\"]\n",
    "impute_creditscore(income_groups)\n",
    "\n",
    "#check for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We no longer have any missing values in the `credit_score` column. We can now tackle the missing values in the `annual_mileage` column. This time, we will do a groupby of the `driving_experience` column and compare the means of each group in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the mean annual mileage for the different driving experience groups\n",
    "df.groupby(by=\"driving_experience\")[\"annual_mileage\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the `credit_score` column, the mean for the different groups in the `driving_experience` do not vary too widely so we can simply impute the null values using the column mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate mean for annual_mileage column\n",
    "mean = df[\"annual_mileage\"].mean()\n",
    "\n",
    "#Fill in null values using the column mean\n",
    "df.fillna({\"annual_mileage\": mean}, inplace=True)\n",
    "\n",
    "#Check for null values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We no longer have any null values in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicate Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check for duplicate rows we can use `.duplicated().sum()` — this will print us the number of duplicated rows in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe the duplicate records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And remove them, keeping only the last instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(keep = 'last',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descriptive statistics is a helpful way to understand characteristics of your data and to get a quick summary of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas in python provide an interesting method `describe()`. The describe function applies basic statistical computations on the dataset like extreme values, count of data points standard deviation etc. Any missing value or `NaN` value is automatically skipped. `describe()` function gives a good picture of the distribution of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also apply `describe` on top of a given data type value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=['int'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It sums the values found in each column (inclunding of text columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the minimum values of each of the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the max values for each of the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Central Tendency Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A measure of central tendency is one capable of representing what are the typical or average values of the data set. Three measures of central tendency are usually used: mean, median and mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the mean. It is the sum of each value in the set of data, divided by the number of observations in the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(['float64' , 'int64']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: When to make use of the median?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes sense to use the median as a measure of centrality, as opposed to the mean, when the data have outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of this measure is to separate distribution into two equal parts. The median will correspond to middle point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the following set `[4,1,3,2,4,4,2]`. First, we will have to sort the values: `{1, 2, 2, 3, 4, 4, 4}`. In this case, three is the median value as it separates the distribution in two parts: `{1, 2, 2}` e `{4, 4, 4}`. In case, of an even number of elements in the set, you will need to proceed by summing the two values that appear at the middle and then divide them by 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(['float64' , 'int64']).median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mode is the most frequent value of a distribution. For example, recent research indicates that there are more Python programmers in the field of data science than any other programming language. In this way, Python is the rage among languages for data science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you apply the mode to the entire dataset, the system will print out the mode for each of the columns. The system will print out more than one line whenever the most frequent value is recorded by more than one value instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measures of Variability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As just shown, mean, median, and mode are measures that aim to summarize in a single number what is typical (or average) in a data set. However, in many practical situations these measures can provide just an incomplete picture of your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance computing the mean of the `annual_mileage` gives us a value of 11694."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['annual_mileage'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this may was not able to present a real picture of the situation. The best thing to do is to use a measure of variability, such as amplitude (variance) or standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance (Amplitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amplitude is a quick measure of variability. It consists of the difference between the highest and lowest value of a\n",
    "particular data set. In our example, the amplitude is 20,000, meaning that there is a big difference between the max and the minimum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amp = df['annual_mileage'].max() - df['annual_mileage'].min()\n",
    "amp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is even more interesting when comparing the value for the `driving_experience` classes. With this, it is clear that the distribution of the `annual_mileage` presents a greater variability for the `driving_experience 10-19y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Amp1 = df[df['driving_experience']=='0-9y']['annual_mileage'].max() - df[df['driving_experience']=='0-9y']['annual_mileage'].min()\n",
    "Amp2 = df[df['driving_experience']=='10-19y']['annual_mileage'].max() - df[df['driving_experience']=='10-19y']['annual_mileage'].min()\n",
    "Amp3 = df[df['driving_experience']=='20-29y']['annual_mileage'].max() - df[df['driving_experience']=='20-29y']['annual_mileage'].min()\n",
    "Amp4 = df[df['driving_experience']=='30y+']['annual_mileage'].max() - df[df['driving_experience']=='30y+']['annual_mileage'].min()\n",
    "print(Amp1)\n",
    "print(Amp2)\n",
    "print(Amp3)\n",
    "print(Amp4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amplitude has the advantage of being simple and quick to calculate. However, it has the disadvantage of relying only on two\n",
    "values of the entire distribution (the smallest value and the largest value). Per this, there will always be a risk that it will be influenced by a single extreme value and very different from the others. The solution is to adopt measures that take into account all values of the distribution, as is the case for standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, the standard deviation can be understood as a measure of data dispersion around the sample mean. A low standard deviation indicates that the data points tend to be close to the mean. A high standard deviation indicates that the data points are spread over a wide range of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['annual_mileage'].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing for the different classes of the `driving_experience` we can observe that the deviation for class for all the classes is high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std1 = df[df['driving_experience']=='0-9y']['annual_mileage'].std()\n",
    "std2 = df[df['driving_experience']=='10-19y']['annual_mileage'].std()\n",
    "std3 = df[df['driving_experience']=='20-29y']['annual_mileage'].std()\n",
    "std4 = df[df['driving_experience']=='30y+']['annual_mileage'].std()\n",
    "print(std1)\n",
    "print(std2)\n",
    "print(std3)\n",
    "print(std3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our cleansed dataset we can go ahead and begin the task of exploring the data. While several different analyses exist for EDA, we can group them under three large umbrellas; `univariate analysis`, `bivariate analysis`, and `multivariate analysis`. We will look at each one of these in turn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the datataset again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/siglimumuni/Datasets/master/customer-data.csv\")\n",
    "#df = pd.read_csv(\"data/customer-data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Univariate analysis is the simplest form of analyzing data. As the name implies, it deals with analyzing data within a single column or variable and is mostly used to describe data. There are different kinds of univariate analyses:\n",
    "- Categorical unordered (nominal)\n",
    "- Categorical ordered (ordinal)\n",
    "- Numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical unordered (nominal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `gender` column contains two sub-categories that describe whether a client is male or female. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the number of unique values in the particular column using `unique()` function in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the count for each category in the \"gender\" column\n",
    "df[\"gender\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### value_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can get a count of each category by using the `value_counts()` method. `value_counts()` can be used with any variable, but works best with categorical variables. This function also informs us of how balanced the classes are within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the count for each category in the \"gender\" column\n",
    "df[\"gender\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can have access  to the indexes and the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = df['gender'].value_counts().index\n",
    "values = df['gender'].value_counts().values\n",
    "print(index)\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also express the data as a percentage by passing normalize = True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gender'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### bar plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better yet, we can visualize this information using a countplot from Seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Create a countplot to visualize the count of each category in the gender column.\n",
    "sns.countplot(data=df,x=\"gender\")\n",
    "plt.title(\"Number of Clients per Gender\")\n",
    "plt.ylabel(\"Number of Clients\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can make use of matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df['gender'].value_counts().plot(kind=\"bar\")\n",
    "plt.title(\"Number of passengers per Gender\")\n",
    "plt.xlabel(\"gender\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.ylabel(\"Number of Clients\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### pie plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Define plot size\n",
    "plt.figure(figsize=[6,6])\n",
    "\n",
    "#Define column to use\n",
    "data = df[\"vehicle_type\"].value_counts(normalize=True)\n",
    "\n",
    "#Define labels\n",
    "labels = [\"sedan\",\"sports car\"]\n",
    "\n",
    "#Define color palette\n",
    "colors = sns.color_palette('pastel')\n",
    "\n",
    "#Create pie chart\n",
    "plt.pie(data,labels=labels,colors=colors, autopct='%.0f%%')\n",
    "plt.title(\"Proportion of Clients by vehicle_type\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('vehicle_type').vehicle_type.count().plot(kind='pie')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical ordered (ordinal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This type of data has a natural rank and progression. Examples from our dataset include `education` and `income`. Let’s explore the `education` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Create a countplot to visualize the count of each category in the education column \n",
    "plt.figure(figsize=[8,5])\n",
    "sns.countplot(data=df,x=\"education\",order=[\"university\",\"high school\",\"none\"],color=\"orange\")\n",
    "plt.title(\"Number of Clients per Education Level\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more clients with a high school education than any other category, followed by university graduates and then clients with no education."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third type of univariate analysis uses numerical data. Univariate numeric data is usually analyzed by calculating functions like the `mean` (see below), `mode`, `max`, `min`, `standard deviation`, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['credit_score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One easy way to get these summary statistics on a numerical column is by using the `describe()` method as we have seen before. Let’s try this on the `credit_score` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['credit_score'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is great information, but it doesn’t tell us how the data is distributed. A histogram is a great way to visualize the frequency distribution of numerical data. We can plot one using the `histplot()` function in Seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Plot a histogram using the \"credit_score\" column\n",
    "plt.figure(figsize=[8,5])\n",
    "sns.histplot(data=df,x=\"credit_score\").set(title=\"Distribution of credit scores\",ylabel=\"Number of clients\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can just display it with the `hist()` function of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['credit_score'].hist(bins=40, figsize=(8,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also resort to matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(df['credit_score'])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `credit_score` column follows a normal distribution or bell curve. Let’s create another histogram for the `annual_mileage` column, but this time we will include a kernel density estimation (kde) to show smoothness or continuity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Plot a histogram using the \"annual_mileage\" column\n",
    "plt.figure(figsize=[8,5])\n",
    "sns.histplot(data=df,x=\"annual_mileage\",bins=20,kde=True).set(title=\"Distribution of Annual Mileage\",ylabel=\"Number of clients\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another bell curve, confirming that data near the mean are more frequent in occurrence than data far from the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### boxplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more useful tool is boxplot which you can use through matplotlib module. Boxplot is a pictorial representation of distribution of data which shows the \"minimum\", the first quartile (Q1), the median, the third quartile (Q3) and the \"maximum\". Additionally, it can tell us what the outliers are and whether or not the data is dispersed. The figure below shows a boxplot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.di.ubi.pt/~rcampos/assets/img_tutorials/DataScience/boxplot.jpg\" width=\"600\" height=\"600\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Source: https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- primeiro quartil (Q1/25% do Percentil);\n",
    "- mediana (Q2/50% Percentil);\n",
    "- terceiro quartil (Q3/75% Percentil);\n",
    "- wiskers: linha azul;\n",
    "- outliers: círculos verdes;\n",
    "- “maximum”: Q3 + 1.5*IQR\n",
    "- “minimum”: Q1 -1.5*IQR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot shows a boxplot of a normal distribution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.di.ubi.pt/~rcampos/assets/img_tutorials/DataScience/boxplot1.jpg\" width=\"600\" height=\"600\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Source: https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider the dataset we’ve been dealing with again and lets draw a boxplot on the `annual_mileage` population:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = sns.boxplot(x=df['past_accidents'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With dataframes only. The bottom row of the box shows the 1st quartile. The top line represents the 3rd quartile. The median is the green line, and the mean is the triangle. The wiskers (blue lines are used to indicate that values outside of them are outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['past_accidents']].boxplot(showmeans=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The [graphics in pandas](https://towardsdatascience.com/9-pandas-visualizations-techniques-for-effective-data-analysis-fc17feb651db) have not been created with the objective of generating graphics with quality of production (those that may appear in a report or on your company website). In reality, pandas graphics are intended to serve as a support tool for the process of studying a data base. To create professional graphics, there is a another more recommended library called Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.boxplot(df['past_accidents'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bivariate analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bivariate analysis involves analyzing data with two variables or columns. This is usually a way to explore the relationships between these variables and how they influence each other, if at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bivariate analysis could take one of three different forms: \n",
    "- numeric-numeric; \n",
    "- numeric-categorical;\n",
    "- categorical-categorical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numeric-numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation is a simple relationship between two variables in a context such that one variable affects the other. Correlation is different from act of causing (see this article: https://rbaeza-yates.medium.com/correlation-in-ai-1916fc8a20c8). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.di.ubi.pt/~rcampos/assets/img_tutorials/DataScience/Correlation.jpg\" width=\"600\" height=\"600\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Source: https://rbaeza-yates.medium.com/correlation-in-ai-1916fc8a20c8</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to calculate correlation among pairs of attributes (numerical variables) is to find Pearson correlation. We can say there is a strong correlation between two variables when Pearson correlation coefficient is close to either 1 or -1. Values close to 1 mean a strongly positive linear correlation (it basically means that one variable is practically synonymous with the other), that is, as one variable increases, the other variable also increases (e.g. , more rain means more flooding). Values close to -1 mean a strongly negative linear correlation (e.g., less rain means more pollution). Finally, values close to 0 mean that there is no linear correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.di.ubi.pt/~rcampos/assets/img_tutorials/DataScience/Pearson.jpg\" width=\"600\" height=\"600\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Source: https://statistics.laerd.com/statistical-guides/pearson-correlation-coefficient-statistical-guide.php</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.di.ubi.pt/~rcampos/assets/img_tutorials/DataScience/Pearson1.jpg\" width=\"600\" height=\"600\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A correlation matrix is useful for identifying the relationship between several variables. The following code shows how to do a correlation matrix between all the selected type variables. Default method is `pearson` thus doing `corr()` or `corr(method=\"pearson\")` is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(['float64' , 'int64']).corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other alternatives are `spearman`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(['float64' , 'int64']).corr(method=\"spearman\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or `kendall`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(['float64' , 'int64']).corr(method=\"kendall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also use a correlation matrix to get more specific information about the relationship between two or more variables. As another example, let’s create a matrix using the `speeding_violations`, `DUIs`, and `past_accidents` columns. All our variables exhibit a positive correlation with each other, meaning when one goes up the other goes up as well and vice-versa. But how do we interpret the strength of this relationship? Generally speaking, a **correlation coefficient between 0.5 and 0.7 indicates variables that can be considered moderately correlated**, while a **correlation coefficient whose magnitude is between 0.3 and 0.5 indicates variables that exhibit weak correlation**, as is the case with most of our variables. This means a moderate, positive correlation exists between the number of past accidents and speeding violations, while a weak, positive correlation exists between the number of past accidents and DUIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a correlation matrix to show relationship between select variables\n",
    "corr_matrix = df[[\"speeding_violations\",\"DUIs\",\"past_accidents\"]].corr()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sort the correlation for a specific variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix[\"past_accidents\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best way to visualize correlation however, is with a heatmap as it allows us to efficiently grasp which variables are strongly correlated with each other. We can easily create one by passing the correlation matrix into the `heatmap()` function in Seaborn. In the following, we construct a heatmap for the entire correlation matrix `df.corr()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Create a heatmap to visualize correlation\n",
    "plt.figure(figsize=[8,5])\n",
    "sns.heatmap(df.select_dtypes(['float64' , 'int64']).corr(),annot=True,cmap='Reds')\n",
    "plt.title(\"Correlation between Selected Variables\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a further alternative example let's see the correlation matrix for the three variables previously analyzed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Create a heatmap to visualize correlation\n",
    "plt.figure(figsize=[8,5])\n",
    "sns.heatmap(corr_matrix,annot=True,cmap='Reds')\n",
    "plt.title(\"Correlation between Selected Variables\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatter plots are a common way to compare two numeric variables. Let’s investigate the relationship between `annual_mileage` and `speeding_violations`. With seaborn we can create such scatter plot. From the graph, we can infer a negative correlation between annual mileage and the number of speeding violations. This means the more miles a client drives per year, the fewer speeding violations they commit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"annual_mileage\",y=\"speeding_violations\", data=df, palette=\"Dark2\", s=80)\n",
    "plt.title(\"Annual Mileage vrs Speeding Violations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatter plots can also be produced with matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Create a scatter plot to. show relationship between \"annual_mileage\" and \"speeding_violations\"\n",
    "plt.figure(figsize=[8,5])\n",
    "plt.scatter(data=df,x=\"annual_mileage\",y=\"speeding_violations\")\n",
    "plt.title(\"Annual Mileage vrs Speeding Violations\")\n",
    "plt.ylabel(\"Speeding Violations\")\n",
    "plt.xlabel(\"Annual Mileage\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this through dataframes plot function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x=\"annual_mileage\",y=\"speeding_violations\",kind = 'scatter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible for example to make a matrix of scatter plots. The diagonal would be full of straight lines (since each variable would be plotted against itself) so we choose, alternatively, to plot a histogram of the variables on the diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "scatter_matrix(df[['annual_mileage','speeding_violations']], figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get a matrix of scatter plots for all the numerical attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "scatter_matrix(df.select_dtypes(['float64' , 'int64']), figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numeric-categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we analyze data using one set of numeric variables and another set of categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### boxplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best way to understand the relationship between a numeric variable and a categorical variable is through a boxplot. Box plots display a five-number summary of a set of data; the minimum, first quartile, median, third quartile, and maximum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code and observe that both variables have similar medians (denoted by the middle line that runs through the box) though clients who made a claim have slightly higher median annual mileage than clients who didn’t. The same can be said for the first and third quartiles (denoted by the lower and upper borders of the box respectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot two boxplots to compare dispersion\n",
    "sns.boxplot(data=df,x='outcome', y='annual_mileage')\n",
    "plt.title(\"Distribution of Annual Mileage per Outcome\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can compare the distributions of the two categories in `outcome` based on their annual mileage, but this time we’ll make use of a bivariate histogram by setting the `hue` argument in the `histplot()` function to `outcome`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create histograms to compare distribution \n",
    "sns.histplot(df,x=\"annual_mileage\",hue=\"outcome\",element=\"step\")\n",
    "plt.title(\"Distribution of Annual Mileage per Outcome\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will repeate this process for the credit_score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create histograms to compare distribution \n",
    "sns.histplot(df,x=\"credit_score\",hue=\"outcome\",element=\"step\")\n",
    "plt.title(\"Distribution of Credit Score per Outcome\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bar plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do a `groupby` to group the results by `gender` before summing the `speeding_violations`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = df.groupby('gender')['speeding_violations'].sum()\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can then be used to make a bar plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=summary.index , y=summary.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = df.groupby('gender')['speeding_violations'].sum()\n",
    "plt.bar(x=summary.index , height=summary.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical-Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may have guessed by now, this involves a set of two categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### crosstab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we will explore how the `outcome` variable relates to categories like `age`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['outcome'],df['age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or `outcome` and the `vehicle_year`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the results in terms of % and with an All column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['outcome'],df['vehicle_year'], normalize=True, margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.heatmap(pd.crosstab(df['outcome'],df['vehicle_year']))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bar plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we will explore how the `outcome` variable relates to categories like `age`. Now let’s check how the claim rate is distributed between the different categories of `age`. From the below, it is clear that younger people are more likely to make an insurance claim. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the average claim rate per age group\n",
    "plt.figure(figsize=[8,5])\n",
    "df.groupby('age')['outcome'].mean().plot(kind=\"bar\")\n",
    "plt.title(\"Claim Rate by Age Group\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same for `vehicle_year`. Clients with older vehicles are much more likely to file a claim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the average claim rate per age group\n",
    "plt.figure(figsize=[8,5])\n",
    "df.groupby('vehicle_year')['outcome'].mean().plot(kind=\"bar\")\n",
    "plt.title(\"Claim Rate by Vehicle Year\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This comprises data analysis involving more than two variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One possibility is to make use of scatter plots to see the relationship between two numeric variables `annual_mileage` and `speeding_violations` by using a third variable `outcome`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"annual_mileage\",y=\"speeding_violations\", hue=\"outcome\", data=df, palette=\"Dark2\", s=80)\n",
    "plt.title(\"Annual Mileage vrs Speeding Violations, stratified by outcome\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common type of multivariate analysis is the heatmap. Heatmaps provide a fast and simple way for visual recognition of patterns and trends. We can easily check the relationship between variables in our data set like `education` and `income` by using a third variable, `outcome`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edu_income = pd.pivot_table(data=df,index='education',columns='income',values='outcome',aggfunc='mean')\n",
    "edu_income"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then pass in our pivot table to the `heatmap()` function in Seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a heatmap to visualize income, education and claim rate\n",
    "plt.figure(figsize=[8,5])\n",
    "sns.heatmap(edu_income,annot=True,cmap='coolwarm',center=0.117)\n",
    "plt.title(\"Education Level and Income Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High school graduates in the poverty income class have the highest claim rate, followed by university graduates in the poverty income class. Clients in the upper class income category with no education have the lowest claim rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliography <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.geeksforgeeks.org/exploratory-data-analysis-in-python/\n",
    "- https://towardsdatascience.com/exploratory-data-analysis-in-python-a-step-by-step-process-d0dfa6bf94ee\n",
    "- https://deepnote.com/@code-along-tutorials/A-Beginners-Guide-to-Exploratory-Data-Analysis-with-Python-f536530d-7195-4f68-ab5b-5dca4a4c3579\n",
    "- https://www.analyticsvidhya.com/blog/2022/02/exploratory-data-analysis-in-python/#h2_19\n",
    "- https://www.digitalocean.com/community/tutorials/exploratory-data-analysis-python#8-filter-the-data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
